---
title: "Composite Scores"
---

```{r loadpackcomposite}
#| include: false
source("loader.R")
```


Clinicians use [composite scores]{.defword title="A **composite variable** is the (possibly weighted) sum of two or more variables."} whenever they can because they tend to be more reliable and valid than single scores. How much more reliable is a composite score than a single score? It, of course, depends. First, the reliability coefficients of all of the scores that make up the composite score matter. Second, the correlations among the components of the composite matter. The more reliable and correlated the components, the more reliable the composite.

As discussed previously, it is quite possible for different scores from the same test to have different reliability coefficients. Scores from the same test tend to have similar reliability coefficients, though it is quite possible for differences in reliability to be large. In a well designed test, such cases are rare. If we can assume that the test is well designed and that test score reliability coefficients are within a fairly narrow range, we can assume that the classical reliability coefficient is a good estimate of the reliability of all scores from a particular test. To the degree that such assumptions are incorrect, the following equations will be less accurate. 

Before we can discuss the reliability of composites, we must cover more basic statistics as they apply to composite scores. I will present this material in three ways. First, I will show the equations in standard notation. Adjacent to this, I will show the same equations in matrix notation.^[Why bother with matrices? It does seem to be an unnecessary complication at first. However, many things become clearer and simpler with matrices, which I hope to illustrate. Furthermore, R was designed to work elegantly with vectors and matrices.] Then I will walk through the calculations using R.

## The Mean of the Sum Is the Sum of the Means

This section is going to make a simple idea look complicated. If you get lost, this is what I am trying to say: 

> If we create a new random variable by adding a bunch of random variables together, the mean of that new variable is found by adding together the means of all the variables we started with.


I'm sorry for what comes next, but the work we put into it will pay off later. Okay, now let's make that simple idea formal and hard to understand: 

We can start with the example of two variables: $X_1$ and $X_2.$ The sum of these variables, which we will call $X_S$, has a mean, which is the sum of the means of $X_1$ and $X_2.$ That is, $\mu_{S}=\mu_{1}+\mu_{2}.$


What if we have three variables? or four? or five? It would be tedious to illustrate each case one by one. We need a way of talking about the sum of a bunch of random variables but without worrying about how many variables there are. Here we go:

### Calculating a Sum 

Suppose $k$ is a positive integer greater than 1. So if there are $k$ random variables, the notation for the set of all them is $\{X_1,...,X_k\}.$ However, it is even more compact to use matrix notation such that $\mathbf{X}=\{X_1,...,X_k\}.$

Now, $\mathbf{X}$ is a set of random variables in their entirety, without referencing any particular values those variables might generate. A set of particular values of these variables would be shown as $\mathbf{x}$ or $\{x_1,...,x_k\}.$ In regular notation, the sum of these particular values would be:

$$
x_S=\sum_{i=1}^{k}{x_i}
$$

```{r sum_standard_notation}
#| tbl-colwidths: [20, 80]
tibble::tribble(
  ~Symbol,                                                                           ~Meaning,
  "$x_S$",                             r"(The sum of all $k$ scores in $\{x_1,...,x_k\}$)",
  "$x_i$",                                   "A particular score generated by variable $X_i$",
  "$k$", r"(The number of variables in $\{X_1,...,X_k\}$, $(k \in \mathbb{N}_1)$)") %>% 
  knitr::kable(escape = F) 
```


The same formula is more compact in matrix notation:

$$
x_S=\mathbf{x'1}
$$


Where

```{r sum_matrix_notation}
#| tbl-colwidths: [20, 80]
tibble::tribble(
  ~Symbol,  ~Meaning,
  r"($\mathbf{x}$)", r"(A $k \times 1$ vector of scores $\{x_1,x_2,...,x_k\}$)",
  r"($\mathbf{1}$)", r"(A $k \times 1$ vector of ones $\{1_1,1_2,...,1_k\}$)") %>% 
  knitr::kable(escape = FALSE)
```

The $\mathbf{1}$ symbol may be a bit confusing. It is a column of ones that has the same length (number of elements) as $\mathbf{x}.$ Suppose that $\mathbf{x}$ has a length of three. In this case:

$$\mathbf{1}_3=\begin{bmatrix}1\\ 1\\ 1 \end{bmatrix}$$


Also, $\mathbf{x'}$, is $\mathbf{x}$ [transposed]{.defword title="To **transpose** means to make all columns of a matrix into rows (or all rows into columns)."}. 


:::{.column-margin}

Transposition is noted with a prime symbol $(\mathbf{^\prime})$. If 

$$
\mathbf{A}= \begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
\end{bmatrix}
$$ 
then

$$
\mathbf{A'}= \begin{bmatrix}
1 & 4\\
2 & 5\\
3 & 6\\
\end{bmatrix}
$$

A column of 3 ones:

$$
\mathbf{1}_3=\begin{bmatrix}1\\1\\1\end{bmatrix}
$$

Transposed, a column of 2 ones is a row of 2 ones.

$$
\mathbf{{1'}}_2=\begin{bmatrix}1&1\end{bmatrix}
$$


Typically, the number of ones is implied such that the length of the column or row will be compatible with the adjacent matrix. For example, post-multiplying by a vector ones will create a vector of row totals:

$$
\begin{align*}
\mathbf{A1}&=\begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
\end{bmatrix}\begin{bmatrix}1\\1\\1\end{bmatrix}\\
&=\begin{bmatrix}1+2+3\\4+5+6\end{bmatrix}\\
&=\begin{bmatrix}6\\15\end{bmatrix}
\end{align*}
$$

Pre-multiplying by a vector of ones will create column totals:

$$
\begin{align*}
\mathbf{1'A}&=\begin{bmatrix}1\\1\end{bmatrix}'\begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
\end{bmatrix}\\
&=\begin{bmatrix}1&1\end{bmatrix}\begin{bmatrix}
1 & 2 & 3\\
4 & 5 & 6\\
\end{bmatrix}\\
&=\begin{bmatrix}1+4 & 2+5 & 3+6\end{bmatrix}\\&=\begin{bmatrix}5 & 7 & 9\end{bmatrix}
\end{align*}
$$

To create a sum of the entire matrix, multiply by $\mathbf{1}$ on both sides:

$$
\mathbf{1}'\mathbf{A1}=21
$$

:::

Therefore, $\mathbf{x}'$ is a row vector. A row vector multiplied by a column vector is the sum of the product of each analogous element in the pair of vectors. Thus,

$$
\begin{align*}
\mathbf{x'1}&=\begin{bmatrix} x_1\\ x_2\\ x_3 \end{bmatrix}'
\begin{bmatrix}1\\ 1\\ 1 \end{bmatrix} \\
&=\begin{bmatrix} x_1 & x_2 & x_3\end{bmatrix}
\begin{bmatrix}1\\ 1\\ 1\end{bmatrix}\\
&=x_1 \times 1 + x_2 \times 1 + x_3 \times 1\\
&=  x_1 + x_2 + x_3
\end{align*}
$$

Let's do some calculations in R with a particular example. Suppose that there are three variables: $\mathbf{X}=\{X_1, X_2, X_3\}.$ In R, we will create a vector of the names of variables in $\mathbf{X}$:

```{r demo-MakeXNames}
Xnames <- c("X1","X2","X3")
```

Now suppose that there are three particular scores: $\mathbf{x}=\{100,120,118\}$



```{r demo-MakeX}
# x = vector of particular scores from variables X1, X2, X3
x <- c(110,120,118)

# The `names` function returns or sets the names of vector elements.
# Applying Xnames to x (to make output easier to read)
names(x) <- Xnames 

```

:::{callout-note}

Alternatively, names and values can be set in a single line:

```{r demo-MakeX-alternative}
x <- c(X1 = 110, X2 = 120, X3 = 118)
```

For reasons of clarity and readability, I prefer this approach for name assignments that I only need to perform once. However, when I need to apply the same names to different objects repeated, I use the `names` function to apply a vector of names.

:::

The sum of these three scores can be calculated in a variety of ways. Here is the easiest:

```{r demo-SumX}
# x_S = The sum of scores x1, x2, and x3
x_S <- sum(x)
```

However, if we want to be matrix algebra masochists (and, apparently, at least one of us does!), we could do this:

```{r demo-SumXmatrix}
# A vector of ones the same length as x
ones <- rep(1, length(x))

# Notes: 
# The rep function creates vectors of repeated elements. 
# For example, rep(5,3) is the same as c(5,5,5).
# 
# The length function returns the number of elements in a vector. 
# For example, length(c(3,3)) returns `2`.


# Calculating x_S with matrix algebra
x_S <- t(x) %*% ones

# Notes: 
# The t function transposes a vector or matrix.
# The operator %*% multiplies compatible matrices.
```

Either way that we calculate it, $x_S = `r x_S`.$

### Calculating the Mean of a Sum 

The mean of $X_S$ is:

$$
\mu_{S}=\sum_{i=1}^{k}{\mu_i}=\mathbf{\mu'1}
$$

```{r composite_sum}
tibble::tribble(
  ~Symbol,  ~Meaning,
  "$\\mu_S$", "The mean of $X_S$",
  "$\\mu_i$", "The mean of $X_i$",
  "$\\mathbf{\\mu}$", "A $k \\times 1$ vector of means of the variables in $\\mathbf{X}$") %>% 
  knitr::kable(escape = FALSE)
```

Suppose that the means of $X_1$, $X_2$, and $X_3$ are all 100. 

```{r MakeMeans, echo=TRUE}
# m = vector of means of X
m <- c(100,100,100)
```
Again, the mean of $X_S$ $(\mu_S)$ can be calculated in two ways:

```{r MeanC, echo=TRUE}
# m_S = The mean of S
# The easy way
m_S <- sum(m)

# With matrix algebra
m_S <- t(m) %*% ones
```
Running this code, we can see that $\mu_S = `r m_S`.$

### Calculating the Mean of a Weighted Sum 

The mean of a weighted sum is the weighted sum of the means. That is, if

\begin{equation}
x_S=\sum_{i=1}^{k}{x_i w_i}=\mathbf{x'w}
\end{equation}

Where

\begin{conditions*}
w_i & The weight for $X_i$\\
\mathbf{w} & A $k \times 1$ vector of weights for each of the variables in $\mathbf{X}$\\
\end{conditions*}

then

\begin{equation}
\mu_S=\sum_{i=1}^{k}{\mu_i w_i}=\mathbf{\mu'w}
\end{equation}

:::{.column-margin}
Note that the calculation of $X_S$ and $\mu_S$ with matrix algebra is the same as it was with an equally weighted sum except that instead of post-multiplying by a vector of ones $(\mathbf{x^\prime 1})$, we post-multiply by a vector of weights $(\mathbf{x^\prime w}).$ In truth, an equally weighted sum is a special case of a weighted sum in which $\mathbf{w}$ consists entirely of ones.
:::

Suppose that $\mathbf{w} = \{0.5,1,2\}.$ That is, the weight for $X_1$ is 0.5, the weight for $X_2$ is 1, and the weight for $X_3$ is 2. We will continue to use the same values for $\mathbf{x}$ and $\mathbf{\mu}$ as before:



```{r WeightedSum, echo=TRUE}
# w = The vector of weight for variables X1, X2, and X3
w = c(0.5, 1, 2)

# The easy way
x_S_weighted <- sum(x * w)
m_S_weighted <- sum(m * w)

# Notes:
# The multiplication operator * multiplies analogous elements 
# of vectors  and matrices. In the example, `x * w` is 
# c(110 * 0.5, 120 * 1, 118 * 2)

# With matrix algebra
x_S_weighted <- t(x) %*% w
m_S_weighted <- t(m) %*% w
```

Running the code shows that $x_S = `r x_S_weighted`$ and that $\mu_S = `r m_S_weighted`.$

## The Variance of the Sum Is the Sum of the Covariance Matrix

:::{.column-margin}
Unfortunately, the notation for a covariance matrix is a bold capital sigma $\mathbf{\Sigma}$, which is easily confused with the summation symbol, which is generally larger and not bold: $\sum.$
:::

If variables are uncorrelated, the variance of their sum is the sum of their variances. However, this is not true when variables are substantially correlated. The formula for the variance of a sum looks more complex than it is. It is just the sum of the covariance matrix.

$$
\sigma_{X_S}^2=\sum_{i=1}^{k}{\sum_{j=1}^{k}{\sigma_{ij}}}=\mathbf{1'\Sigma 1}
$$ {#eq-variancesum}

```{r covariance_sum}
tibble::tribble(
  ~Symbol,  ~Meaning,
  "$\\sigma_{X_S}^2$", "The variance of $X_S$",
  "$\\sigma_{ij}$", "The covariance between $X_i$ and $X_j$ $(\\sigma_{ij}=\\sigma_i^2$ if $i=j)$",
  "$\\mathbf{\\Sigma}$", "The $k \\times k$ covariance matrix of all the variables in $\\mathbf{X}$") %>% 
  knitr::kable(escape = FALSE)
```

:::{.column-margin}
The symbol for a sample correlation is the Roman lowercase $r$, and a matrix of such correlations is an uppercase $\mathbf{R}.$ Therefore, the population correlation coefficient is a Greek lowercase rho: $\rho.$ This, unfortunately means that a matrix of correlations should be an uppercase rho: $\mathbf{P}.$ Somehow, statisticians are okay with $\rho$ looking a lot like an italicized Roman letter *p*. However, using an uppercase rho $(\mathbf{P})$ for a correlation matrix is too weird even for statisticians! You know, hobgoblins of little minds and all...
:::

:::{#fig-bienayme .column-margin}

![](images/Bienayme.jpeg)

Irénée-Jules Bienaymé (1796--1878)<br>[Image Credit](https://commons.wikimedia.org/wiki/File:Ir%C3%A9n%C3%A9e-Jules_Bienaym%C3%A9.jpeg)
:::

Irénée-Jules Bienaymé is credited with the derivation of @eq-variancesum. The 2-test version of the equation, @eq-bienayme, is called [Bienayme's Identity](https://en.wikipedia.org/wiki/Bienaym%C3%A9's_identity).

$$
\sigma_{X+Y}^2 = \sigma_X^2 + 2\sigma_{XY} +\sigma_Y^2
$$ {#eq-bienayme}

:::{.callout-note title="Proof of @eq-bienayme" collapse="true"}

To prove @eq-bienayme, recall that variance in general is defined as 

$$
\begin{aligned}
\mathrm{Var}(X)&=\mathcal{E}\left(\left(X-
\mathcal{E}\left(X\right)\right)^2\right)\\
&=\mathcal{E}\left(\left(X-
\mu_X\right)^2\right)\\
&=\sigma_X^2
\end{aligned}
$$ {#eq-variancedef}

and that covariance is defined as

$$
\begin{aligned}
\mathrm{Cov}(XY)&=\mathcal{E}\left(\left(X-
\mathcal{E}\left(X\right)\right)\left(Y-
\mathcal{E}\left(Y\right)\right)\right)\\
&=\mathcal{E}\left(\left(X-
\mu_X\right)\left(Y-
\mu_Y\right)\right)\\
&=\sigma_{XY}
\end{aligned}
$$ {#eq-covariancedef}

Applying @eq-variancedef to $X+Y$ and making substitutions from @eq-covariancedef:

$$
\begin{aligned}
\rm{Var}(X+Y)&=\mathcal{E}\left(\left(\left(X+Y\right)-
\mathcal{E}\left(X+Y\right)\right)^2\right)\\
&=\mathcal{E}\left(\left(X+Y-
\mathcal{E}\left(X\right)-\mathcal{E}\left(Y\right)\right)^2\right)\\
&=\mathcal{E}\left(\left(X+Y-
\mu_X-\mu_Y\right)^2\right)\\
&=\mathcal{E}\left(\left(\left(X-
\mu_X\right)+\left(Y-\mu_Y\right)\right)^2\right)\\
&=\mathcal{E}\left(\left(X-
\mu_X\right)^2+2\left(X-\mu_X\right)\left(Y-\mu_Y\right)+\left(Y-\mu_Y\right)^2\right)\\
&=\mathcal{E}\left(\left(X-
\mu_X\right)^2\right)+2\mathcal{E}\left(\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right)+\mathcal{E}\left(\left(Y-\mu_Y\right)^2\right)\\
&=\sigma_X^2+2\sigma_{XY}+\sigma_Y^2
\end{aligned}
$$
Proof adapted from @sochProofVarianceSum2020.

:::



Suppose that the standard deviations of $X_1$, $X_2$, and $X_3$ are all 15. Thus, $\sigma=\{15,15,15\}.$ The correlations among the three variables are shown in matrix $\mathbf{R}$:

```{r setR, include=FALSE}
# R = correlation matrix of variables in X
R <- matrix(c(1, 0.5, 0.6, 
              0.5, 1, 0.7,  
              0.6, 0.7, 1),
    nrow = 3,
    ncol = 3,
    byrow = TRUE)

R
```

$$
\mathbf{R} = `r bmatrix(R)`
$$

To create this matrix in R:

```{r demo-setR}
<<setR>>
```  





The covariance matrix $\mathbf{\Sigma}$ can be computed from the correlation matrix and the standard deviations like so:

$$
\sigma_{ij} = \sigma_{i} \sigma_{j} \rho_{ij}
$$


```{r cov2cortable}
tibble::tribble(
  ~Symbol,  ~Meaning,
  "$\\sigma_{ij}$", "The covariance between $X_i$ and $X_j$",
  "$\\rho_{ij}$", "The correlation between $X_i$ and $X_j$",
  "$\\sigma_{i}$", "The standard deviation of variable $X_i$",
  "$\\sigma_{j}$", "The standard deviation of variable $X_j$") %>% 
  knitr::kable(escape = FALSE)
```

Unfortunately, computing a covariance matrix like this in a computer program is inelegant because we have to make use of looping:

```{r demo-LoopingCM}
# R = Correlation matrix of variables in X
R <- matrix(c(1,0.5,0.6,
              0.5,1,0.7,
              0.6,0.7,1), nrow = 3)
rownames(R) <- colnames(R) <- Xnames #Apply names

# s = The standard deviations of variables in X
s <- c(15,15,15)

# k = The number of variables
k <- length(s)

# CM = Covariance Matrix
# Initialize k by k matrix of zeroes
CM <- matrix(0, k, k)
rownames(CM) <- colnames(CM) <- Xnames
for (i in seq(1, k)) {
  for (j in seq(1, k)) {
    CM[i, j] = s[i] * s[j] * R[i, j]
  }
}

CM
```

:::{.column-margin}
The $\mathtt{diag}$ function has three purposes. First, it can extract the diagonal vector from a matrix:
$$
A_{kk} =
 \begin{bmatrix}
  a_{11} & a_{12} & \cdots & a_{1k} \\
  a_{21} & a_{22} & \cdots & a_{2k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  a_{k1} & a_{k2} & \cdots & a_{kk}
 \end{bmatrix}
$$
Then 
$$\mathtt{diag}(\mathbf{A}) = \{a_{11},a_{22},...,a_{kk}\}$$
Second, the `diag` function inserts a vector into the the diagonal of a $k \times k$ matrix of zeros:
$$
\mathtt{diag}(\mathbf{a}) = 
\begin{bmatrix}
  a_{1} & 0 & \cdots & 0 \\
  0 & a_{2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & a_{k}
 \end{bmatrix}
$$
Third, the `diag` function converts integer $k$ into a $k \times k$ identity matrix:
$$
\mathtt{diag}(k) = 
\begin{bmatrix}
  1 & 0 & \cdots & 0 \\
  0 & 1 & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & 1
 \end{bmatrix}
$$
:::

To calculate the covariance matrix in matrix notation, we can make use of the $\mathtt{diag}$ function. 


$$
\begin{align*}
\mathbf{\Sigma}&=\mathtt{diag}(\mathbf{\sigma}) \mathbf{R}\, \mathtt{diag}(\mathbf{\sigma})\\ 
&=\begin{bmatrix}
  \sigma_{1} & 0 & \cdots & 0 \\
  0 & \sigma_{2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \sigma_{k}
 \end{bmatrix}
  \begin{bmatrix}
  1 & \rho_{12} & \cdots & \rho_{1k} \\
  \rho_{21} & 1 & \cdots & \rho_{2k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \rho_{k1} & \rho_{k2} & \cdots & 1
 \end{bmatrix}
\begin{bmatrix}
  \sigma_{1} & 0 & \cdots & 0 \\
  0 & \sigma_{2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \sigma_{k}
 \end{bmatrix}\\
  &=  \begin{bmatrix}
  \sigma_{1}^2 & \sigma_{1}\sigma_{2}\rho_{12} & \cdots & \sigma_{1}\sigma_{k}\rho_{1k} \\
  \sigma_{2}\sigma_{1}\rho_{21} & \sigma_{2}^2 & \cdots & \sigma_{2}\sigma_{k}\rho_{2k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k}\sigma_{1}\rho_{k1} & \sigma_{k}\sigma_{2}\rho_{k2} & \cdots & \sigma_{k}^2
 \end{bmatrix}\\
   &=  \begin{bmatrix}
  \sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1k} \\
  \sigma_{21} & \sigma_{2}^2 & \cdots & \sigma_{2k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k1} & \sigma_{k2} & \cdots & \sigma_{k}^2
 \end{bmatrix}
\end{align*}
$$

```{r diagcovtable}
tibble::tribble(
  ~Symbol,  ~Meaning,
  "$\\mathbf{\\Sigma}$", "The covariance matrix of variables in $\\mathbf{X}$",
  "$\\mathbf{\\sigma}$", "The standard deviations of variables in  $\\mathbf{X}$",
  "$\\mathbf{R}$", "The correlation matrix of variables in $\\mathbf{X}$") %>% 
  knitr::kable(escape = FALSE)
```

So this is where matrix algebra starts to shine. We do not need to initialize the variable that contains the covariance matrix, or calculate $k$, or do any looping. 

```{r CovarianceMatrix, echo=TRUE}
# CM = Covariance matrix of variables in X
CM <- diag(s) %*% R %*% diag(s)
```

Beautiful!

Running the code, we get:
$$
\mathbf{\Sigma}=`r bmatrix(CM)`
$$

To calculate the variance of $X_S$ if it is an unweighted sum we apply @eq-variancesum:

```{r CalculatingVarianceofComposite}
# var_S = The variance of X_S
var_S <- sum(CM)

# With matrix algebra
var_S <- t(ones) %*% CM %*% ones
#
```
Here we see that $\sigma_X^2=`r var_S`$

## Calculating the Variance of a Weighted Sum

If $X_S$ is a weighted sum, its variance is the weighted sum of the covariance matrix.


$$
\sigma_S^2=\sum_{i=1}^{k}{\sum_{j=1}^{k}{w_i w_j \sigma_{ij}}}=\mathbf{w'\Sigma w}
$$ {#eq-varianceweightedsum}

Continuing with the same variables in our example, we see that things are starting to become clumsy and ugly without matrix algebra:

```{r CalculatingVariance, echo=TRUE}
# First we initialize var_S as 0
var_S_weighted <- 0
# Now we loop through k rows and k columns of CM
for (i in seq(1, k)) {
  for (j in seq(1, k)) {
    var_S_weighted <- var_S_weighted + w[i] * w[j] * CM[i, j]
  }
}
```

With matrix algebra, this all happens with a single line of code:

```{r CalculatingVarianceWeightedSumMatrixAlgebra, echo=TRUE}
var_S_weighted <- t(w) %*% CM %*% w
```

If we don't need to know the covariance matrix, we can skip its calculation:

```{r CalculatingVarianceWeightedSumMatrixAlgebraDirectly, echo=TRUE}
var_S_weighted <- t(w * s) %*% R %*% (w * s)
```


All three methods give the same answer: $\sigma_X^2 = `r var_S_weighted`$

## Calculating a Composite Score

We now have all of the information needed to make a composite score. First we will make an unweighted composite.

\begin{equation}
C = \frac{x_S-\mu_S}{\sigma_{X_S}}\sigma_C + \mu_C
(\#eq:Composite)
\end{equation}

```{r compositesymbols}
tibble::tribble(
  ~Symbol,  ~Meaning,
  "$C$", "The composite variable $C$",
  "$x_S$", "The sum of all $k$ scores in $\\{x_1,...,x_k\\}$",
  "$\\sigma_{X_S}$", "The standard deviation of $X_S$",
  "$\\sigma_C$", "The standard deviation of $C$",
  "$\\mu_C$", "The mean of $C$") %>% 
  knitr::kable(escape = FALSE)
```

Continuing with our example, suppose that $\mu_C=100$ and $\sigma_C=15.$

```{r CompositeGeneral, echo=TRUE}
# m_C = Composite mean
m_C <- 100
# s_C = Composite standard deviation
s_C <- 15
# C = The composite score
C <- ((x_S - m_S) / sqrt(var_S)) * s_C + m_C
```

$$
C=`r round(C)`
$$

### Calculating a Weighted Composite Score

In matrix notation, an unweighted composite score is calculated like so:

$$
C=\mathbf{\frac{1'(x-\mu_x)}{\sqrt{1'\Sigma 1}}}\sigma_C+\mu_C
$$
Replacing each vector of ones $\mathbf{1}$ with a weight vector $\mathbf{w}$ gives us the formula for computing a weighted composite score:

$$
C_w=\mathbf{\frac{w'(x-\mu_x)}{\sqrt{w'\Sigma w}}}\sigma_C+\mu_C
$$

:::{.column-margin}
In clinical practice, the most common kind of composite score is an equally weighted composite consisting of scores with the same mean and standard deviation. Sometimes scores are weighted by the degree to which they correlate with the construct the composite is intended to measure. Other weighting schemes are also possible. In most cases, it makes sense to first convert all scores to z-scores and then weight the z-scores. Failing to convert the scores to a common metric such as z-scores will result in an implicit weighting by standard deviations. That is, the score with the largest standard deviation will have the most weight in the composite score. Converting to z-scores first will equalize the each score&rsquo;s influence on the composite score. We can convert all the scores in $\mathbf{x}$ to z-scores like so:
  
$$
\mathbf{z} = \frac{\mathbf{x}-\mathbf{\mu}}{\mathbf{\sigma}}
$$

The nice thing about z-scores is that their means are zeros, their standard deviations are ones, and their covariances are correlations. Thus, the formula for a weighted composite score consisting of z-scores is fairly simple, especially if the composite score is a z-score itself:
  
$$
C = \frac{\mathbf{w^\prime z}}{\sqrt{\mathbf{w^\prime Rw}}}\sigma_C+\mu_C
$$
:::


Continuing with our example, computing a composite score with $X_1=`r x[1]`$, $X_2=`r x[2]`$, and $X_3=`r x[3]`$:

```{r CalculateWeightedComposite, echo=TRUE}
# C = The composite score
C <- (t(w) %*% (x - m) / sqrt(t(w) %*% CM %*% w)) * 15  + 100

# With index scores, we round to the nearest integer
C <- round(C)

# Notes:
# The round function rounds to the nearest integer by default,
# but you can round to any number of digits you wish. For example, 
# rounding to 2 significant digits (i.e., the hundredths place), 
# would be round(x,2).
  
```

Here we see, that the weighted composite score $C = `r C`.$ If we had wanted an equally weighted composite score, the `w` vector would be set to equal ones. If we had done so, $C$ would have been $`r round(100 + sum(x - m) / sqrt(sum(R)))`$
instead of $`r C`.$

## The Reliability Coefficient of a Composite

Calculating the reliability coefficient of a composite score is much easier than it might appear at first. Remember that reliability is the ratio of a score's true score variance to its total variance:

$$
r_{XX} = \frac{\sigma_T^2}{\sigma_X^2}
$$

The variance of a composite score is the sum of the covariance matrix of the composite's component scores. 

$$
\sigma_X^2=\mathbf{1}'\begin{bmatrix}
  \sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1k} \\
  \sigma_{21} & \sigma_{2}^2 & \cdots & \sigma_{2 k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k1} & \sigma_{k2} & \cdots & \sigma_{k}^2
 \end{bmatrix}\mathbf{1}
$$

The covariance matrix of the component true scores is exactly the same as the covariance matrix of the component observed scores, except that the variances on the diagonal are multiplied by the reliability coefficients of each of the component scores like so:


$$
\sigma_T^2=\mathbf{1}'\begin{bmatrix}
  {\color{firebrick}r_{11}}\sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1k} \\
  \sigma_{21} & {\color{firebrick}r_{22}}\sigma_{2}^2 & \cdots & \sigma_{2 k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k1} & \sigma_{k2} & \cdots & {\color{firebrick}r_{kk}}\sigma_{k}^2
 \end{bmatrix}\mathbf{1}
$$

```{r CalculateReliability, echo=TRUE}
# r_XX = Reliability coefficients for X1, X2, and X3
r_XX <- c(0.88,0.80,0.76)

# CM_T = Covariance matrix of true scores
CM_T <- CM

# Replace diagonal with true score variances
diag(CM_T) <- diag(CM) * r_XX

# r_C = Reliability coefficient of composite score
r_C <- sum(CM_T) / sum(CM)
```

The reliability coefficient of the unweighted composite is `r round(r_C, 2)`.

For a weighted composite, the reliability is calculated by relplacing each vector of ones $\mathbf{1}$ with a weight vector $\mathbf{w}$: 

$$
\rho_{XX}=\frac{\sigma_T^2}
               {\sigma_X^2}
         =\frac{\mathbf{w}^{\prime}
               \begin{bmatrix}
  {\color{firebrick}r_{11}}\sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1k} \\
  \sigma_{21} & {\color{firebrick}r_{22}}\sigma_{2}^2 & \cdots & \sigma_{2 k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k1} & \sigma_{k2} & \cdots & {\color{firebrick}r_{kk}}\sigma_{k}^2
                \end{bmatrix}
                \mathbf{w}}
          {\mathbf{w}^{\prime}
               \begin{bmatrix}
  \sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1k} \\
  \sigma_{21} & \sigma_{2}^2 & \cdots & \sigma_{2 k} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \sigma_{k1} & \sigma_{k2} & \cdots & \sigma_{k}^2
 \end{bmatrix}\mathbf{w}}
$$

```{r weightedreliability, echo=TRUE}
# Reliability coefficient of the weighted composite
r_C_w <- t(w) %*% CM_T %*% w / t(w) %*% CM %*% w
```

The reliability coefficient of the weighted composite is `r round(r_C_w, 2)`.

## Composite Scores and Their Correlations

If the population correlations among all of the components are known, it is possible to calculate the correlations among composite scores made from these components. Such correlations can be used in many practical applications, including in prediction models and in the evaluation of difference scores.

Suppose that Composite $A$ is calculated from the sum of two component tests, $A_1$ and $A_2.$ Composite $B$ is calculated from the sum of $B_1$ and $B_2.$ Suppose that the correlation matrix for the four components is:



```{r}
factornames <- c("A", "B")
testnames <- WJSmisc::cross_vectors(factornames, 1:2)
factorname_colors <- c("royalblue","firebrick")
testname_colors <- rep(factorname_colors, c(2,2))


w <- c(1,1,0,0,
       0,0,1,1) %>% 
  matrix(nrow = 4, ncol = 2) %>% 
  `colnames<-`(factornames) %>% 
  `rownames<-`(testnames)

# cell colors
w_colors <- w * NA
w_colors[1:2,1] <- "royalblue"
w_colors[3:4,2] <- "firebrick"

w_latex <- WJSmisc::insert_latex_color(m = w,
                            color_cells = w_colors, 
                            color_rownames = testname_colors, 
                            color_colnames = factorname_colors) %>% 
  WJSmisc::latexarray(.) 

w_latex_nolabel <- w %>% 
  `dimnames<-`(NULL) %>% 
  WJSmisc::insert_latex_color(.,
                            color_cells = w_colors, 
                            color_rownames = testname_colors, 
                            color_colnames = factorname_colors) %>% 
  WJSmisc::latexarray(., left="[", right = "]")

R <- c(1, .30, .35, .40,
        .30, 1, .42, .48,
        .35, .42, 1, .56,
        .40, .48, .56, 1) %>%
  matrix(4) %>% 
  `rownames<-`(testnames) %>% 
  `colnames<-`(testnames)

R_colors <- diag(nrow(R)) * NA

R_colors[1:2, 1:2] <- "royalblue"
R_colors[3:4, 3:4] <- "firebrick"
R_colors[1:2, 3:4] <- "purple"
R_colors[3:4, 1:2] <- "purple"


R_latex <- 	R %>%
  apply(c(1,2), WJSmisc::prob_label) %>%
  WJSmisc::insert_latex_color(.,
    color_cells = R_colors,
    color_rownames = testname_colors,
    color_colnames = testname_colors
  ) %>%
  WJSmisc::latexarray(.) 


R_latex_nolabel <- 	R %>%
  `dimnames<-`(NULL) %>% 
  apply(c(1,2), WJSmisc::prob_label) %>%
  WJSmisc::insert_latex_color(.,
    color_cells = R_colors) %>%
  WJSmisc::latexarray(., left = "[", right = "]") 

sigma_AB <- t(w) %*% R %*% w
sigma_AB_colors <- matrix(c("royalblue", "purple", "purple", "firebrick"), 2)

sigma_AB_latex <- sigma_AB %>%
  apply(c(1,2), scales::number, accuracy = 0.01) %>%
  WJSmisc::insert_latex_color(sigma_AB_colors,
                              factorname_colors,
                              factorname_colors) %>%
  WJSmisc::latexarray(.)

sigma_AB_latex_nolabel <- sigma_AB %>%
  apply(c(1,2), scales::number, accuracy = 0.01) %>%
  WJSmisc::insert_latex_color(sigma_AB_colors) %>%
  WJSmisc::latexarray(., includenames = F, left="[", right = "]")

rows <- list(1:2, 3:4, 1:2, 3:4)
cols <- list(1:2, 1:2, 3:4, 3:4)

sigma_AB_step1 <- map2(rows, cols, function(r,c) {
  R[r,c] %>% 
    WJSmisc::prob_label() %>% 
    paste(collapse = " + ")
  }) %>% 
  matrix(2) %>% 
  `rownames<-`(factornames) %>% 
  `colnames<-`(factornames) %>% 
  WJSmisc::insert_latex_color(color_cells = sigma_AB_colors, factorname_colors, factorname_colors) %>% 
  WJSmisc::latexarray(.)

sigma_colors <- sigma_AB_colors
sigma_colors[1, 2] <- NA
sigma_colors[2, 1] <- NA
sigma_step1 <- sigma_AB %>%
  diag() %>% 
  scales::number(accuracy = 0.01) %>%
  WJSmisc::insert_latex_color(factorname_colors) %>%
  matrix %>% 
  WJSmisc::latexarray(., left="[", right = "]")

sigma_step2 <- sigma_AB %>%
  diag() %>% 
  diag() %>% 
  WJSmisc::prob_label() %>% 
  matrix(2) %>% 
  WJSmisc::insert_latex_color(sigma_colors) %>%
  WJSmisc::latexarray(., left="[", right = "]")

sigma_step3 <- sigma_AB %>%
  diag() %>% 
  diag() %>% 
  sqrt() %>% 
  WJSmisc::prob_label(accuracy = 0.0001) %>% 
  matrix(2) %>% 
  WJSmisc::insert_latex_color(sigma_colors) %>%
  WJSmisc::latexarray(., left="[", right = "]")

sigma_step4 <- sigma_AB %>%
  diag() %>% 
  sqrt() %>% 
  `^`(-1) %>% 
  diag() %>% 
  WJSmisc::prob_label(accuracy = 0.0001) %>% 
  matrix(2) %>% 
  WJSmisc::insert_latex_color(sigma_colors) %>%
  WJSmisc::latexarray(., left="[", right = "]")

R_AB <- sigma_AB %>% 
  cov2cor()

R_AB_nolabel <- R_AB %>% 
  apply(c(1,2), WJSmisc::prob_label) %>% 
  WJSmisc::insert_latex_color(sigma_AB_colors) %>% 
  WJSmisc::latexarray(includenames = F, left="[", right = "]")

```


$$
\mathbf{R} = `r R_latex`
$$

```{r, echo=TRUE}
# Make correlation matrix R
R <- matrix(c(1, .30, .35, .40,
              .30, 1, .42, .48,
              .35, .42, 1, .56,
              .40, .48, .56, 1),
            nrow = 4,
            ncol = 4)

# Display R
R
```



The correlation between Composite $A$ and Composite $B$ is calculating by adding up the numbers is all three shaded regions of the correlation matrix and then dividing the sum of "between" correlations in purple by the geometric mean of the sums from the "within" correlations in the blue and red regions in the correlation matrix $\mathbf{R}$ like so:

$$r_{AB}=\dfrac{\color{purple}{\text{Sum of Correlations between A and B}}}{\sqrt{\color{royalblue}{\text{Sum of Correlations Within A}}\times\color{firebrick}{{\text{Sum of Correlations Within B}}}}}$$

To calculate these sums using matrix algebra, we first construct a "weight matrix" $\mathbf{W}$ that tells us which tests are in which composite. The 1s in the first two rows of column 1 tell us that tests $A_1$ and $A_2$ belong to composite $A$. Likewise, the 1s in the last two rows of column 2 tell us that $B_1$ and $B_2$ belong to composite $B$.

$$
\mathbf{W}=
`r w_latex` 
$$

Here is one way to make the weight matrix in R:

```{r weight matrix, echo=TRUE}
# Make a 4 by 2 matrix of 0s
W <- matrix(0, nrow = 4, ncol = 2)

# Assign 1s to rows 1 and 2 to column 1
W[1:2,1] <- 1
# Assign 1s to rows 3 and 4 to column 2
W[3:4,2] <- 1

# Display W
W
```


```{r}


```

The covariance matrix $\mathbf{\Sigma_{AB}}$ is calculated by pre- and post-mulitplying $\mathbf{R}$ by the wieght matrix \mathbf{W}.

$$
\begin{aligned}\mathbf{\Sigma_{AB}} &= \mathbf{W'RW}\\[2ex] 
&=`r w_latex_nolabel`'`r R_latex_nolabel``r w_latex_nolabel`\\[2ex]
&=`r sigma_AB_step1`\\[2ex]
&=`r sigma_AB_latex`
\end{aligned}
$$

```{r, echo=TRUE}
# Covariance matrix of composites A and B
Sigma_AB <- t(W) %*% R %*% W

# Display Sigma_AB
Sigma_AB
```



Now we need to extract the variance diagonal from the covariance matrix so that we can use them to convert the covariance matrix to a correlation matrix. The variances are put on a diagonal matrix, and then taking the square root converts the variances to standard deviations in matrix $\mathbf{\sigma}$. 

$$
\begin{aligned}
\mathbf{\sigma}
&=\mathtt{diag}(\mathtt{diag}(\mathbf{\Sigma_{AB}}))^{\frac{1}{2}}\\
&=\mathtt{diag}\left(
\mathtt{diag}\left({
`r sigma_AB_latex_nolabel`
   }\right)
   \right)^{\frac{1}{2}}\\
   &=\mathtt{diag}\left(
   `r sigma_step1`
   \right)^{\frac{1}{2}}\\
   &=`r sigma_step2`^{\frac{1}{2}}\\
   &=`r sigma_step3`
\end{aligned}
$$

```{r, echo=TRUE}
# Standard deviations
sigma <- diag(diag(Sigma_AB)^(0.5))

# Display sigma
sigma
```


Pre- and post-multiplying the covariance matrix $\mathbf{\Sigma_{AB}}$ by the inverted standard devation matrix $\mathbf{\sigma}$ to yield the correlations between composites $A$ and $B$.

$$
\begin{aligned}
\mathbf{R_{AB}}&=\mathbf{\sigma}^{-1}\mathbf{\Sigma_{AB}}\mathbf{\sigma}^{-1}\\[2ex]
&={\mathbf{\sigma}^{-1} \atop `r sigma_step3`^{-1}}
   {\mathbf{\Sigma_{AB}} \atop `r sigma_AB_latex_nolabel`}
   {\mathbf{\sigma}^{-1} \atop `r sigma_step3`^{-1}}\\[1ex]
&=`r sigma_step4`
   `r sigma_AB_latex_nolabel`
   `r sigma_step4`\\[1ex]
&=`r R_AB_nolabel`
\end{aligned}
$$

:::{.column-margin}
The `solve` function inverts a square matrix. Inverting a single number $x$ gives its reciprocal. If you multiply the inverse of a number by the nunbe itself, you get 1, the *identity* for the multiplication operator. $x^{-1}x=1$, multiplying a matrix by its inverse creates an identity matrix (a matrix with ones on the diagonal and zeroes elsewhere):
  
$$\mathbf{A^{-1}A = I}$$
:::

```{r compositecor, echo=T}
# Correlations between composites A and B
R_AB <- solve(sigma) %*% Sigma_AB %*% solve(sigma)
R_AB
```


These calculations in R can be greatly simplified using the `cov2cor` function, which converts covariances to correlations:

```{r, echo=TRUE}
# Correlation matrix of composites A and B
R_AB <- cov2cor(t(W) %*% R %*% W)

R_AB
```

