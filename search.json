[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Individual Psychometrics",
    "section": "",
    "text": "Preface\nI have ambitious goals for this book, but it is not nearly complete. I have been working on it off and on since 2012. It is accompanied by the R package psycheval (Schneider, 2023), which is also in a preliminary state of development.\n\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\n\n\n\nMost of the figures for this book were created in R (via ggplot2 and base R), \\small\\LaTeX (via TikZ), or Observable. To make the content as accessible and transparent as possible, I have included buttons that will reveal the code used to make each figure or table. For example, the initial setup code used for this book can be seen by expanding the note below:\n\n\n\n\n\n\n\nR Code\n\n\n\n\n\n# Load packages\nlibrary(conflicted)\nconflicts_prefer(dplyr::select, \n                 dplyr::filter, \n                 scales::alpha, \n                 dplyr::lag,\n                 tibble::add_case)\nlibrary(extrafont)\nloadfonts(\"win\", quiet = TRUE)\n# library(tufte)\nlibrary(knitr)\nlibrary(sn)\nlibrary(fMultivar)\nlibrary(IDPmisc)\nlibrary(psych)\nlibrary(tidyverse)\nlibrary(scales)\n\nlibrary(gganimate)\nlibrary(ggforce)\nlibrary(sjmisc)\nlibrary(WJSmisc)\n# library(tippy)\nlibrary(tikzDevice)\nlibrary(patchwork)\nlibrary(qualvar)\nlibrary(modeest)\nlibrary(tinter)\nlibrary(ggfx)\nlibrary(ggtext)\nlibrary(lemon)\nlibrary(signs)\n\nlibrary(psycheval)\nlibrary(bezier)\nlibrary(DescTools)\nlibrary(ggh4x)\nlibrary(ggthemes)\nlibrary(rsvg)\nlibrary(ggarrow)\nlibrary(arrowheadr)\narrow_head_deltoid &lt;- purrr::partial(arrowheadr::arrow_head_deltoid, d = 2.3)\n# Set options\noptions(knitr.kable.digits = 2, knitr.kable.na = '')\nknitr::opts_template$set(marginfigure = list(fig.column = \"margin\", fig.cap.location = \"top\", out.width = \"100%\", fig.align = \"left\"),\n                         bodyfigure = list(fig.column = \"body\", fig.cap.location = \"margin\"))\n\n# Default fonts and colors\nbfont = \"Equity Text A Tab\"\nbsize = 16\nmyfills &lt;- c(\"royalblue4\", \"firebrick4\", \"#51315E\")\n\n# Function for converting base size to geom_text size\nggtext_size &lt;- function(base_size, ratio = 0.8) {\n  ratio * base_size / ggplot2:::.pt\n}\n\nbtxt_size = ggtext_size(bsize)\n\n# Default geoms and themes\nggplot2::update_geom_defaults(\"text\",\n                              list(family = bfont, size = btxt_size))\nggplot2::update_geom_defaults(\"label\",\n                              list(\n                                family = bfont,\n                                size = btxt_size,\n                                label.padding = unit(0, \"lines\"),\n                                label.size = 0\n                              ))\n\nggplot2::update_geom_defaults(\"richtext\",\n                              list(family = bfont, size = btxt_size))\n\nggplot2::update_geom_defaults(\"density\", list(fill = myfills[1]))\ngeom_text_fill &lt;- function(...) {\n  geom_label(...,\n             label.padding = unit(0, \"lines\"),\n             label.size = 0)\n}\ntheme_set(theme_minimal(base_size = bsize, base_family = bfont))\n\n\n# font family\nspan_style &lt;- function(x, style = \"font-family:serif\") {\n  paste0('&lt;span style=\\\"',\n         style,\n         '\\\"&gt;',\n         x,\n         \"&lt;/span&gt;\")\n}\n\n# Probability labels\nprob_label &lt;- function(p,\n                       accuracy = 0.01,\n                       digits = NULL,\n                       max_digits = NULL,\n                       remove_leading_zero = TRUE,\n                       round_zero_one = TRUE) {\n  if (is.null(digits)) {\n    l &lt;- scales::number(p, accuracy = accuracy)\n  } else {\n    sig_digits &lt;- abs(ceiling(log10(p + p / 1000000000)) - digits)\n    sig_digits[p &gt; 0.99] &lt;- abs(ceiling(log10(1 - p[p &gt; 0.99])) - digits + 1)\n    sig_digits[(ceiling(log10(p)) == log10(p)) & (-log10(p) &gt;= digits)] &lt;- sig_digits[ceiling(log10(p)) == log10(p)] - 1\n    sig_digits[is.infinite(sig_digits)] &lt;- 0\n    l &lt;- purrr::map2_chr(p,\n                         sig_digits,\n                         formatC,\n                         format = \"f\",\n                         flag = \"#\")\n\n  }\n  if (remove_leading_zero) l &lt;- sub(\"^-0\",\"-\", sub(\"^0\",\"\", l))\n\n  if (round_zero_one) {\n    l[p == 0] &lt;- \"0\"\n    l[p == 1] &lt;- \"1\"\n    l[p == -1] &lt;- \"-1\"\n  }\n\n  if (!is.null(max_digits)) {\n    if (round_zero_one) {\n      l[round(p, digits = max_digits) == 0] &lt;- \"0\"\n      l[round(p, digits = max_digits) == 1] &lt;- \"1\"\n      l[round(p, digits = max_digits) == -1] &lt;- \"-1\"\n    } else {\n      l[round(p, digits = max_digits) == 0] &lt;- paste0(\".\", paste0(rep(\"0\", max_digits), collapse = \"\"))\n      l[round(p, digits = max_digits) == 1] &lt;- paste0(\"1.\", paste0(rep(\"0\", max_digits), collapse = \"\"))\n      l[round(p, digits = max_digits) == -1] &lt;- paste0(\"-1.\", paste0(rep(\"0\", max_digits), collapse = \"\"))\n    }\n  }\n\n  dim(l) &lt;- dim(p)\n  l\n}\n\n# Set table column width\n# https://github.com/rstudio/bookdown/issues/122#issuecomment-221101375\nhtml_table_width &lt;- function(kable_output, width, tag = \"&lt;/caption&gt;\"){\n  width_html &lt;- paste0(\n    paste0('&lt;col width=\"',\n           width,\n           '\"&gt;'),\n    collapse = \"\\n\")\n  sub(tag,\n      paste0(tag,\n             \"\\n\",\n             width_html),\n      kable_output)\n}\n\n# Make a matrix with braces\nbmatrix &lt;- function(M, brace = \"bmatrix\", includenames=TRUE) {\n  if (includenames) {\n    M &lt;- cbind(rownames(M),M)\n    M &lt;- rbind(colnames(M), M)\n  }\n  M &lt;-  paste(apply(M,\n                    MARGIN = 1,\n                    FUN = paste0,\n                    collapse = \" & \"),\n              collapse = \"\\\\\\\\\\n\")\n\n\n  if (!is.null(brace)) {\n    M &lt;- paste0(\"\\\\begin{\",brace,\"}\\n\", M, \"\\n\\\\end{\", brace , \"}\")\n    }\n  M\n}\n\n# Hooks -------\n\n# # Enclose collapsible r chunk in  button\n# knitr::opts_hooks$set(button_r = function(options) {\n#   if (isTRUE(options$button_r)) {\n#     options$button_before_r &lt;- TRUE\n#     options$button_after &lt;- TRUE\n#     options$echo = TRUE\n#     options$eval = FALSE\n#   }\n# \n#   options\n# })\n# \n# # Enclose collapsible latex chunk in  button\n# knitr::opts_hooks$set(button_latex = function(options) {\n#   if (isTRUE(options$button_latex)) {\n#     options$button_before_latex &lt;- TRUE\n#     options$button_after &lt;- TRUE\n#     options$echo = TRUE\n#     options$eval = FALSE\n#   }\n# \n#   options\n# })\n\n# before button for collapsible r chunk\nknit_hooks$set(\n  button_before = function(before, options, envir) {\n    if (before) {\n      if (is.null(options$figlabel)) {\n        l &lt;- options$label %&gt;% \n          str_remove(\"^coder\\\\-\") %&gt;% \n          str_remove(\"^codelatex\\\\-\") %&gt;% \n          str_remove(\"^codeojs\\\\-\") \n        if (str_detect(l, \"^fig\\\\-\") | str_detect(l, \"^tbl\\\\-\")) {\n          options$figlabel &lt;- l\n        }\n        \n      } \n      \n      codetype &lt;- options$codelabel\n      if (!is.null(options$figlabel)) {\n        codetype &lt;- paste0(codetype, \" for @\", options$figlabel)\n      } \n      return(\n        paste0(\n          # '&lt;div class=\"wrap-collapsible\" style=\"margin-top: 1em\"&gt;',\n          # \"\\n\",\n          # '&lt;input id=\"collapsible-',\n          # options$label,\n          # '\" class=\"toggle\" type=\"checkbox\"&gt;',\n          # \"\\n\",\n          # '&lt;label for=\"collapsible-',\n          # options$label,\n          # '\" class=\"lbl-toggle\"&gt;', codetype,'&lt;/label&gt;',\n          # '&lt;div class=\"collapsible-content\"&gt;',\n          # \"\\n\",\n          # '&lt;div class=\"content-inner\"&gt;'\n          ':::{.callout-note collapse=\"true\" appearance=\"minimal\"}\\n## ',codetype\n        )\n      )\n    }\n  }\n)\n\n\n# After button for collapsible chunks\nknit_hooks$set(button_after = function(before, options, envir) {\n  # if (!before) return('&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;')\n  if (!before) return('\\n:::\\n')\n})\n\n\n# Solution chunk\n# knitr::opts_hooks$set(solution = function(options) {\n#   options$echo &lt;- TRUE\n#   options$solutionsetter &lt;- TRUE\n#   return(options)\n# })\n\nknitr::knit_hooks$set(solutionsetter = function(before,options, envir) {\n  \n  if (before) {\n    \n    \"\\n\\n&lt;details&gt;&lt;summary&gt;Suggested Solution&lt;/summary&gt;\\n\\n\"\n  } else {\n    \n    \"\\n\\n&lt;/details&gt;\\n\\n\"\n  }\n})\n\n\n# Make all chunks with demo-prefix echo = TRUE\n\nknitr::opts_hooks$set(label = function(options) {\n  if (startsWith(options$label, \"demo-\")) {\n    options$echo &lt;- TRUE\n  }\n  if (startsWith(options$label, \"solution-\")) {\n    options$echo &lt;- TRUE\n    options$solutionsetter &lt;- TRUE\n  }\n  if (str_starts(options$label, \"code\")) {\n    options$button_before &lt;- TRUE\n    options$button_after &lt;- TRUE\n    options$echo = TRUE\n    options$eval = FALSE\n    codelanguages &lt;- c(r = \"R Code\", \n                       latex = \"$\\\\small\\\\rm\\\\LaTeX$ Code\",\n                       ojs = \"Observable Code\")\n    mycode &lt;- str_match(options$label, \"^code(.*?)\\\\-\")\n    if (length(mycode) == 2) {\n      options$codelabel = codelanguages[mycode[2]]\n    }\n  }\n  return(options)\n})\n\n\n\n\nIn addition, all the files and code used to create this book can be found in its Github repository.\nTo avoid repeated citation, I must note that in preparing this book, I have drawn heavily—and no doubt unconsciously—from many authoritative sources on psychometrics, statistical analysis, and linear algebra (Cohen et al., 2003; Crocker & Algina, 2006; Eaton, 2007; Furr, 2017; Nunnally, 1967; Raykov & Marcoulides, 2011; Strang, 2016). I also owe a debt of gratitude to the many unsung authors at Wikipedia and Mathematica who maintain wonderfully comprehensive, up-to-date, and well-referenced documentation of all things mathematical and statistical.\n\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences. L. Erlbaum Associates.\n\nCrocker, L., & Algina, J. (2006). Introduction to classical and modern test theory. Cengage Learning.\n\nEaton, M. L. (2007). Multivariate statistics: A vector space approach. Inst. of Mathematical Statistics.\n\nFurr, R. (2017). Psychometrics: An introduction (3rd ed.). Sage.\n\nNunnally, J. C. (1967). Psychometric theory. McGraw-Hill.\n\nRaykov, T., & Marcoulides, G. A. (2011). Introduction to psychometric theory. Routledge.\n\nStrang, G. (2016). Introduction to linear algebra (5th edition). Cambridge press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Purpose of this book\nMost introductory psychometrics textbooks are designed to help researchers create well constructed tests and therefore cover many details that are not useful to clinicians and fail to cover many practical issues that clinicians should know about. This book is intended to help you extract useful information from the data you already have in ways that you may not have known were possible.\nThat my emphasis is on the practical in no way implies that this book is dumbed down. My aim is to make psychometrics useful to clinicians. If some useful ideas are complex, I hope to make them accessible—but without resorting to superficial glossing. Some background knowledge of psychometrics is necessary to understand how these tools work and, more importantly, when their underlying assumptions have been violated.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#inspiration",
    "href": "intro.html#inspiration",
    "title": "1  Introduction",
    "section": "\n1.2 Inspiration",
    "text": "1.2 Inspiration\n\n\n\n\n\n\n\n\nFigure 1.1: Philip Ley (1933–2015)Image Credit\n\n\n\nWhenever I go to a library, I find the book I need and then run my fingers over the books nearby that might also be interesting. Sometimes I see what I expect to find, but sometimes I go places I never expected. I like the idea of “wasting time” productively. I like learning things that seem to be the product of a disciplined mind even if they have no apparent value to me at the time. Casting a wide net can pull in some big surprises.\nThis book would never have been written had I not in graduate school stumbled across Ley’s (1972) Quantitative aspects of psychological assessment while wandering the stacks at Evans library at Texas A&M University. The book was a little quirky, but I like quirky. I especially admire the book’s blend of clarity, practicality, and depth. Why did I write my own book instead of recommending that clinicians download and use Ley’s now freely available book? Well, I do recommend reading Ley’s book. In contrast to my approach, Ley often takes time to gently lay out mathematical proofs of many ideas. Thus Ley’s book is a wonderful and beginner-friendly introduction to the not-famous-for-being-accessible corpus of academic writings on psychometrics.\n\nLey, P. (1972). Quantitative aspects of psychological assessment: An introduction. Duckworth.\n\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nI wanted to present much of the same material as Ley does but with more of an eye to application. I also wanted to present many ideas not included in Ley’s book. In addition, I chose to write this book because I believe that Ley had the right idea but that in an era in which no one had a home computer, few clinicians would have the knowledge, motivation, and stamina to use his equations on a regular basis. Now that computers are used by all clinicians, equations like those presented by Ley can be be made easy to use. All of the “tools” in this toolkit are freely available in various packages programming environment (R Core Team, 2024). Any tools not elsewhere available have been collected in psycheval (Schneider, 2023), a package specifically developed for users of this book.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "gettingready.html",
    "href": "gettingready.html",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "",
    "text": "2.1 Get Ready for a Challenge\nFor R to pay off, you have to buy in. Learning challenging material does not have to be “fun” at every moment to be worthwhile. The beginning data analyst, assailed by statistical concepts, coding conventions, and baffling error messages, feels it all as one great blooming, buzzing confusion.\nThe challenge of learning R is indeed going to be challenging at times, but just there will also be many little a-ha moments, several larger check-out-what-I-can-do celebrations, and a few peak experiences of eudaemonic reverie.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#get-ready-for-a-challenge",
    "href": "gettingready.html#get-ready-for-a-challenge",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "",
    "text": "Figure 2.1: “Blooming, buzzing confusion?” I see what you did there. —WJ",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#what-is-r",
    "href": "gettingready.html#what-is-r",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.2 What is R?",
    "text": "2.2 What is R?\n\n\n\n\n\n\nFigure 2.2: John Chambers\n\n\n\n\n\n\nFigure 2.3: Robert Gentleman\n\n\n\n\n\n\nFigure 2.4: Ross Ihaka\n\n\n\nR is the eighteenth letter of the alphabet, of course, but for data analysts, it has several specialized meanings.\n\n\nR is a programming language built for statistics. In 1976, John Chambers and colleagues at Bell Labs developed S, a programming language specifically designed to facilitate statistical analyses and data visualization. In 1995, Robert Gentleman and Ross Ihaka released an open-source variant of S and named it “R” after their shared first initial. Most S code looks identical to R code, but R has a number of subtle enhancements such as better memory management. You can read more about the origins of R in Ihaka (1998).\n\nR is a computer program that runs code in the programming language R. The primary way that the R language is used is to perform analyses in the R software environment for statistical computing and graphics. This program is free and open source, meaning that not only can anyone download it for free, but its source code can be reused by anyone for any purpose.\n\nR is a data science ecosystem in which the R community flourishes. Strictly speaking, R is just one program. However, people use R in startlingly creative ways, often in combination with other programs and in coordination with other people. I like to think of R as the entire ecosystem of software, services, standards, and cultural practices shared by the entire community of R users.\n\n\nIhaka, R. (1998). R: Past and future history. Interface 98. https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf\nThe technical merits of the R language and software do matter, but the success of R has more to do with the inclusive, welcoming, diverse, and expanding culture of the R community. When an individual R user has a great idea, the R community has as integrated set of technical standards, web services, and cultural practices such that innovations spread quickly to everyone in the community—often with multiple tutorials aimed at users of all levels of expertise. With the support of the R community, ordinary people can leverage R to accomplish far more than anyone would have thought possible not so very long ago.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#what-can-you-do-with-r",
    "href": "gettingready.html#what-can-you-do-with-r",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.3 What can you do with R?",
    "text": "2.3 What can you do with R?\nWhat can’t be done with R is an ever-contracting list. If you let it, R will help you in ways you did not know you needed it. You can (and should) use R for tasks that extend well beyond analyzing data. Because R plays well with many other programs, you can leverage its capacities to do all sorts of things:\n\nManage, analyze, and visualize data\nControl and run other programs\nWrite and publish books, papers, and posters\nAutomate reports and customized letters\nCreate slide presentations and interactive tutorials\nMake websites and blogs\nDesign web applications and dashboards\nMake music and digital aRt\n\nFor example, this book was “created in R.” That is, I wrote the text in a simple format and then let R (and RStudio) handle all the complex and tedious coordination of many other programs to make this webpage. It does most of its work under the hood without my awareness.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#what-is-rstudio",
    "href": "gettingready.html#what-is-rstudio",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.4 What is RStudio?",
    "text": "2.4 What is RStudio?\nMost users never actually open the R program directly. Why not? R’s text editor works well for what it does, but it has limited functionality compared to other ways of interacting with R. Instead, we will run R code in an integrated development environment (IDE) called RStudio. An IDE is a program that makes programming easier. RStudio has many features that automates tasks what would otherwise be confusing, difficult, or tedious.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#code-clicks",
    "href": "gettingready.html#code-clicks",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.5 Code > Clicks",
    "text": "2.5 Code &gt; Clicks\n\n\n\n\n\nArtwork by @allison_horst\n\n\n\nFigure 2.5: Frustrations with R are normal. Working through them pays off.\n\nThough R will be frustrating at times, stick with it—the dividends on your invested efforts are huge. Nothing beats the flexibility, freedom, and power of using code. Not only can I do things much faster than I could with point-and-click programs, I can do things I never would have considered doing. If there are no functions that do exactly what you want, you can adapt or make one that fits your needs perfectly. If you need to to re-run your analysis with a point-and-click menu interface, you need a perfect memory of what you did before. With code, every step is perfectly preserved.\nThat said, if you need to complete a simple data-analysis task quickly, and you have no time to learn R, I recommend using Jamovi. Jamovi has a beautiful point-and-click interface that you can learn to use in just a few minutes. Jamovi uses R under the hood, and like R, it is free.\n\n\nJamovi: A point-and-click alternative to R and RStudio\n\nThe ease of use of a point-and-click interface has a trade-off. If you never learn to code, you will be stuck with a limited set of skills, and many tasks will be prohibitively complicated or dauntingly tedious. In Figure 2.6 I recreate a plot made by many others before me.\n\n\n\n\n\n\n\nFigure 2.6: Different Learning Curves for Different Tasks\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 2.6\n\n\n\n\n\ntibble(\n  Complexity = seq(0, 100, 0.01),\n  Code = 10.5 * Complexity ^ (1 / 3.1),\n  `Point-and-Click` = 0.0002 * Complexity ^ 3.1) %&gt;%\n  gather(Approach, Difficulty,-Complexity) %&gt;%\n  mutate(\n    rotation =  180 * atan((Difficulty - lag(Difficulty)) / 0.01) / pi) %&gt;%\n  filter(Difficulty &lt;= 100) %&gt;%\n  ggplot(aes(Complexity, Difficulty, color = Approach)) +\n  geom_arrow(arrow_head = arrow_head_deltoid()) +\n  ggtext::geom_richtext(\n    data = . %&gt;% filter((Complexity == 62.5 & Approach == \"Point-and-Click\") | (Complexity == 75 & Approach == \"Code\")),\n    aes(label = Approach, angle = rotation),\n    vjust = -0.2,\n    hjust = 0.5,\n    size = 6,\n    fill = \"white\",\n    label.color = NA,\n    label.padding = grid::unit(rep(0, 4), \"pt\")\n  ) +\n  annotate(geom = \"richtext\", label = \"Point-and-click&lt;br&gt;programs are great&lt;br&gt;for simple tasks but&lt;br&gt;can be tedious for&lt;br&gt;complex projects\", x = 83, y = 87.5, hjust = 0.5, vjust = .5, color = myfills[2], label.color = NA) +\n  annotate(geom = \"richtext\", label = \"...but it makes complex&lt;br&gt;tasks much easier.\", x = 75, y = (37.5 + 25) / 2, hjust = 0.5, vjust = .5, color = myfills[1], label.color = NA) +\n  annotate(geom = \"richtext\", label = \"Coding has a steep&lt;br&gt;learning curve...\", x = 12.5, y = 37.5, hjust = 0.5, vjust = .5, color = myfills[1], label.color = NA) +\n  coord_fixed() +\n  scale_x_continuous(\"Task Complexity\", limits = c(0, 100), expand = expansion(.02)) +\n  scale_y_continuous(\"Task Difficulty\", limits = c(0, 100), expand = expansion(.02)) +\n  scale_color_manual(values = myfills) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#installing-r",
    "href": "gettingready.html#installing-r",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.6 Installing R",
    "text": "2.6 Installing R\nInstallation depends on your operating system. If you have already installed R, make sure you have the latest version.\nGo to the CRAN (Comprehensive R Archive Network) site. Click the link that is appropriate for your operating system (Windows, Mac, or Linux) and follow the instructions.\n\n2.6.1 Windows\nClick here and download the latest version of R for Windows. Open the file and follow installation instructions.\n\n2.6.2 Mac\nClick here and download the latest version of R for Mac. Double-click the file and follow installation instructions.]\n\n2.6.3 Linux\nInstall R on Ubuntu with this Bash script:]{.instruction}\n\n\n\nMore detailed instructions here:\n\n\nDebian.\n\nFedora/Redhat.\n\nUbuntu.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#installing-rstudio",
    "href": "gettingready.html#installing-rstudio",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.7 Installing RStudio",
    "text": "2.7 Installing RStudio\n\n2.7.1 RStudio Desktop\nRStudio is free. Please do not pay for anything. The paid versions of RStudio have nothing you will need for this course. They are not better than the free version. They simply have features tailored to businesses and developers.\nInstall the free version of RStudio Desktop here.\n\n2.7.2 RStudio Cloud\nOne alternative to installing R and RStudio on your machine is to use RStudio in a web browser in Posit Cloud. After signing up for a free account, you can use RStudio online. As the internet speeds up, I imagine that this option will become increasingly attractive.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#recommended-rstudio-customization",
    "href": "gettingready.html#recommended-rstudio-customization",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.8 Recommended RStudio Customization",
    "text": "2.8 Recommended RStudio Customization\nTo customize RStudio, click the Tools menu, then Global Options.\n\n\nGlobal Options Menu\n\nI strongly recommend changing the default data saving behavior to never restoring old sessions and never saving data in memory. Old variables lurking unnoticed in memory cause mystifying errors and baffling behavior. Starting each session fresh will prevent hours and hours of frustrating debugging.\n\n\nGeneral Options\n\nIf you use a dark theme, you might have less eye strain. Up to you, though.\nI change my code theme from time to time, but for some reason I always return to the Tomorrow Night 80s theme.\nFor teaching, I use Consolas. For myself, I use Fira Code. It uses font ligatures to combine symbols in a pleasing manner.\n\n\nAppearance Options\n\nA complete list of options explained here.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#using-rstudio",
    "href": "gettingready.html#using-rstudio",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.9 Using RStudio",
    "text": "2.9 Using RStudio\nOpen RStudio the same way you would any other program in your operating system.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#a-quick-tour-of-rstudios-4-panes",
    "href": "gettingready.html#a-quick-tour-of-rstudios-4-panes",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.10 A Quick Tour of RStudio’s 4 Panes",
    "text": "2.10 A Quick Tour of RStudio’s 4 Panes\nRStudio may appear overwhelming at first. That feeling will fade quickly with use. By default, RStudio has 4 panes where you can interact with R in different ways.\n\n\nQuick Tour of RStudio\n\n\n2.10.1 The Console\nThe console (lower left) is where you submit quick temporary calculations and run code you have no intention of saving. Hit Enter to submit code to R. R is interactive in the sense that it will display the result of the code in the console where you typed.\n\n2.10.2 Scripts\nThe script pane (upper left) is for writing code you want to preserve in a script file (.R) or RMarkdown file (.Rmd). More on RMarkdown later. Save your script files frequently.\nYou can submit code to the console by hitting Ctrl+Enter (or ⌘+Enter on Macs). You can also submit code by hitting the Run button. If you have code selected, only the selected part will be submitted. If you have no code selected, the current line (wherever the cursor is) will be submitted.\n\n2.10.3 Environment Variables\nIn the Environment tab in the upper right pane, you can see which variables have been created and a preview of what they contain.\n\n2.10.4 Viewer\nThe lower right pane has several tabs:\n\n\nFiles: Interact with project files\n\nPlots: Preview plots\n\nPackages: Install and update packages\n\nHelp: Search for help\n\nViewer: View documents created by RStudio",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#installing-and-updating-packages",
    "href": "gettingready.html#installing-and-updating-packages",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.11 Installing and Updating Packages",
    "text": "2.11 Installing and Updating Packages\nWhat comes installed with R is called “Base R,” which by itself is quite powerful. However, what makes R especially great is the fact that tens of thousands of free “packages” are available in a central repository called CRAN. These packages extend what R can do far beyond what any one person could have imagined.\nIn the bottom right pane, click the Packages tab.\n\n\nR Packages\n\nAny new package you need but do not have, you can click the Install button and type the name of a new package. For example, click Install button, type tidyverse in the Packages textbox, and click Install.\nYou might have to wait a while, but now many new packages have been installed on your machine.\nYou do not need to install packages again. However, I update all my packages each time I use RStudio to make sure I have the best and most up-to-date versions of the packages available.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#create-a-new-project",
    "href": "gettingready.html#create-a-new-project",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.12 Create a New Project",
    "text": "2.12 Create a New Project\nWhen you take on a new data-analysis task, you will want to keep your files organized in a particular folder. You want to make sure that this folder is the first place that R will look when you specify a file name. Otherwise we will have to write out the entire file name, which will be likely different if we need to run the same analysis on a different computer.\nRStudio’s “projects” are useful for making sure that all your analyses are in the same folder and that your analyses will work no matter which machine runs it.\nWithout projects, we would need to write out the entire file name like so:\n\nread_csv(\"C:/Users/my_user_name/Research/My_Project/my_data.csv\")\n\nWith projects, we can import data with just the bare file name, like so:\n\nread_csv(\"my_data.csv\")\n\nMuch better!\nOkay, let’s get to it.\n\n\n\n\n\n\nSteps to create a project\n\n\n\n\nIn your operating system, create a folder on your computer with a name that describes what the project is about.\nOpen RStudio.\nClick File-&gt;New Project\n\nSelect Existing Directory\n\nClick Browse\n\nNavigate to the new folder.\nClick Open\n\nClick Create Project",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#be-kind-to-future-you",
    "href": "gettingready.html#be-kind-to-future-you",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.13 Be Kind to Future-You",
    "text": "2.13 Be Kind to Future-You\nHadley Wickham reminds us that the most important person to write code for is future-you.\nAt all times, work with the assumption that 10 minutes from now, you will suffer catastrophic datamnesia—memory failure for everything related to your analysis.\n\n\n\nDatamnesia happens all the time.\n\n\nPresent-you thinks future-you has god-like memory capacity. Future-you has very human and very fallible memory. Predicting is hard, especially about the future and what future-you will remember.\nFuture-you spends an inordinate amount of time recreating what present-you thought future-you would understand and remember—so much time that future-you neglects to take care of future–future-you. Future-you won’t take care of anything unless present-you learns to take care of future-you.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#folder-organization",
    "href": "gettingready.html#folder-organization",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.14 Folder Organization",
    "text": "2.14 Folder Organization\n\nBad: All files on your desktop\nAlso Bad: All files in the default folder (e.g., My Documents)\nWorse: Important files scattered in various folders with no organization.\nEven worse: Important files are in your Downloads folder where they are likely to be overwritten or deleted.\nGood: Use a file synchronization service like Dropbox, Box, OneDrive, GoogleDrive, etc.\nAlso Good: Within your file synchronization service, have a well-organized hierarchical structure to your folders.\nEven Better: Use version control software like Git, and save the code in an online repository like GitHub.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "gettingready.html#file-name-guidance",
    "href": "gettingready.html#file-name-guidance",
    "title": "\n2  Getting Ready to Use R via RStudio\n",
    "section": "\n2.15 File Name Guidance",
    "text": "2.15 File Name Guidance\nGiving convenient, easily understood names to files, whether data files, image files, or script files can have far-reaching consequences for both present-you and future-you. I draw heavily from the advice on naming things from Jenny Bryan.\n\n2.15.1 Give your files long, descriptive names.\nMake it clear to future-you exactly what is in the file.\n\n\nBad: data.xlsx\n\n\nBetter: dissertation_data.csv\n\n\nEven Better: student_questionnaire_time_1.csv\n\n\n2.15.2 Separate words with underscores, not spaces.\nReplace spaces with underscores (_). Spaces often work fine, but sometimes they do not, which can result in hours and hours of debugging. Play it safe and don’t use spaces in file names.\nstudent scores.xlsx → student_scores.xlsx\n\n2.15.3 No special characters in file names:\nAvoid including in file names characters that have special meanings in many programming languages such as *@^$! and many others. Otherwise unexpected results can make your life complicated.\nparent@emotion*survey.csv → parent_emotion_survey.csv\n\n2.15.4 Separate dates with hyphens using ISO format\n\n\n\nfrom XKCD\n\n\nISO format for dates is YYYY-MM-DD, meaning that a four-digit year comes first, followed by a two-digit month, followed by a two-digit day. This format makes sorting order much easier than the formats used in the U.S.\nHere is a data file name that begins with a date, followed by the school district from from the data were collected:\n2020-01-12_district_A.csv\nOne of the benefits of using dates in the ISO format is that they sort chronologically. Which file names would you prefer to deal with?\n\n\n\n\nSorted Traditional Dates\nSorted ISO Dates\n\n\n\nAugust 29, 2009_District_M.csv\n2007-05-22_District_U.csv\n\n\nDecember 31, 2012_District_L.csv\n2009-08-29_District_M.csv\n\n\nJuly 29, 2011_District_K.csv\n2011-07-29_District_K.csv\n\n\nMay 22, 2007_District_U.csv\n2011-11-12_District_H.csv\n\n\nMay 23, 2013_District_Z.csv\n2012-10-17_District_W.csv\n\n\nNovember 12, 2011_District_H.csv\n2012-12-31_District_L.csv\n\n\nOctober 17, 2012_District_W.csv\n2013-05-23_District_Z.csv\n\n\nSeptember 18, 2014_District_E.csv\n2014-09-18_District_E.csv",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Ready to Use R via RStudio</span>"
    ]
  },
  {
    "objectID": "scalars.html",
    "href": "scalars.html",
    "title": "\n3  Scalars\n",
    "section": "",
    "text": "3.1 Variable Assignment\nIf you want to make a “scalar” in R, you assign a value to a variable name like so:\nx &lt;- 1\nThe &lt;- is called the assignment operator. Instead of typing both characters, I use the Alt+-Alt+- keyboard shortcut (i.e., press and hold AltAlt while pressing -), which puts spaces around the assignment operator.",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scalars</span>"
    ]
  },
  {
    "objectID": "scalars.html#variable-assignment",
    "href": "scalars.html#variable-assignment",
    "title": "\n3  Scalars\n",
    "section": "",
    "text": "In most situations, the = sign also functions as an assignment operator. It is easier to type than &lt;-, so feel free to use it.\n\nx = 1\n\nThe spaces around the assignment operator are optional, and you can use a more compact style if you wish:\n\nx=1\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nCreate a variable x by assigning it a value of 5.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nx &lt;- 5\n\n# If you prefer, the = assignment operator also works:\nx = 5\n\n# To display the value of x, just run it on its own line like so:\nx\n\n[1] 5",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scalars</span>"
    ]
  },
  {
    "objectID": "scalars.html#scalarscalar-operations",
    "href": "scalars.html#scalarscalar-operations",
    "title": "\n3  Scalars\n",
    "section": "\n3.2 Scalar–Scalar Operations",
    "text": "3.2 Scalar–Scalar Operations\nScalar operations are the basic arithmetical operators you are familiar with: addition, subtraction, multiplication, division, and exponentiation.\nFor example,\n4 + 1 = 5\n4 and 1 are both scalars, and adding them results in a scalar (i.e., 5).\nIn R, scalar operations like addition are straightforward:\n\n4 + 1\n\n[1] 5\n\n\n\n\nBy convention, we put spaces around most operator symbols (e.g., +, -, *, /, ^, =) because it makes complex code easier to read. However, the spaces are optional, and R does not care if things look messy:\n\n# Scalar addition\n# Putting single spaces around the + operator looks nice\n4 + 5\n\n[1] 9\n\n# R does not need spaces, though\n4+5\n\n[1] 9\n\n# R does not care if you are inconsistent or have multiple spaces\n4 +    5\n\n[1] 9\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nAdd 3 + 2 in R\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\n3 + 2\n\n[1] 5\n\n\n\n\n\nOther basic operators are also straightforward. Scalar subtraction with the - operator:\n\n4 - 1\n\n[1] 3\n\n\nScalar multiplication with the * operator:\n\n5 * 6\n\n[1] 30\n\n\nScalar division with the / operator:\n\n6 / 3\n\n[1] 2\n\n\nScalar exponentiation with the ^ operator:\n\n3 ^ 2\n\n[1] 9\n\n4 ^ -1\n\n[1] 0.25\n\n4 ^ .5\n\n[1] 2\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nCalculate 101 − 78 in R.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\n101 - 78\n\n[1] 23\n\n\n\nCalculate 29 × 42 in R.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\n29 * 42\n\n[1] 1218\n\n\n\nCalculate 33 squared in R.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\n33 ^ 2\n\n[1] 1089",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Scalars</span>"
    ]
  },
  {
    "objectID": "vectors.html",
    "href": "vectors.html",
    "title": "\n4  Vectors\n",
    "section": "",
    "text": "4.1 Making vectors in R",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#making-vectors-in-r",
    "href": "vectors.html#making-vectors-in-r",
    "title": "\n4  Vectors\n",
    "section": "",
    "text": "R is a peculiar peculiar language in that almost every object created in R is ultimately a vector of some kind or another. Here we are concerned with vectors made with real numbers (i.e., positive and negative numbers that can have decimals).R can create vectors from many other types of data (e.g., integers, complex numbers, logical values, text, and raw binary numbers). A more advanced discussion about vectors in R can be found here.\n\n4.1.1 Making vectors with the c function\nVectors in R are created in many ways, but the most common way to join vector elements is with the c function. The c function “combines” (or concatenates) elements into a vector. For example, to combine 3 and 5:To concatenate is to link things together in a chain or series.\n\nx &lt;- c(3, 5)\n\n\n\n\n\n\n\nYou Try\n\n\n\nCreate a vector x with values 2 and 6.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nx &lt;- c(2, 6)\n\n\n\n\n\n4.1.2 Making vector sequences with the : operator\nYou can make sequential integer sequences with the : operator. To make a sequence from 1 to 5:\n\n1:5\n\n[1] 1 2 3 4 5\n\n\nTo make a decreasing vector from 4 to 0:\n\n4:0\n\n[1] 4 3 2 1 0\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nCreate a sequence of integers from 5 to 11 with the : operator.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\n5:11\n\n[1]  5  6  7  8  9 10 11\n\n\n\n\n\n\n4.1.3 Making vector sequences with seq function\nThe seq function allows for sequences of any interval or length.\n\nseq(1,5)\n\n[1] 1 2 3 4 5\n\n\nDecreasing from 5 to 1:\n\nseq(5,1)\n\n[1] 5 4 3 2 1\n\n\nThe by argument specifies the interval between elements. Here we specify a sequence from 0 to 1 by .1 intervals:\n\nseq(0, 1, 0.1)\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n\n\nFor decreasing sequences, the by argument needs to be negative:\n\nseq(5, 3, -0.5)\n\n[1] 5.0 4.5 4.0 3.5 3.0\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nCreate a sequence of integers from 3 to 6 with the seq function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nseq(3, 6)\n\n[1] 3 4 5 6\n\n\n\nCreate a sequence of even integers from 2 to 10 with the seq function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nseq(2, 10, 2)\n\n[1]  2  4  6  8 10\n\n\n\nCreate a decreasing sequence of integers from 6 to 3 with the seq function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nseq(6, 3)\n\n[1] 6 5 4 3\n\n\n\nCreate a decreasing sequence of numbers from 1 to -1 decreasing by increments of 0.2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nseq(1, -1, -0.2)\n\n [1]  1.0  0.8  0.6  0.4  0.2  0.0 -0.2 -0.4 -0.6 -0.8 -1.0",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#selecting-specific-elements-in-a-vector",
    "href": "vectors.html#selecting-specific-elements-in-a-vector",
    "title": "\n4  Vectors\n",
    "section": "\n4.2 Selecting Specific Elements in a Vector",
    "text": "4.2 Selecting Specific Elements in a Vector\nIn R, there are several ways to select elements in a vector—by position, by condition, and by name.\n\n4.2.1 Selecting by Position\nSuppose we have a vector with 5 elements:\n\\vec{x}=\\left(10,20,30,40,50\\right)\nFirst I define \\vec{x}\n\nx &lt;- seq(10, 50, 10)\nx\n\n[1] 10 20 30 40 50\n\n\nThe 3rd element, 30, can be selected like so:\n\nx[3]\n\n[1] 30\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nSelect the 4th element of x.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nx[4]\n\n[1] 40\n\n\n\n\n\nIf I want both the 3rd and 5th elements, I use a vector inside the square brackets:\n\nx[c(3, 5)]\n\n[1] 30 50\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nSelect first and fourth element of x.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nx[c(1, 4)]\n\n[1] 10 40\n\n\n\n\n\n\n4.2.2 Selecting by Condition\nR can select elements meeting any condition that evaluates to the logical value of TRUE. For example, which values of \\vec{x} are greater than 35?\n\nx &gt; 35\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n\nThis creates a vector of 5 elements that are either TRUE or FALSE. We can select just the elements for which the condition is TRUE like so:\n\nx[x &gt; 35]\n\n[1] 40 50\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nSelect elements of x less than 15.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nx[x &lt; 15]\n\n[1] 10\n\n\n\n\n\n\n4.2.3 Selecting by Name\nIn R, vector elements can have names. We can assign them directly like this:\n\nx &lt;- c(a = 1, b = 2, c = 3, d = 4, e = 5)\nx\n\na b c d e \n1 2 3 4 5 \n\n\nWe can also assign names using the names function:\n\nx &lt;- 1:5\nx_names &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\nnames(x) &lt;- x_names\nx\n\na b c d e \n1 2 3 4 5 \n\n\nWe can select specific named elements like so:\n\nx[c(\"a\", \"d\")]\n\na d \n1 4 \n\n\nWe can select them in any order:\n\nx[c(\"d\", \"a\")]\n\nd a \n4 1 \n\n\nWe can even have repeats:\n\nx[c(\"a\", \"a\", \"b\")]\n\na a b \n1 1 2 \n\n\n\n\n\n\n\n\nYou Try\n\n\n\nSelect elements b and a of x, in that order .\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nx[c(\"b\", \"a\")]\n\nb a \n2 1",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#scalarvector-operations",
    "href": "vectors.html#scalarvector-operations",
    "title": "\n4  Vectors\n",
    "section": "\n4.3 Scalar–Vector Operations",
    "text": "4.3 Scalar–Vector Operations\nWhen a scalar and a vector appear together in an operation, the operation is applied to the scalar and to every element in the vector. For example, to multiply a scalar by a vector, multiply every element in the vector by the scalar. If a is a scalar,\na\\vec{x}=\\left(ax_1, ax_2,\\ldots, ax_n\\right)\nThey are called “scalars” because a scalar multiplied by a vector “scales” the vector by changing its magnitude.\n\\left\\|a\\vec{x}\\right\\|=a\\left\\|\\vec{x}\\right\\|\nYou can play with scalar multiplication with the web app in Figure 4.1. Set the x and y coordinates for the blue vector, and alter the scalar a to see how the red vector’s magnitude changes. When a is negative, the red vector reverses direction.\n\nviewof fs = \nInputs.form({\n  xi: Inputs.range([-1,1], {\n    format: (x) =&gt; x,\n    value: 1, \n    step: .01, \n    label: md`*x* `,\n    labelStyle: \"width: 50px\"\n  }),\n  yi: Inputs.range(\n    [-1,1], \n    {value: 1, \n     step: .01, \n     vertical: true, \n     label: md`*y*`}),\n  a: Inputs.range(\n    [-5,5], \n    {value: 2, \n     step: .01, \n     label: md`*a* `, \n     description: 'Scalar'})\n  })\nax = fs.xi * fs.a\nay = fs.yi * fs.a\ntheta=Math.atan2(ay,ax)\ndd=[{\n  x: fs.xi, \n  y: fs.yi, \n  ax: ax, \n  ay: ay, \n  x0: 0, \n  y0: 0, \n  a: fs.a, \n  theta: theta }]\n\nns = Inputs.range().classList[0]\n\nhtml`\n&lt;style&gt;\nform.${ns} {--input-width: 600px; --label-width: 15px;}\n.${ns}-input&gt;input[type=number] {flex-shrink:6;}\n&lt;/style&gt;\n`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({height: 600, width: 600,\nx: {domain: [-5, 5], grid: true, ticks: 9},\ny: {domain: [-5, 5], grid: true, ticks: 9},\nstyle: {\n    fontSize: 14, \n    fontFamily: \"equity_text_a_tab\"\n  },\nmarks: [\nPlot.ruleY([0]),\nPlot.ruleX([0]),\nPlot.arrow(\n  dd, \n  {x1: \"x0\", \n   y1: \"y0\", \n   x2: \"ax\", \n   y2: \"ay\", \n   stroke: \"#8B1A1A\", \n   headAngle: 30, \n   headLength: 10, \n   fill: \"#8B1A1A\"}),\nPlot.arrow(\n  dd, \n  {x1: \"x0\", \n   y1: \"y0\", \n   x2: \"x\", \n   y2: \"y\", \n   stroke: \"#27408B\", \n   headAngle: 30, \n   headLength: 10, \n   fill: \"#27408B\"}),\n  Plot.text(\n    dd, \n    {x: (d) =&gt; d.ax + .3 * Math.cos(d.theta), \n     y: (d) =&gt; d.ay + .3 * Math.sin(d.theta), \n     text: (d) =&gt; '('+(d.ax).toFixed(2)+', '+(d.ay).toFixed(2)+')'})\n]})\n\n\n\n\n\n\n\nFigure 4.1: Multiplying a vector by a scalar changes its magnitude.\n\n\n\n\n\n\n\n\n\n\nObservable Code for Figure 4.1\n\n\n\n\n\nviewof fs = \nInputs.form({\n  xi: Inputs.range([-1,1], {\n    format: (x) =&gt; x,\n    value: 1, \n    step: .01, \n    label: md`*x* `,\n    labelStyle: \"width: 50px\"\n  }),\n  yi: Inputs.range(\n    [-1,1], \n    {value: 1, \n     step: .01, \n     vertical: true, \n     label: md`*y*`}),\n  a: Inputs.range(\n    [-5,5], \n    {value: 2, \n     step: .01, \n     label: md`*a* `, \n     description: 'Scalar'})\n  })\nax = fs.xi * fs.a\nay = fs.yi * fs.a\ntheta=Math.atan2(ay,ax)\ndd=[{\n  x: fs.xi, \n  y: fs.yi, \n  ax: ax, \n  ay: ay, \n  x0: 0, \n  y0: 0, \n  a: fs.a, \n  theta: theta }]\n\nns = Inputs.range().classList[0]\n\nhtml`\n&lt;style&gt;\nform.${ns} {--input-width: 600px; --label-width: 15px;}\n.${ns}-input&gt;input[type=number] {flex-shrink:6;}\n&lt;/style&gt;\n`\nPlot.plot({height: 600, width: 600,\nx: {domain: [-5, 5], grid: true, ticks: 9},\ny: {domain: [-5, 5], grid: true, ticks: 9},\nstyle: {\n    fontSize: 14, \n    fontFamily: \"equity_text_a_tab\"\n  },\nmarks: [\nPlot.ruleY([0]),\nPlot.ruleX([0]),\nPlot.arrow(\n  dd, \n  {x1: \"x0\", \n   y1: \"y0\", \n   x2: \"ax\", \n   y2: \"ay\", \n   stroke: \"#8B1A1A\", \n   headAngle: 30, \n   headLength: 10, \n   fill: \"#8B1A1A\"}),\nPlot.arrow(\n  dd, \n  {x1: \"x0\", \n   y1: \"y0\", \n   x2: \"x\", \n   y2: \"y\", \n   stroke: \"#27408B\", \n   headAngle: 30, \n   headLength: 10, \n   fill: \"#27408B\"}),\n  Plot.text(\n    dd, \n    {x: (d) =&gt; d.ax + .3 * Math.cos(d.theta), \n     y: (d) =&gt; d.ay + .3 * Math.sin(d.theta), \n     text: (d) =&gt; '('+(d.ax).toFixed(2)+', '+(d.ay).toFixed(2)+')'})\n]})\n\n\n\n\nScalar-vector division looks just like it does in regular algebra and can take on a variety of forms:\n\n\\vec{x}/a=\\frac{\\vec{x}}{a}=\\frac{1}{a}\\vec{x}=a^{-1}\\vec{x}=\\vec{x}\\div a\n\nScalar–vector addition and subtraction work in similar fashion. To add scalar to a vector:\n\\vec{x}-a=\\left(x_1+a,x_2+a,\\ldots,x_n+a\\right)\nTo subtract:\n\\vec{x}-a=\\left(x_1-a, x_2-a,\\ldots,x_n-a\\right)\nIn R,\n\nx &lt;- c(1,2)\nx + 5\n\n[1] 6 7\n\nx - 1\n\n[1] 0 1\n\n\n\n4.3.1 Vector Addition\nTo add two vectors, add each element at the same position:\n\n\\begin{aligned}\n\\vec{x}&= \\left(x_1,x_2,\\ldots,x_n\\right)\\\\\n\\vec{y}&= \\left(y_1,y_2,\\ldots,y_n\\right)\\\\\n\\vec{x}+\\vec{y}&= \\left(x_1+y_1,x_2+y_2,\\ldots,x_n+y_n\\right)\\\\\n\\end{aligned}\n\nAs an example:\n\n\\begin{aligned}\n\\vec{x}&= \\left(1,2\\right)\\\\\n\\vec{y}&= \\left(3,4\\right)\\\\\n\\\\\n\\vec{x}+\\vec{y}&= \\left(1+3,2+4\\right)\\\\\n&= \\left(4,6\\right)\n\\end{aligned}\n\nVector subtraction works the same way as vector addition, subtracting each element at the same position:\n\n\\vec{x}-\\vec{y}= \\left(x_1-y_1,x_2-y_2,\\ldots,x_n-y_n\\right)\\\\\n\nAdding and subtracting two vectors is only defined when the vectors have the same number of elements. Thus, if \\vec{x}= \\left(1,2\\right) and \\vec{y}= \\left(3,2,1\\right), there is no defined way to add \\vec{x} and \\vec{y}.\nIn R, adding and subtracting vectors is beautifully easy. Unlike many programming language, we do not need to create looping structures to add each element pair one at a time. We just define the vectors and add them:\n\nx &lt;- c(1, 2)\ny &lt;- c(3, 4)\nx + y\n\n[1] 4 6\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nIf vector \\vec{a} is (5, 6) and vector $ is (10, 1), what is the sum of $ and $? Use R to compute your answer.\n\n\nSuggested Solution\na &lt;- c(5, 6)\nb &lt;- c(10, 1)\na + b\n\n[1] 15  7\n\n\n\n\n\nIn Figure 4.2, you can see that adding vectors is like\n\nviewof fsab = \nInputs.form({\n  ax: Inputs.range([-1,1], {\n    format: (x) =&gt; x,\n    value: 1, \n    step: .01, \n    label:html`&lt;em&gt;a&lt;sub&gt;x&lt;/sub&gt;&lt;/em&gt;`,\n    labelStyle: \"width: 50px\"\n  }),\n  ay: Inputs.range(\n    [-1,1], \n    {value: 0.5, \n     step: .01, \n     label: html`&lt;em&gt;a&lt;sub&gt;y&lt;/sub&gt;&lt;/em&gt;`}),\n  bx: Inputs.range(\n    [-1,1], \n    {value:0.5, \n     step: .01, \n     label: html`&lt;em&gt;b&lt;sub&gt;x&lt;/sub&gt;&lt;/em&gt;`}),\n  by: Inputs.range(\n    [-1,1], \n    {value: 1, \n     step: .01, \n     label: html`&lt;em&gt;b&lt;sub&gt;y&lt;/sub&gt;&lt;/em&gt;`})\n  })\nab=[{x0: 0, \n     y0: 0, \n     ax: fsab.ax, \n     ay: fsab.ay, \n     bx: fsab.bx, \n     by: fsab.by, \n     abx: fsab.ax + fsab.bx, \n     aby: fsab.ay + fsab.by, \n     theta: Math.atan2(fsab.ay + fsab.by, fsab.ax + fsab.bx), \n     thetaa: Math.atan2(fsab.ay, fsab.ax), \n     thetab: Math.atan2(fsab.by, fsab.bx), \n     a: 'a', \n     b: 'b', \n     ab:'a + b', \n     fill: '#ffffff', \n     r: 200}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({height: 600, width: 600,\nx: {domain: [-2, 2], grid: true, ticks: 9},\ny: {domain: [-2, 2], grid: true, ticks: 9},\nstyle: {\n    fontSize: 14, fontFamily: \"equity_text_a_tab\"\n  },\nmarks: [\n  Plot.ruleY([0]),\n  Plot.ruleX([0]),\n  Plot.arrow(\n    ab, \n    {x1: \"x0\", \n     y1: \"y0\", \n     x2: \"ax\", \n     y2: \"ay\", \n     stroke: \"#8B1A1A\", \n     headAngle: 30, \n     headLength: 10, \n     fill: \"#8B1A1A\"}),\n  Plot.arrow(\n    ab, \n    {x1: \"ax\", \n     y1: \"ay\", \n     x2: \"abx\", \n     y2: \"aby\", \n     stroke: \"#27408B\", \n     headAngle: 30, \n     headLength: 10, \n     fill: \"#27408B\", \n     alpha: .5}),\n  Plot.arrow(\n    ab, \n    {x1: \"x0\", \n     y1: \"y0\", \n     x2: \"abx\", \n     y2: \"aby\", \n     stroke: \"#51315E\", \n     headAngle: 30, \n     headLength: 10, \n     fill: \"#51315E\"}),\n  Plot.dot(\n    ab, \n    {x: (d) =&gt; d.ax/2, \n     y: (d) =&gt; d.ay/2, \n     fill: \"fill\", \n     r: 7}),\n  Plot.dot(\n    ab, \n    {x: (d) =&gt; d.ax + d.bx/2, \n     y: (d) =&gt; d.ay + d.by/2, \n     fill: \"fill\", \n     r: 7}),\n  Plot.dot(\n    ab, \n    {x: (d) =&gt; d.abx/2, \n     y: (d) =&gt; d.aby/2, \n     fill: \"fill\", \n     r: 18}),\n  Plot.text(\n    ab, \n    {x: (d) =&gt; d.ax/2, \n     y: (d) =&gt; d.ay / 2, \n     text: 'a', \n     rotate: (d) =&gt; Math.atan(Math.tan(d.thetaa)) * -180 / Math.PI, \n     fill: '#8B1A1A'}),\n   Plot.text(\n     ab, \n     {x: (d) =&gt; d.ax + d.bx/2, \n      y: (d) =&gt; d.ay + d.by / 2, \n      text: 'b', \n      rotate: (d) =&gt; Math.atan(Math.tan(d.thetab)) * -180 / Math.PI, \n      fill: '#27408B'}),\n  Plot.text(\n    ab, \n    {x: (d) =&gt; d.abx/2, \n     y: (d) =&gt; d.aby / 2, \n     text: 'ab', \n     rotate: (d) =&gt; Math.atan(Math.tan(d.theta)) * -180 / Math.PI, \n     fill: '#51315E'})\n]})\n\n\n\n\n\n\n\nFigure 4.2: Adding vectors a and b.\n\n\n\n\n\n\n\n\n\n\nObservable Code for Figure 4.2\n\n\n\n\n\nviewof fsab = \nInputs.form({\n  ax: Inputs.range([-1,1], {\n    format: (x) =&gt; x,\n    value: 1, \n    step: .01, \n    label:html`&lt;em&gt;a&lt;sub&gt;x&lt;/sub&gt;&lt;/em&gt;`,\n    labelStyle: \"width: 50px\"\n  }),\n  ay: Inputs.range(\n    [-1,1], \n    {value: 0.5, \n     step: .01, \n     label: html`&lt;em&gt;a&lt;sub&gt;y&lt;/sub&gt;&lt;/em&gt;`}),\n  bx: Inputs.range(\n    [-1,1], \n    {value:0.5, \n     step: .01, \n     label: html`&lt;em&gt;b&lt;sub&gt;x&lt;/sub&gt;&lt;/em&gt;`}),\n  by: Inputs.range(\n    [-1,1], \n    {value: 1, \n     step: .01, \n     label: html`&lt;em&gt;b&lt;sub&gt;y&lt;/sub&gt;&lt;/em&gt;`})\n  })\nab=[{x0: 0, \n     y0: 0, \n     ax: fsab.ax, \n     ay: fsab.ay, \n     bx: fsab.bx, \n     by: fsab.by, \n     abx: fsab.ax + fsab.bx, \n     aby: fsab.ay + fsab.by, \n     theta: Math.atan2(fsab.ay + fsab.by, fsab.ax + fsab.bx), \n     thetaa: Math.atan2(fsab.ay, fsab.ax), \n     thetab: Math.atan2(fsab.by, fsab.bx), \n     a: 'a', \n     b: 'b', \n     ab:'a + b', \n     fill: '#ffffff', \n     r: 200}]\n\n\nPlot.plot({height: 600, width: 600,\nx: {domain: [-2, 2], grid: true, ticks: 9},\ny: {domain: [-2, 2], grid: true, ticks: 9},\nstyle: {\n    fontSize: 14, fontFamily: \"equity_text_a_tab\"\n  },\nmarks: [\n  Plot.ruleY([0]),\n  Plot.ruleX([0]),\n  Plot.arrow(\n    ab, \n    {x1: \"x0\", \n     y1: \"y0\", \n     x2: \"ax\", \n     y2: \"ay\", \n     stroke: \"#8B1A1A\", \n     headAngle: 30, \n     headLength: 10, \n     fill: \"#8B1A1A\"}),\n  Plot.arrow(\n    ab, \n    {x1: \"ax\", \n     y1: \"ay\", \n     x2: \"abx\", \n     y2: \"aby\", \n     stroke: \"#27408B\", \n     headAngle: 30, \n     headLength: 10, \n     fill: \"#27408B\", \n     alpha: .5}),\n  Plot.arrow(\n    ab, \n    {x1: \"x0\", \n     y1: \"y0\", \n     x2: \"abx\", \n     y2: \"aby\", \n     stroke: \"#51315E\", \n     headAngle: 30, \n     headLength: 10, \n     fill: \"#51315E\"}),\n  Plot.dot(\n    ab, \n    {x: (d) =&gt; d.ax/2, \n     y: (d) =&gt; d.ay/2, \n     fill: \"fill\", \n     r: 7}),\n  Plot.dot(\n    ab, \n    {x: (d) =&gt; d.ax + d.bx/2, \n     y: (d) =&gt; d.ay + d.by/2, \n     fill: \"fill\", \n     r: 7}),\n  Plot.dot(\n    ab, \n    {x: (d) =&gt; d.abx/2, \n     y: (d) =&gt; d.aby/2, \n     fill: \"fill\", \n     r: 18}),\n  Plot.text(\n    ab, \n    {x: (d) =&gt; d.ax/2, \n     y: (d) =&gt; d.ay / 2, \n     text: 'a', \n     rotate: (d) =&gt; Math.atan(Math.tan(d.thetaa)) * -180 / Math.PI, \n     fill: '#8B1A1A'}),\n   Plot.text(\n     ab, \n     {x: (d) =&gt; d.ax + d.bx/2, \n      y: (d) =&gt; d.ay + d.by / 2, \n      text: 'b', \n      rotate: (d) =&gt; Math.atan(Math.tan(d.thetab)) * -180 / Math.PI, \n      fill: '#27408B'}),\n  Plot.text(\n    ab, \n    {x: (d) =&gt; d.abx/2, \n     y: (d) =&gt; d.aby / 2, \n     text: 'ab', \n     rotate: (d) =&gt; Math.atan(Math.tan(d.theta)) * -180 / Math.PI, \n     fill: '#51315E'})\n]})\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3: A vector of (4, 3) moves 4 units to the right and 3 units up from the origin.\n\n\n\nA vector with two scalar elements can be thought of as an arrow in a coordinate plane, as in Figure 4.3. The vector \\left(4, 3\\right) is an arrow that displaces any point it is added to by 4 units to the right and 3 units up.\n\n\n\n\n\n\n\nR Code for Figure 4.3\n\n\n\n\n\ntibble(x = c(0, x_plot), y = c(0, y_plot)) |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(color = myfills[1], size = .75) +\n  geom_arrow(arrow_head = arrow_head_deltoid(),\n    linewidth = 1,\n    color = myfills[1],\n        resect_head = 1,\n  ) +\n  geom_arrow_segment(arrow_head = arrow_head_deltoid(), \n             arrow_fins = arrow_head_deltoid(),\n             data = tibble(x = c(-5.5, 0),\n                           y = c(0, -5.5),\n                           xend = c(5.5, 0),\n                           yend = c(0, 5.5)),\n             aes(xend = xend, yend = yend),\n    linewidth = .75,\n    linemitre = 2,\n    linejoin = \"mitre\",\n    color = \"gray\"\n  ) +\n  theme_classic(\n    base_family = bfont,\n    base_size = 20,\n    base_line_size = .5\n  ) +\n  theme(\n    axis.text = element_text(color = \"gray40\"),\n    axis.line = element_blank(),\n    axis.ticks = element_line(color = \"gray\"),\n    axis.title.x = element_text(\n      angle = 0,\n      vjust = .5,\n      face = \"italic\",\n      color = \"gray40\"\n    ),\n    axis.title.y = element_text(\n      angle = 0,\n      vjust = .5,\n      face = \"italic\",\n      color = \"gray40\"\n    )\n  ) +\n  scale_x_continuous(name = \"y\", \n                     breaks = c(-5:-1, 1:5), \n                     labels = WJSmisc::signs_centered) +\n  scale_y_continuous(name = \"x\", \n                     breaks = c(-5:-1, 1:5), \n                     labels = signs) +\n  ggh4x::coord_axes_inside(\n    xlim = c(-5, 5),\n    ylim = c(-5, 5),\n    labels_inside = T,\n    ratio = 1\n  ) +\n  annotate(\n    \"richtext\",\n    x_plot,\n    y_plot,\n    label = paste0(\"**v** = (\", \n                   x_plot, \n                   \", \", \n                   y_plot, \n                   \")\"),\n    label.color = NA,\n    vjust = -.2,\n    hjust = 0.5,\n    fill = NA,\n    size = WJSmisc::ggtext_size(18),\n    label.r = unit(5, \"mm\"),\n    color = myfills[1]\n  ) +\n  annotate(\n    \"richtext\",\n    x_plot / 2,\n    y_plot / 2,\n    label = paste0(\"Magnitude of **v**: \\u2016**v**\\u2016 = \", \n                   round(sqrt(x_plot ^ 2 + y_plot ^ 2),2)),\n    label.color = NA,\n    angle = atan2(y_plot,x_plot) * 180 / pi,\n    vjust = 0,\n    hjust = 0.5,\n    fill = NA,\n    size = WJSmisc::ggtext_size(18),\n    label.r = unit(5, \"mm\"),\n    color = myfills[1]\n  )\n\n\n\n\n\n4.3.2 Vector Norms\nIf we visualize a vector as an arrow, we can ask how long the arrow is from end to end. The norm is the vector’s magnitude. Imagine the vector’s n elements plotted as a single point in n-dimensional space. The norm is the distance of the point to the origin (i.e., a vector of n zeroes). The distance referred to here is technically the Euclidean distance, which is a generalization of the Pythagorean Theorem to more than 2 dimensions. To calculate it, take the square root of the sum of all squared values in the vector.A vector’s norm is the Euclidean distance of the vector’s n elements to the origin in n-dimensional space.\n\n\nVector norms sometimes have notation that can be confused with absolute values:\n\\left|\\vec{x}\\right|\nThe notational confusion is not accidental. Scalar absolute values and vector norms both refer to magnitude. Indeed, if you apply the formula for vector norms to a vector with one element, the result is the absolute value:\n\\sqrt{x_1^2} = |x_1|\nHowever, to avoid the ambiguity between vector norms and taking the absolute value of each element, I will use the double bar notation:\n\\left\\|\\vec{x}\\right\\|\nI tend to be fastidious about distinguishing between vector length (the number of elements in a vector) and vector norms (the vector’s magnitude) because the concepts are easily confused. Further confusion is that the norm of a vector is not the same thing as a normal vector, which is a vector that is perpendicular to a surface (e.g., a plane, a sphere, or any other multidimensional object) at a given point. A normal vector has nothing to do with the normal distribution.\n\\left\\|\\vec{x}\\right\\|=\\sqrt{x_1^2+x_2^2+\\ldots+x_n^2}\nTo calculate a vector norm in R by hand is straightforward:\n\nx &lt;- c(4, 3)\nsqrt(sum(x ^ 2))\n\n[1] 5\n\n\nIf you can remember that this kind of norm (there are others) is a “type 2” norm, then a vector norm can be calculated with the norm function:\n\nnorm(x, type = \"2\")\n\n[1] 5\n\n\n\n4.3.3 Unit Vectors\nA vector norm is useful for creating unit vectors, which have the same direction as the original vector but have a norm (i.e, magnitude) of 1.A unit vector \\vec{u} has a norm of 1: \\left\\|\\vec{u}\\right\\|=1\nSo a unit vector \\vec{u} that has the same direction as vector \\vec{x} = \\left(3,4\\right) is:\n\n\\begin{align*}\n\\vec{u}&=\\frac{\\vec{x}}{\\left\\|\\vec{x}\\right\\|}\\\\\n&=\\frac{\\left(3,4\\right)}{\\sqrt{3^2+4^2}}\\\\\n&=\\frac{\\left(3,4\\right)}{5}\\\\\n&=\\left(.6,.8\\right)\n\\end{align*}\n\nWe can verify that \\vec{u} has a norm of 1:\n\n\\begin{align*}\n\\left\\|\\vec{u}\\right\\|&=\\left\\|\\left(.6,.8\\right)\\right\\|\\\\\n&=\\sqrt{.6^2+.8^2}\\\\\n&=1\n\\end{align*}\n\nUnit vectors have many uses, including the calculation of correlation coefficients.\n\n4.3.4 Vector Multiplication\nThere are several different ways that vectors can be multiplied. For now, the most important are element-wise multiplication and dot-product multiplication.\n\n4.3.4.1 Element-wise Multiplication (\\circ)\n\nThis kind of multiplication is analogous to vector addition and subtraction. If two vectors have the same length (i.e, number of elements), element-wise multiplication creates a vector of the same length by multiplying each element. Thus,\n\n\\begin{aligned}\n\\vec{x}&= \\left(x_1,x_2,\\ldots,x_n\\right)\\\\\n\\vec{y}&= \\left(y_1,y_2,\\ldots,y_n\\right)\\\\\n\\vec{x}\\circ \\vec{y}&= \\left(x_1y_1,x_2 y_2,\\ldots,x_n y_n\\right)\\\\\n\\end{aligned}\n\nIn R, element-wise multiplication is straightforward. Just multiply the vectors with the * operator.\n\nx &lt;- c(1, 2, 3)\ny &lt;- c(2, 4, 6)\nx * y\n\n[1]  2  8 18\n\n\n\n4.3.4.2 Dot-Product Multiplication (\\cdot)\n\nThis is the same as element-wise multiplication, but the products are added together to create a scalar.\n\n\\begin{aligned}\n\\vec{x}&= \\left(x_1,x_2,\\ldots,x_n\\right)\\\\\n\\vec{y}&= \\left(y_1,y_2,\\ldots,y_n\\right)\\\\\n\\vec{x}\\cdot \\vec{y}&= x_1 y_1+x_2 y_2+\\ldots+x_n y_n\\\\\n\\end{aligned}\n\nIf \\vec{x}=(1,2) and \\vec{y}=(3,4),\n\n\\begin{aligned}\n\\vec{x} \\cdot \\vec{y}&=1\\cdot 3+2\\cdot 4\\\\\n&=3+8\\\\\n&=11\n\\end{aligned}\n\nAs will be seen later, the dot-product of 2 vectors is performed in R with the same operator as matrix multiplication: %*%.\n\nx &lt;- c(1, 2)\ny &lt;- c(3, 4)\nx %*% y\n\n     [,1]\n[1,]   11\n\n\nHowever, if this is hard to remember, then the dot product can thought of as “the sum of the elements multiplied” like so:\n\nsum(x * y)\n\n[1] 11\n\n\nI was surprised to learn that in matrix algebra dot-product multiplication is much more often used than element-wise multiplication. Indeed, matrix multiplication consists of many dot-product multiplications. Apart from that, dot-product multiplication has a number of important applications related to the Law of Cosines, finding orthogonal vectors (i.e., vectors at right angles), and the geometric interpretation of correlation coefficients. I will leave those topics for later.",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vectors</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html",
    "href": "matrices_intro.html",
    "title": "\n5  Matrices\n",
    "section": "",
    "text": "5.1 Appending Matrices\n\\color{RoyalBlue}{\\mathbf{A}}=\\begin{bmatrix}\n\\color{RoyalBlue}{1} & \\color{RoyalBlue}{2} & \\color{RoyalBlue}{3} & \\color{RoyalBlue}{4} \\\\\n\\color{RoyalBlue}{5} & \\color{RoyalBlue}{6} & \\color{RoyalBlue}{7} & \\color{RoyalBlue}{8} \\\\\n\\color{RoyalBlue}{9} & \\color{RoyalBlue}{10} & \\color{RoyalBlue}{11} & \\color{RoyalBlue}{12}\n\\end{bmatrix}, \\color{Firebrick}{\\mathbf{B}}=\\begin{bmatrix}\n\\color{Firebrick}{13} & \\color{Firebrick}{14} & \\color{Firebrick}{15} & \\color{Firebrick}{16} \\\\\n\\color{Firebrick}{17} & \\color{Firebrick}{18} & \\color{Firebrick}{19} & \\color{Firebrick}{20}\n\\end{bmatrix}\nThe equation below means, “Make a new matrix called C by appending B to the bottom of A.”\n\\mathbf{C}=\\begin{bmatrix}\n\\color{RoyalBlue}{\\mathbf{A}} \\\\ \\color{FireBrick}{\\mathbf{B}}\n\\end{bmatrix}=\n\\begin{bmatrix}\n\\color{RoyalBlue}{1} & \\color{RoyalBlue}{2} & \\color{RoyalBlue}{3} & \\color{RoyalBlue}{4} \\\\\n\\color{RoyalBlue}{5} & \\color{RoyalBlue}{6} & \\color{RoyalBlue}{7} & \\color{RoyalBlue}{8} \\\\\n\\color{RoyalBlue}{9} & \\color{RoyalBlue}{10} & \\color{RoyalBlue}{11} & \\color{RoyalBlue}{12}\\\\\n\\color{Firebrick}{13} & \\color{Firebrick}{14} & \\color{Firebrick}{15} & \\color{Firebrick}{16} \\\\\n\\color{Firebrick}{17} & \\color{Firebrick}{18} & \\color{Firebrick}{19} & \\color{Firebrick}{20}\n\\end{bmatrix}\nIn R, adding new rows to a matrix is done with the rbind function (Think “row bind”). To append rows, matrices must have the same number of columns to be compatible.\nA &lt;- matrix(1:12, nrow = 3, byrow = TRUE)\nB &lt;- matrix(13:20, nrow = 2, byrow = TRUE)\nC &lt;- rbind(A,B)\nC\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n[5,]   17   18   19   20\nThe cbind function (Think “column bind”) works the same way but it appends columns to the right of a matrix. Matrices must have the same number of rows to be compatible.\nA &lt;- matrix(1:4, nrow = 2)\nB &lt;- matrix(5:8, nrow = 2)\nC &lt;- cbind(A,B)\nC\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#appending-matrices",
    "href": "matrices_intro.html#appending-matrices",
    "title": "\n5  Matrices\n",
    "section": "",
    "text": "You Try\n\n\n\nMake a 2 by 2 matrix A:\n\nA=\\begin{bmatrix}\n  11&13\\\\\n  17&19\n\\end{bmatrix}\n\nMake a 2 by 2 matrix B:\n\nB=\\begin{bmatrix}\n  11&13\\\\\n  17&19\n\\end{bmatrix}\n\nAppend A and B with rbind to make a 4 by 2 matrix like so:\n\n\\begin{bmatrix}\n  2&3\\\\\n  5&7\\\\\n  11&13\\\\\n  17&19\n\\end{bmatrix}\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA &lt;- matrix(c(2, 5, 3, 7), nrow = 2)\nB &lt;- matrix(c(11,17,13,19), nrow = 2)\nrbind(A, B)\n\n     [,1] [,2]\n[1,]    2    3\n[2,]    5    7\n[3,]   11   13\n[4,]   17   19\n\n\n\nNow use cbind to make a 2 by 4 matrix:\n\n\\begin{bmatrix}\n  2&3&11&13\\\\\n  5&7&17&19\n\\end{bmatrix}\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\ncbind(A, B)\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    3   11   13\n[2,]    5    7   17   19",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#selecting-parts-of-a-matrix",
    "href": "matrices_intro.html#selecting-parts-of-a-matrix",
    "title": "\n5  Matrices\n",
    "section": "\n5.2 Selecting parts of a matrix",
    "text": "5.2 Selecting parts of a matrix\nLet’s make this matrix:\n\n\\mathbf{A} =\n\\begin{bmatrix}\n  1&2&3&4\\\\\n  5&6&7&8\\\\\n  9&10&11&12\n\\end{bmatrix}\n\n\nA &lt;- matrix(1:12, nrow = 3, byrow = TRUE)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\n\n5.2.1 Select a single element of a matrix\nTo select a single element of a matrix, specify the row and column in brackets after the matrix. For example, the element \\mathbf{A}_{3,2} (i.e., the 3rd row and 2nd column of \\mathbf{A}) is\n\nA[3,2]\n\n[1] 10\n\n\n\n\n\n\n\n\nYou Try\n\n\n\nSelect the element in row 2, column 3\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA[2,3]\n\n[1] 7\n\n\n\n\n\n\n5.2.2 Select a matrix row\nLeaving one of the slots in the bracket empty means that you want all of the elements in that row or column.\n\\mathbf{A}_{1\\bullet} is the 1st row of \\mathbf{A}.\n\nA[1, ]\n\n[1] 1 2 3 4\n\n\n\n5.2.3 Select a matrix column\n\\mathbf{A}_{\\bullet 3} is the 3rd column of \\mathbf{A}.\n\nA[, 3]\n\n[1]  3  7 11\n\n\nBy default, whenever a single row, column, or element is returned from a matrix, R drops the row and column dimensions. If you wish to preserve the result in matrix form, set drop to FALSE:\n\nA[, 3, drop = FALSE]\n\n     [,1]\n[1,]    3\n[2,]    7\n[3,]   11\n\n\n\nSelect column 1 of A.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA[, 1]\n\n[1] 1 5 9\n\n\n\n\n\n5.2.4 Select several columns or rows\nA vector of integers will select whichever rows or columns you wish. Here are the 2nd and 3rd rows:\n\nA[2:3, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    5    6    7    8\n[2,]    9   10   11   12\n\n\nHere are the 1st and 4th columns:\n\nA[, c(1, 4)]\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    5    8\n[3,]    9   12\n\n\n\nSelect columns 1 and 3 of A.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA[, c(1, 3)]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    5    7\n[3,]    9   11\n\n\n\n\n\n5.2.5 Selecting with Boolean vectors\nHere is the first two rows of \\mathbf{A}:\n\nA[c(TRUE,TRUE,FALSE),]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n\n\nThis seems like a strange way to do this but it is actually quite powerful. Any vector of TRUE and FALSE values can be used to select things. For example, select a column only if its first value is greater than 2:\n\ns &lt;- A[1,] &gt; 2 # Creates a vector that tests whether the first row elements are greater than 2\nA[,s] # Select only the columns whose first value is greater than 2\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    7    8\n[3,]   11   12\n\n\n\nUse a Boolean vector to select columns that begin with 2. The operator for “is equal to” is ==.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\ns &lt;- A[1,] == 2 \n# When only one column or row is selected, R returns a vector\nA[,s]\n\n[1]  2  6 10\n\n# To force R to return a matrix, set drop = FALSE\nA[,s, drop = FALSE]\n\n     [,1]\n[1,]    2\n[2,]    6\n[3,]   10\n\n\n\n\n\n5.2.6 Selecting with name vectors\nWe can give row and column names to a matrix like so:\n\nrownames(A) &lt;- c(\"Gold\",\"Silver\",\"Bronze\")\ncolnames(A) &lt;- c(\"Vault\",\"Uneven Bars\",\"Balance Beam\",\"Floor\")\nA\n\n       Vault Uneven Bars Balance Beam Floor\nGold       1           2            3     4\nSilver     5           6            7     8\nBronze     9          10           11    12\n\n\nNow we can select rows and columns by names:\n\nA[c(\"Gold\",\"Bronze\"),]\n\n       Vault Uneven Bars Balance Beam Floor\nGold       1           2            3     4\nBronze     9          10           11    12\n\n\n\nUse the column names to select the Uneven Bars column\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA[,\"Uneven Bars\"]\n\n  Gold Silver Bronze \n     2      6     10 \n\n# or to force returning a matrix\nA[,\"Uneven Bars\", drop = FALSE]\n\n       Uneven Bars\nGold             2\nSilver           6\nBronze          10\n\n\n\nSelect the intersection of Silver and Floor\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA[\"Silver\", \"Floor\"]\n\n[1] 8\n\n# or\nA[\"Silver\", \"Floor\", drop = FALSE]\n\n       Floor\nSilver     8",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#replace-portions-of-a-matrix",
    "href": "matrices_intro.html#replace-portions-of-a-matrix",
    "title": "\n5  Matrices\n",
    "section": "\n5.3 Replace portions of a matrix",
    "text": "5.3 Replace portions of a matrix\nAny portion of a matrix can be replaced with new values. For example, this will replace the first row with zeros:\n\nA[1, ] &lt;- c(0, 0, 0, 0)\nA\n\n       Vault Uneven Bars Balance Beam Floor\nGold       0           0            0     0\nSilver     5           6            7     8\nBronze     9          10           11    12\n\n\nThis can be done by column or row name as well\n\nA[\"Gold\", ] &lt;- c(0, 0, 0, 0)\nA\n\n       Vault Uneven Bars Balance Beam Floor\nGold       0           0            0     0\nSilver     5           6            7     8\nBronze     9          10           11    12\n\n\n\nReplace the Vault column of \\mathbf{A} with a vector of (10,20,30)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA[, \"Vault\"] &lt;- c(10, 20, 30)\nA\n\n       Vault Uneven Bars Balance Beam Floor\nGold      10           0            0     0\nSilver    20           6            7     8\nBronze    30          10           11    12",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#transposing-matrices",
    "href": "matrices_intro.html#transposing-matrices",
    "title": "\n5  Matrices\n",
    "section": "\n5.4 Transposing matrices",
    "text": "5.4 Transposing matrices\nTo transpose a matrix is to write its rows as columns and its columns as rows.\nTo transpose a matrix is to flip its rows into columns.\n\\mathbf{A}' is matrix \\mathbf{A} transposed.\nIf\n\\mathbf{A}=\\begin{bmatrix}\n1&2&3\\\\\n4&5&6\n\\end{bmatrix}\nThen\n\\mathbf{A}'=\\begin{bmatrix}\n1&4\\\\\n2&5\\\\\n3&6\n\\end{bmatrix}\n\n5.4.1 Transposing in R\nIn R the t function transposes matrices.\n\nA &lt;- matrix(1:6, nrow = 2, byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\nAprime &lt;- t(A)\nAprime\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\n\nMake a 2 \\times 5 matrix A of even numbers up to 20.\nTranspose A, assigning it to a variable called Aprime.\nReplace the last row (row 5) of Aprime with zeroes.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nA &lt;- matrix(seq(2, 20, 2), nrow = 2)\nA\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    6   10   14   18\n[2,]    4    8   12   16   20\n\nAprime &lt;- t(A)\nAprime\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    6    8\n[3,]   10   12\n[4,]   14   16\n[5,]   18   20\n\nAprime[5, ] &lt;- c(0, 0)\nAprime\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    6    8\n[3,]   10   12\n[4,]   14   16\n[5,]    0    0",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#square-matrices",
    "href": "matrices_intro.html#square-matrices",
    "title": "\n5  Matrices\n",
    "section": "\n5.5 Square matrices",
    "text": "5.5 Square matrices\nIn a square matrix, the number of rows is equal to the number of columns.",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#symmetric-matrices",
    "href": "matrices_intro.html#symmetric-matrices",
    "title": "\n5  Matrices\n",
    "section": "\n5.6 Symmetric matrices",
    "text": "5.6 Symmetric matrices\nA symmetric matrix is a square matrix that is equal to its transpose.\n\\mathbf{A}=\\mathbf{A}'\nThis means that for all elements, a_{ij}=a_{ji}.\nHere is an example of a symmetric matrix:\n\\begin{bmatrix}\n\\color{green}a & \\color{Firebrick}b & \\color{RoyalBlue}c\\\\\n\\color{Firebrick}b & \\color{gold}d & \\color{DarkOrchid}e\\\\\n\\color{RoyalBlue}c & \\color{DarkOrchid}e & \\color{orange}f\n\\end{bmatrix}\nTo verify that a matrix is symmetric in R:\n\nall(A == t(A))\n# Or use a dedicated function\nisSymmetric(A)\n\nCorrelation matrices and covariance matrices are always symmetric.",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_intro.html#diagonal-matrices",
    "href": "matrices_intro.html#diagonal-matrices",
    "title": "\n5  Matrices\n",
    "section": "\n5.7 Diagonal matrices",
    "text": "5.7 Diagonal matrices\nA diagonal matrix is a square matrix consisting of zeroes everywhere except the diagonal. For example,\n\n\\mathbf{A} = \\begin{bmatrix}\na & 0 & 0\\\\\n0 & b & 0\\\\\n0 & 0 & c\n\\end{bmatrix}\n\nTo create a diagonal matrix, specify the diagonal vector and then insert it into the diag function like so:\n\na &lt;- 1:4\nA &lt;- diag(a)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    2    0    0\n[3,]    0    0    3    0\n[4,]    0    0    0    4\n\n\nAs we will see later, the diag function actually has several different purposes:\n\nIt creates a diagonal matrix \\mathbf{A} from a vector \\vec{a}.A &lt;- diag(a)\n\nIt extracts a diagonal vector \\vec{a} from a matrix \\mathbf{A}.a &lt;- diag(A)\n\nIt creates an identity matrix \\mathbf{I} from a positive integer n.I &lt;- diag(n)\n\nIt replaces the diagonal of matrix \\mathbf{A} with a new vector \\vec{b}.diag(A) &lt;- b",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Matrices--Introduction</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html",
    "href": "matrices_operations.html",
    "title": "\n6  Matrices\n",
    "section": "",
    "text": "6.1 Adding Matrices\nIn order to add matrices, they must be compatible, meaning that they must have same number of rows and columns.\nTo add compatible matrices, simply add elements in the same position. \n\\begin{aligned}\\mathbf{A}+\\mathbf{B}&=\n\\begin{bmatrix}\na_{11} & a_{12}\\\\\na_{21} & a_{22}\\\\\na_{31} & a_{32}\n\\end{bmatrix}+\n\\begin{bmatrix}\nb_{11} & b_{12}\\\\\nb_{21} & b_{22}\\\\\nb_{31} & b_{32}\n\\end{bmatrix}\\\\ &=\n\\begin{bmatrix}\na_{11}+b_{11} & a_{12}+b_{12}\\\\\na_{21}+b_{21} & a_{22}+b_{22}\\\\\na_{31}+b_{31} & a_{32}+b_{32}\n\\end{bmatrix}\n\\end{aligned}",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#subtracting-matrices",
    "href": "matrices_operations.html#subtracting-matrices",
    "title": "\n6  Matrices\n",
    "section": "\n6.2 Subtracting Matrices",
    "text": "6.2 Subtracting Matrices\nSubtracting matrices works the same way.\n\n\\begin{aligned}\\mathbf{A}-\\mathbf{B}&=\n\\begin{bmatrix}\na_{11} & a_{12}\\\\\na_{21} & a_{22}\\\\\na_{31} & a_{32}\n\\end{bmatrix}-\n\\begin{bmatrix}\nb_{11} & b_{12}\\\\\nb_{21} & b_{22}\\\\\nb_{31} & b_{32}\n\\end{bmatrix}\\\\ &=\n\\begin{bmatrix}\na_{11}-b_{11} & a_{12}-b_{12}\\\\\na_{21}-b_{21} & a_{22}-b_{22}\\\\\na_{31}-b_{31} & a_{32}-b_{32}\n\\end{bmatrix}\n\\end{aligned}\n\n\n6.2.1 Adding and Subtracting Matrices in R\n\nA &lt;- matrix(1:6,nrow = 2)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nB &lt;- matrix(seq(10,60,10),nrow = 2)\nB\n\n     [,1] [,2] [,3]\n[1,]   10   30   50\n[2,]   20   40   60\n\nAPlusB &lt;- A + B\nAPlusB\n\n     [,1] [,2] [,3]\n[1,]   11   33   55\n[2,]   22   44   66\n\nAMinusB &lt;- A - B\nAMinusB\n\n     [,1] [,2] [,3]\n[1,]   -9  -27  -45\n[2,]  -18  -36  -54",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#scalar-matrix-multiplication",
    "href": "matrices_operations.html#scalar-matrix-multiplication",
    "title": "\n6  Matrices\n",
    "section": "\n6.3 Scalar-Matrix Multiplication",
    "text": "6.3 Scalar-Matrix Multiplication\nA defword is a single number, not in a matrix.\nTo multiply a scalar by a matrix, multiply the scalar by every element in the matrix:\nk\\mathbf{A}=\nk\\begin{bmatrix}\na_{11} & a_{12} & a_{13}\\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix}=\n\\begin{bmatrix}\nka_{11} & ka_{12} & ka_{13}\\\\\nka_{21} & ka_{22} & ka_{23}\n\\end{bmatrix}\n\n6.3.1 Scalar-Matrix Multiplication in R\nTo perform scalar-matrix multiplication in R, define a scalar, create a matrix, and then multiply them with the * operator.\n\nk &lt;- 10\nA &lt;- matrix(1:6,nrow = 2)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nk * A\n\n     [,1] [,2] [,3]\n[1,]   10   30   50\n[2,]   20   40   60\n\n\n\nCreate a scalar k equal to 5.\nCreate a matrix A equal to\n\n\\mathbf{A} = \\begin{bmatrix}\n1,0\\\\\n3,5\n\\end{bmatrix}\n\nCalculate k\\mathbf{A}:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nk &lt;- 5\nA &lt;- matrix(c(1,3,0,5), nrow = 2)\nk * A\n\n     [,1] [,2]\n[1,]    5    0\n[2,]   15   25",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#matrix-multiplication",
    "href": "matrices_operations.html#matrix-multiplication",
    "title": "\n6  Matrices\n",
    "section": "\n6.4 Matrix Multiplication",
    "text": "6.4 Matrix Multiplication\nMatrix multiplication is considerably more complex than matrix addition and subtraction. It took me an embarrassingly long time for me to wrap my head around it. I will state things in the abstract first, but it is hard to see what is going on until you see a concrete example.\nIn order for matrices to be compatible for multiplication, the number of columns of the left matrix must be the same as the number of rows of the right matrix. The product of A and B will have the the same number of rows as A and the same number of columns as B.\nImagine that matrix A has n rows and m columns. Matrix B has m rows and p columns. When A and B are multiplied, the resulting product is matrix C with n rows and p columns.\n\n\\mathbf{A}_{n\\times m} \\mathbf{B}_{m\\times p} = \\mathbf{C}_{n\\times p}\n\nElement c_{ij} of \\mathbf{C} is the dot-product of row i of \\mathbf{A} and column j of \\mathbf{B}. That is,\nc_{ij}=\\mathbf{A}_{i\\bullet}\\mathbf{B}_{\\bullet j}\nThis schematic gives a nice visual summary:\n\n\nMatrix Multiplication\n\n\n6.4.1 Matrix Multiplication Example\n\\mathbf{A}=\\begin{bmatrix}\n\\color{FireBrick}a&\\color{FireBrick}b&\\color{FireBrick}c\\\\\n\\color{RoyalBlue}e&\\color{RoyalBlue}d&\\color{RoyalBlue}f\n\\end{bmatrix}\n\\mathbf{B}=\\begin{bmatrix}\n\\color{green}g&\\color{DarkOrchid}h\\\\\n\\color{green}i&\\color{DarkOrchid}j\\\\\n\\color{green}k&\\color{DarkOrchid}l\n\\end{bmatrix}\n\\mathbf{AB}=\\begin{bmatrix}\n\\color{FireBrick}a\\color{Green}g+\\color{FireBrick}b\\color{green}i+\\color{FireBrick}c\\color{green}k&\\color{FireBrick}a\\color{DarkOrchid}h+\\color{FireBrick}b\\color{DarkOrchid}j+\\color{FireBrick}c\\color{DarkOrchid}l\\\\\n\\color{RoyalBlue}e\\color{green}g+\\color{RoyalBlue}d\\color{green}i+\\color{RoyalBlue}f\\color{green}k&\\color{RoyalBlue}e\\color{DarkOrchid}h+\\color{RoyalBlue}d\\color{DarkOrchid}j+\\color{RoyalBlue}f\\color{DarkOrchid}l\n\\end{bmatrix}\nUsing specific numbers:\n\\mathbf{A}=\\begin{bmatrix}\n\\color{FireBrick}1&\\color{FireBrick}2&\\color{FireBrick}3\\\\\n\\color{RoyalBlue}4&\\color{RoyalBlue}5&\\color{RoyalBlue}6\n\\end{bmatrix}\n\\mathbf{B}=\\begin{bmatrix}\n\\color{green}{10}&\\color{DarkOrchid}{40}\\\\\n\\color{green}{20}&\\color{DarkOrchid}{50}\\\\\n\\color{green}{30}&\\color{DarkOrchid}{60}\n\\end{bmatrix}\n\n\\begin{align}\n\\mathbf{AB}&=\n\\begin{bmatrix}\n\\color{FireBrick}1\\cdot\\color{green}{10}+\\color{FireBrick}2\\cdot\\color{green}{20}+\\color{FireBrick}3\\cdot\\color{green}{30}&\\color{FireBrick}1\\cdot\\color{DarkOrchid}{40}+\\color{FireBrick}2\\cdot\\color{DarkOrchid}{50}+\\color{FireBrick}3\\cdot\\color{DarkOrchid}{60}\\\\\n\\color{RoyalBlue}4\\cdot\\color{green}{10}+\\color{RoyalBlue}5\\cdot\\color{green}{20}+\\color{RoyalBlue}6\\cdot\\color{green}{30}&\\color{RoyalBlue}4\\cdot\\color{DarkOrchid}{40}+\\color{RoyalBlue}5\\cdot\\color{DarkOrchid}{50}+\\color{RoyalBlue}6\\cdot\\color{DarkOrchid}{60}\n\\end{bmatrix}\\\\[1ex]\n&=\\begin{bmatrix}\n140&320\\\\\n320&770\n\\end{bmatrix}\n\\end{align}\n\n\n6.4.2 Matrix Multiplication in R\nThe %*% operator multiplies matrices (and the inner products of vectors).\n\nA &lt;- matrix(1:6,nrow = 2,byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\nB &lt;- matrix(seq(10,60,10),nrow = 3)\nB\n\n     [,1] [,2]\n[1,]   10   40\n[2,]   20   50\n[3,]   30   60\n\nC &lt;- A %*% B\nC\n\n     [,1] [,2]\n[1,]  140  320\n[2,]  320  770",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#identity-elements",
    "href": "matrices_operations.html#identity-elements",
    "title": "\n6  Matrices\n",
    "section": "\n7.1 Identity Elements",
    "text": "7.1 Identity Elements\nThe identity element for a binary operation is the value that when combined with something leaves it unchanged. For example, the additive identity is 0.\nX+0=X\nThe number 0 is also the identity element for subtraction.\nX-0=X\nThe multiplicative identity is 1.\nX \\times 1 = X\nThe number 1 is also the identity element for division and exponentiation.\nX \\div 1=X\nX^1=X\n\n7.1.1 Identity Matrix\nFor matrix multiplication with square matrices, the identity element is called the identity matrix, \\mathbf{I}.\n\\mathbf{AI}=\\mathbf{A}\nThe identity matrix is a diagonal matrix with ones on the diagonal. For example, a 2 \\times 2 identity matrix looks like this:\n\\mathbf{I}_2=\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\n\\end{bmatrix}\nA size-3 identity matrix looks like this:\n\\mathbf{I}_3=\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}\nIt is usually not necessary to use a subscript because the size of the identity matrix is usually assumed to be the same as that of the matrix it is multiplied by.\nThus, although it is true that \\mathbf{AI}=\\mathbf{A} and \\mathbf{IA}=\\mathbf{A}, it is possible that the \\mathbf{I} is of different sizes in these equations, depending on the dimensions of \\mathbf{A}.\nIf \\mathbf{A} has m rows and n columns, in \\mathbf{AI}, it is assumed that \\mathbf{I} is of size n so that it is right-compatible with \\mathbf{A}. In \\mathbf{IA}, it is assumed that \\mathbf{I} is of size m so that it is left-compatible with \\mathbf{A}.\n\n7.1.2 The Identity Matrix in R\nTo create an identity matrix, use the diag function with a single integer as its argument. For example diag(6) produces a 6 by 6 identity matrix.\n\ndiag(6)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#multiplicative-inverse",
    "href": "matrices_operations.html#multiplicative-inverse",
    "title": "\n6  Matrices\n",
    "section": "\n7.2 Multiplicative Inverse",
    "text": "7.2 Multiplicative Inverse\nX multiplied by its multiplicative inverse yields the multiplicative identity, 1. The multiplicative inverse is also known as the reciprocal.\nX\\times \\frac{1}{X}=1\nAnother way to write the reciprocal is to give it an exponent of -1.\nX^{-1}=\\frac{1}{X}",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#matrix-inverse",
    "href": "matrices_operations.html#matrix-inverse",
    "title": "\n6  Matrices\n",
    "section": "\n7.3 Matrix Inverse",
    "text": "7.3 Matrix Inverse\nOnly square matrices have multiplicative inverses. Multiplying square matrix \\mathbf{A} by its inverse (\\mathbf{A}^{-1}) produces the identity matrix.\n\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}\nThe inverse matrix produces the identity matrix whether it is pre-multiplied or post-multiplied.\n\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}\nThe calculation of an inverse is quite complex and is best left to computers.\nAlthough only square matrics can have inverses, not all square matrices have inverses. The procedures for calculating the inverse of a matrix sometimes attempt to divide by 0, which is not possible. Because zero cannot be inverted (i.e., \\frac{1}{0} is undefined), any matrix that attempts division by 0 during the inversion process cannot be inverted.\nFor example, this matrix of ones has no inverse.\n\\begin{bmatrix}\n1 & 1\\\\\n1 & 1\n\\end{bmatrix}\nThere is no matrix we can multiply it by to produce the identity matrix. In the algorithm for calculating the inverse, division by 0 occurs, and the whole process comes to a halt. A matrix that cannot be inverted is called a singular matrix.\nCollinear means that at least one of the variables can be perfectly predicted from the other variables.\nThe covariance matrix of collinear variables is singular. In multiple regression, we use the inverse of the covariance matrix of the predictor variables to calculate the regression matrix. If the predictor variables are collinear, the regression coefficients cannot be calculated. For example, if Z=X+Y, we cannot use X, Y, and Z together as predictors in a multiple regression equation. Z is perfectly predicted from X and Y. In the calculation of the regression coefficients, division by 0 will be attempted, and the calculation can proceed no further.\nIf use to bother me that that collinear variables could not be used together as predictors. However, thinking a little further, revealed why it is impossible. The definition of a regression coefficient is the independent effect of a variable after holding the other predictors constant. If a variable is perfectly predicted by the other variables, that variable cannot have an independent effect. Controlling for the other predictor, the variable no longer varies. It become a constant. Contants have no effect.\nWhile regression with perfectly collinear predictors is impossible, regression with almost perfectly collinear predictors can produce strange and unstable results. For example, if we round Z, the rounding error makes Z nearly collinear with X and Y but not quite perfectly collinear with them. In this case, the regression will run but might give misleading results that might differ dramatically depending on how finely rounded Z is.\n\n7.3.1 Calculating Inverses in R\nYou would think that the inverse function in R would be called “inverse” or “inv” or something like that. Unintuitively, the inverse function in R is solve. The reason for this is that solve covers a wider array of problems than just the inverse. To see how, imagine that we have two matrices of known constants \\mathbf{A}_{m\\times m} and \\mathbf{B}_{m\\times n}. We also have a matrix of unknowns \\mathbf{X}_{m\\times n}. How do we solve this equation?\n\\mathbf{AX}=\\mathbf{B}\nWe can pre-multiply both sides of the equation by the inverse of \\mathbf{A}.\n\\begin{aligned}\\mathbf{AX}&=\\mathbf{B}\\\\\n\\mathbf{A}^{-1}\\mathbf{AX}&=\\mathbf{A}^{-1}\\mathbf{B}\\\\\n\\mathbf{IX}&=\\mathbf{A}^{-1}\\mathbf{B}\\\\\n\\mathbf{X}&=\\mathbf{A}^{-1}\\mathbf{B}\\end{aligned}\nYou may have encountered this kind of problem in an algebra class when you used matrices to solve systems of linear equations. For example, these equations:\n\\begin{aligned}\n2x -9y -2z &= 5\\\\\n-2x + 5y + 3z &= 3\\\\\n2x + 4y - 3z &= 12\n\\end{aligned}\ncan be rewritten as matrices\n\\begin{aligned}\\mathbf{AX}&=\\mathbf{B}\\\\\n\\begin{bmatrix}\n\\phantom{-}2 & -9 & -2\\\\\n-2 & \\phantom{-}5 & \\phantom{-}3\\\\\n\\phantom{-}2 & \\phantom{-}4 & -3\n\\end{bmatrix}\n\\begin{bmatrix}\nx  \\\\\ny \\\\\nz\n\\end{bmatrix}&=\n\\begin{bmatrix}\n5  \\\\\n3 \\\\\n12\n\\end{bmatrix}\n\\end{aligned}\nIn R, problems of this sort are solved like so:\nX -&gt; solve(A,B)\n\nA &lt;- matrix(c(2, -9, -2,\n             -2,  5,  3,\n              2,  4, -3),\n            nrow = 3,byrow = TRUE)\nB &lt;- matrix(c(5,3,-12),ncol = 1)\nX &lt;- solve(A,B)\nX\n\n     [,1]\n[1,]    2\n[2,]   -1\n[3,]    4\n\n\nIf \\mathbf{B} is unspecified in the solve function, it is assumed that it is the identity matrix and therefore will return the inverse of \\mathbf{A}. That is, if \\mathbf{B=I}, then\n\\begin{aligned}\n\\mathbf{AX}&=\\mathbf{B}\\\\\n\\mathbf{AX}&=\\mathbf{I}\\\\\n\\mathbf{A^{-1}AX}&=\\mathbf{A^{-1}I}\\\\\n\\mathbf{IX}&=\\mathbf{A^{-1}I}\\\\\n\\mathbf{X}&=\\mathbf{A^{-1}}\\\\\n\\end{aligned}\nThus, solve(A) is \\mathbf{A}^{-1}\n\nA &lt;- matrix(c(1,0.5,0.5,1),nrow = 2)\nA\n\n     [,1] [,2]\n[1,]  1.0  0.5\n[2,]  0.5  1.0\n\nAinverse &lt;- solve(A)\nAinverse\n\n           [,1]       [,2]\n[1,]  1.3333333 -0.6666667\n[2,] -0.6666667  1.3333333\n\nA %*% Ainverse\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\n\n\n\n\n\nYou Try\n\n\n\n\n\n\n\n\\begin{aligned}\n\\mathbf{A} &= \\begin{bmatrix}\n17 & 14 & 1 & 19\\\\\n11 & 2 & 12 & 14\\\\\n5 & 16 & 1 & 20\n\\end{bmatrix}\\\\[1ex]\n\\mathbf{B} &= \\begin{bmatrix}\n5 & 16 & 20\\\\\n9 & 9 & 12\\\\\n15 & 5 & 8\\\\\n12 & 8 & 17\n\\end{bmatrix}\\\\[1ex]\n\\mathbf{C} &= \\begin{bmatrix}\n5 & 16 & 20\\\\\n9 & 9 & 12\\\\\n15 & 5 & 8\\\\\n12 & 8 & 17\n\\end{bmatrix}\n\\end{aligned}\n\n\nMake a 3 \\times 3 identity matrix.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\ndiag(3)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\n\n(\\mathbf{AB})^{-1}=\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nsolve(A %*% B)\n\n             [,1]        [,2]        [,3]\n[1,] -0.004573830  0.01403053 -0.00667532\n[2,]  0.011859448  0.03171994 -0.04419406\n[3,] -0.004178158 -0.02857500  0.03284660\n\n\n\n\n\\mathbf{AB(AB)}^{-1}=\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\n(A %*% B) %*% solve(A %*% B)\n\n             [,1] [,2]          [,3]\n[1,] 1.000000e+00    0  0.000000e+00\n[2,] 4.440892e-16    1 -7.105427e-15\n[3,] 0.000000e+00    0  1.000000e+00\n\n\n\n\n\\mathbf{(C'C)^{-1}}=\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nsolve(t(C) %*% C)\n\n             [,1]        [,2]        [,3]\n[1,]  0.008075633  0.01097719 -0.01218111\n[2,]  0.010977186  0.06675299 -0.05145894\n[3,] -0.012181111 -0.05145894  0.04298947",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#creating-sums-with-matrices",
    "href": "matrices_operations.html#creating-sums-with-matrices",
    "title": "\n6  Matrices\n",
    "section": "\n7.4 Creating Sums with Matrices",
    "text": "7.4 Creating Sums with Matrices\nA non-bolded 1 is just the number one.\nA bolded \\mathbf{1} is a column vector of ones. For example,\n\n\\mathbf{1}_1=\\begin{bmatrix}\n1\n\\end{bmatrix}\\\\\n\\mathbf{1}_2=\\begin{bmatrix}\n1\\\\ 1\n\\end{bmatrix}\\\\\n\\mathbf{1}_3=\\begin{bmatrix}\n1\\\\ 1\\\\ 1\n\\end{bmatrix}\\\\\n\\vdots\\\\\n\\mathbf{1}_n=\\begin{bmatrix}\n1\\\\ 1\\\\ 1\\\\ \\vdots \\\\ 1\n\\end{bmatrix}\n\nLike the identity matrix, the length of \\mathbf{1} is usually inferred from context.\nThe one vector is used to create sums. Post multiplying a matrix by \\mathbf{1} creates a column vector of row sums.\nSuppose that\n\n\\mathbf{X}=\n\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}\n\n\n\\mathbf{X1}=\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n1\\\\ 1\n\\end{bmatrix}\n=\\begin{bmatrix}\n3\\\\\n7\n\\end{bmatrix}\n\nPre-multiplying by a transposed one matrix creates a row vector of column totals.\n\n\\mathbf{1'X}=\n\\begin{bmatrix}\n1& 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}\n=\\begin{bmatrix}\n4&6\n\\end{bmatrix}\n\nMaking a “one sandwich” creates the sum of the entire matrix.\n\n\\mathbf{1'X1}=\n\\begin{bmatrix}\n1& 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 2\\\\\n3 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\n1\\\\ 1\n\\end{bmatrix}\n=\\begin{bmatrix}\n10\n\\end{bmatrix}\n\nTo create a \\mathbf{1} vector that is compatible with the matrix it post-multiplies, use the ncol function inside the rep function:\n\nA &lt;- matrix(1:20,nrow = 4)\nOnes &lt;- matrix(1, nrow = ncol(A)) \nA %*% Ones\n\n     [,1]\n[1,]   45\n[2,]   50\n[3,]   55\n[4,]   60\n\n\nUse the nrow function to make a \\mathbf{1} vector that is compatible with the matrix it pre-multiplies:\n\nOnes &lt;- matrix(1, nrow = nrow(A))\nt(Ones) %*% A\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]   10   26   42   58   74\n\n\nOf course, creating \\mathbf{1} vectors like this can be tedious. Base R has convenient functions to calculate row sums, column sums, and total sums.\nrowSums(A) will add the rows of \\mathbf{A}:\n\nrowSums(A)\n\n[1] 45 50 55 60\n\n\ncolSums(A) with give the column totals of \\mathbf{A}:\n\ncolSums(A)\n\n[1] 10 26 42 58 74\n\n\nsum(A) will give the overall total of \\mathbf{A}:\n\nsum(A)\n\n[1] 210",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "matrices_operations.html#eigenvectors-and-eigenvalues",
    "href": "matrices_operations.html#eigenvectors-and-eigenvalues",
    "title": "\n6  Matrices\n",
    "section": "\n7.5 Eigenvectors and Eigenvalues",
    "text": "7.5 Eigenvectors and Eigenvalues\nConsider this equation related Square matrix \\mathbf{A}, column vector \\mathbf{x}, and column vector \\mathbf{b}:\n\\mathbf{A}\\vec{x}=\\vec{b}\nSquare matrix \\mathbf{A} scales and rotates vector \\vec{x} into vector \\vec{b}.\nIs there a non-zero vector \\mathbf{v} that \\mathbf{A} scales but does not rotate? If so, \\mathbf{v} is an eigenvector. The value \\lambda by which \\mathbf{v} is scaled is the eigenvalue.\n\\mathbf{A}\\vec{v}=\\lambda\\vec{v}\nEvery eigenvector that exists for matrix \\mathbf{A}, is accompanied by an infinite number of parallel vectors of varying lengths that are also eigenvectors. Thus, we focus on the unit eigenvectors and their accompanying eigenvalues.\nEigenvectors and eigenvalues are extremely important concepts in a wide variety of applications in many disciplines. For us, they play a pivotal role in principal components analyses, factor analysis, and multivariate analyses such as MANOVA.\nEigenvectors (via principal components) help us to summarize multivariate data with a smaller number of variables.\n\n7.5.1 Eigenvectors and Eigenvalues in R\nSuppose that matrix \\mathbf{A} is a correlation matrix:\n\n\n\n\\mathbf{A}=\\begin{bmatrix}\n1 & 0.8 & 0.5\\\\\n0.8 & 1 & 0.4\\\\\n0.5 & 0.4 & 1\n\\end{bmatrix} The word, orthogonal derives from the Greek word for “right-angled.” Orthogonal vectors are mutually perpendicular.\nBecause \\mathbf{A} is a 3 × 3 matrix, there are three orthogonal unit vectors that are eigenvectors, \\vec{v}_1, \\vec{v}_2, and \\vec{v}_3. We will collect the three eigenvectors as columns of matrix \\mathbf{V}:\n\n\n\n\\mathbf{V}= \\begin{bmatrix}\n\\overset{\\vec{v}_1}{.63} & \\overset{\\vec{v}_2}{.25} & \\overset{\\vec{v}_3}{.74}\\\\\n.61 & .44 & −.67\\\\\n.48 & −.87 & −.13\n\\end{bmatrix}\n\n\n\n\n\\boldsymbol{\\lambda} = \\{2.15,0.66,0.19\\}\n\nThe eigenvectors of correlation matrix A below, represent the orientation vectors of the ellipsoid that contains the multivariate normal data.\n\n\n\n\nFor symmetric matrices (e.g., correlation and covariance matrices), eigenvectors are orthogonal.\n\n\n\n\n\n\nYou Try\n\n\n\nExtract the eigenvalues and eigen vectors from correlation matrix rho.\n\nR=\\begin{bmatrix}\n1&.7\\\\\n.7&1\n\\end{bmatrix}\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nSuggested Solution\nrho &lt;- matrix(c(1,0.7,0.7,1),2)\neigenrho &lt;- eigen(rho)\neigenrho\n\neigen() decomposition\n$values\n[1] 1.7 0.3\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\n\n\n\n\nEigenvalues and eigenvectors are essential ingredients in principal component analysis, a useful data-reduction technique. Principal component analysis allow us to summarize many variables in a way that minimizes information loss. To take a simple example, if we measure the size of people’s right and left feet, we would have two scores per person. The two scores are highly correlated because the left foot and right foot are nearly the same size in most people. However, for some people the size difference is notable.\nPrincipal components transform the two correlated variables into two uncorrelated variables, one for the overall size of the foot and one for the difference between the two feet. The eigenvalues sum to the 2 (then number of variables being summarized) and are proportional to the variances of the principal components.\nEigenvectors have a magnitude of 1, but if they are scaled by the square root of the eigenvalues (and by the appropriate ), they become the principal axes of the ellipse that contains the 95% of the data:\n\n\n\n\n\n\n\nFigure 7.1: The eigenvectors of rho scaled by the square roots of the eigenvectors are proportional to the principal axes of the ellipse that contains 95% of the data.\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 7.1\n\n\n\n\n\n# the tri2cor converts a vector of correlations from the lower triangle of a correlation matrix into a full correlation matrix\nrho &lt;- tri2cor(.8, variable_names = c(\"x\", \"y\"))\neigenrho &lt;- eigen(rho)\n\n# Scaling factor for 95% of 2D ellipse\nz &lt;- sqrt(qchisq(.95,2))\n\nd_scaledeigenvectors &lt;- eigenrho$values %&gt;% \n  sqrt() %&gt;% \n  `*`(z) %&gt;% \n  diag() %&gt;% \n  `%*%`(t(eigenrho$vectors)) %&gt;% \n  `colnames&lt;-`(c(\"x\", \"y\")) %&gt;% \n  as_tibble()\n\nd_points &lt;- mvtnorm::rmvnorm(\n  n = 1000, \n  mean = c(x = 0, y = 0), \n  sigma = rho) %&gt;% \n  as_tibble()\n\ntibble(t = z,\n       alpha = c(.4,.2)) %&gt;% \n  mutate(data = pmap(list(t = t), \n                    ellipse::ellipse, \n                    x = rho,\n                    npoints = 1000) %&gt;% \n           map(as_tibble) %&gt;% \n           map(`colnames&lt;-`, \n               value = c(\"x\", \"y\"))) %&gt;% \n  unnest(data) %&gt;% \n  ggplot(aes(x,y)) + \n  geom_point(data = d_points, \n             pch = 16, \n             size = .5, \n             color = \"gray30\") +\n  geom_polygon(aes(alpha = alpha), fill = myfills[1]) +\n  geom_arrow_segment(\n    data = d_scaledeigenvectors,\n    aes(xend = x, \n        x = 0, \n        yend = y, \n        y = 0),\n    color = \"white\",\n    arrow_head = arrow_head_deltoid()) +\n  coord_equal(xlim = c(-3, 3),\n              ylim = c(-3, 3)) +\n  scale_x_continuous(labels = \\(x) WJSmisc::prob_label(x, 1),\n                     breaks = seq(-4, 4)) +\n  scale_y_continuous(labels = \\(x) WJSmisc::prob_label(x, 1),\n                     breaks = seq(-4, 4)) +\n  scale_alpha_identity() +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Matrix Algebra in R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Matrices--Operations</span>"
    ]
  },
  {
    "objectID": "variables.html",
    "href": "variables.html",
    "title": "\n7  Variables\n",
    "section": "",
    "text": "7.1 Nominal Scales\nFigure 7.2: A (nearly) true dichotomy\n\\small\\rm\\LaTeX Code for Figure 7.2\n\n\n\n\n\n% A (nearly) true dichotomy\n\n\\documentclass[tikz = true, border=0mm]{standalone}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning}\n\\usetikzlibrary{arrows,shapes}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}]\n\\definecolor{firebrick4}{HTML}{8B1A1A}\n\\definecolor{royalblue4}{RGB}{39,64,139}\n\\definecolor{myviolet}{HTML}{51315E}\n\n\\begin{document}\n    \\begin{tikzpicture}\n    [node distance=1.1cm and 1.1cm,\n    post/.style={-&gt;,\n                 black!70,\n                 shorten &gt;=2pt,\n                 shorten &lt;=2pt,\n                 &gt;=latex',\n                 very thick,\n                 font=\\large},\n    ob/.style={rectangle,\n               inner sep=1mm,\n               minimum width=2.5cm,\n               minimum height=2.5cm,\n               rounded corners=0.75mm,\n               font=\\large,\n               text = white}]\n    \\node[shape=diamond,\n          fill=myviolet,\n          rounded corners=0.5mm,\n          shape aspect=1.25,\n          text = white,\n          font = \\large] (Trisomy) at (0,0) {Trisomy 21?};\n    \\node [ob,\n           fill=firebrick4,\n           left=of Trisomy,\n           align=center] (Down) {Down\\\\Syndrome};\n    \\node [ob,\n          fill=royalblue4,\n          right=of Trisomy,\n          align=center] (NoDown) {No Down\\\\Syndrome};\n    \\draw[post] (Trisomy) -- (Down) node [pos=.46,above] {Yes};\n    \\draw[post] (Trisomy) -- (NoDown) node [pos=.46,above] {No};\n\\end{tikzpicture}\n\\end{document}\nIn the messy world of observable reality, few true nominal variables exist as defined here. Most so-called nominal variables in psychology are merely nominal-ish. With respect to Down syndrome, we could say that people either have three copies of the 21st chromosome or they do not. However, if we did say that—and meant it—we’d be wrong. In point of fact, there are many cases of partial trisomy. There are other cases of Down syndrome in which part of chromosome 21 is copied to another chromosome. However, because cases like this are sufficiently rare and because such distinctions are usually not of vital importance, Down Syndrome is treated as if it were a true nominal variable. Even though Down Syndrome might technically come in degrees (both phenotypically and in terms of the underlying chromosomal abnormalities), the distinction between having the condition and not having it is not arbitrary.\nFigure 7.3: A dichotomized continuum\nR Code for Figure 7.3\n\n\n\n\n\n# A dichotomized continuum\nggplot() + \n  # geom_vline(xintercept = 70, linetype = \"dashed\") + \n  stat_function(xlim = c(40, 70), \n                fun = dnorm, \n                args = list(mean = 100, sd = 15), \n                geom = \"area\", \n                fill = myfills[2], \n                n = 1000) + \n  stat_function(xlim = c(70, 160), \n                fun = dnorm, \n                args = list(mean = 100, sd = 15), \n                geom = \"area\", \n                fill = myfills[1], \n                n = 1000) + \n  geom_arrow_segment(data = tibble(x = c(71, 69), \n                             xend = c(165, 35), \n                             y = dnorm(100, 100, 15) * 1.024),\n               aes(x = x, \n                   y = y, \n                   xend = xend, \n                   yend = y), \n               color = myfills[1:2], \n               linewidth = .75,\n               arrow_head = arrow_head_deltoid()) + \n  geom_text(data = tibble(x = c(100, 52.5), \n                          y = dnorm(100, 100, 15) * 1.02,\n                          l = c(\"No Intellectual\\nDisability\", \n                                \"Intellectual\\nDisability\")),\n            aes(x = x, \n                y = y,\n                label = l),\n            size = ggtext_size(bsize),\n            vjust = -0.3, \n            lineheight = 1,\n            color = myfills[1:2]) +\n  annotate(\"segment\", \n           x = 70, \n           xend = 70, \n           linewidth = 0.25,\n           y = dnorm(70, 100, 15), \n           yend = dnorm(100, 100, 15) * 1.14,\n           linetype = \"dashed\",\n           color = \"gray30\") +\n  scale_x_continuous(\"IQ and Adaptive Functioning\", \n                     breaks = seq(40, 160, 15),\n                     expand = expansion(0)) + \n  scale_y_continuous(NULL, breaks = NULL, \n                     expand = expansion(c(.02,0))) + \n  # theme_minimal(base_size = bsize, base_family = bfont) + \n  ggthemes::theme_tufte(base_size = bsize) +\n  theme(\n    axis.ticks.x = element_line(linewidth = 0.25, colour = \"gray60\"),\n    axis.line.x = ggarrow::element_arrow(colour = \"gray60\", linewidth = .5, arrow_head = arrow_head_deltoid(), arrow_fins = arrow_head_deltoid())\n        # axis.line.x = element_line(size = 0.25),\n        # panel.grid = element_blank()\n        ) +\n  coord_cartesian(xlim = c(33, 167), clip = \"off\")\nWhen we list the categories of a nominal variable, the order in which we do so is mostly arbitrary. In the variable college major, no major intrinsically comes before any other. It is convenient to list the categories alphabetically, but the order will change as the names of college majors evolve and will differ from language to language. However, strict alphabetical order is not always logical or convenient. For example, in a variable such as ethnic identity, the number of possible categories is very large, and members of very small groups are given the option of writing in their answer next to the word “other.” To avoid confusion, the other category is placed at the end of the list rather than its alphabetical position.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "variables.html#nominal",
    "href": "variables.html#nominal",
    "title": "\n7  Variables\n",
    "section": "",
    "text": "In a nominal scale, we note only that some things are different from others and that they belong to two or more mutually exclusive categories. If we say that a person has Down syndrome (trisomy 21), we are implicitly using a nominal scale in which there are people with Down syndrome and people without Down syndrome (as in Figure 7.2). In a true nominal scale, there are no cases that fall between categories. To be sure, we might have some difficulty figuring out and reliably agreeing upon the category to which something belongs—but there is no conceptual space between categories.A nominal scale groups observations into unordered categories.In mutually exclusive categories nothing belongs to more than one category (at the same time and in the same sense).\n\n\n\n\n\nBy contrast, consider the diagnosis of intellectual disability. We might have only two categories in our coding scheme (Intellectual Disability: Yes or No), but it is widely recognized that the condition comes in degrees (e.g., none, borderline, mild, moderate, severe, and profound). Thus, intellectual disability is not even conceptually nominal. It is a continuum that has been divided at a convenient but mostly arbitrary point (Figure 7.3). Distinguishing between a dichotomous  variable that is nominal by nature and one that has an underlying continuum matters because there are statistics that apply only to the latter type of variable, such as the tetrachoric correlation coefficient (Pearson & Heron, 1913). Even so, in many procedures, the two types of dichotomies can be treated identically (e.g., comparing the means of two groups with an independent-samples t-test).A dichotomy is a division of something into two categories.\nPearson, K., & Heron, D. (1913). On Theories of Association. Biometrika, 9(1/2), 159–315. https://doi.org/10.2307/2331805\n\nWhat if the categories in a nominal scale are not mutually exclusive? For example, suppose that we have a variable in which people can be classified as having either Down syndrome or Klinefelter syndrome (a condition in which a person has two X chromosomes and one Y chromosome). Obviously, this is a false dichotomy. Most people have neither condition. Thus, we need to expand the nominal variable to have three categories: Down syndrome, Klinefelter syndrome, and neither. What if a person has both Down syndrome and Klinefelter syndrome? Okay, we just add a fourth category: both. This combinatorial approach is not so much a problem for some purposes (e.g., the ABO blood group system), but for many variables, it quickly becomes unwieldy. If we wanted to describe all chromosomal abnormalities with a single nominal variable, the number of combinations increases exponentially with each new category added. This might be okay if having two or more chromosomal abnormalities is very rare. If, however, the categories are not mutually exclusive and combinations are common enough to matter, it is generally easiest to make the variable into two or more nominal variables (Down syndrome: Yes or No; Klinefelter syndrome: Yes or No; Edwards syndrome: Yes or No; and so forth). Some false dichotomies are so commonly used that people know what you mean, even though they are incomplete (e.g., Democrat vs. Republican) or insensitive to people who do not fit neatly into any of the typical categories (e.g., male vs. female).In a false dichotomy, two alternatives are presented as if they are the only alternatives when, in fact, there are others available.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "variables.html#ordinal-scales",
    "href": "variables.html#ordinal-scales",
    "title": "\n7  Variables\n",
    "section": "\n7.2 Ordinal Scales",
    "text": "7.2 Ordinal Scales\nIn an ordinal scale, things are still classified by category, but the categories have a particular order. Suppose that we are conducting behavioral observations of a child in school and we record when the behavior occurred. The precise time at which the behavior occurred (e.g., 10:38 AM) may be uninformative. If the class keeps a fairly regular schedule, it might be more helpful to divide the day into categories such as early morning, recess, late morning, lunch, and afternoon. This way it is easy to see if behavior problems are more likely to occur in some parts of the daily schedule than in others. It does not matter that these divisions are of unequal lengths or that they do not occur at precisely the same time each day. In a true ordinal variable, the distance between categories is either undefined, unspecified, or irrelevant.An ordinal scale groups observations into ordered categories.\n\n\n\n\n\n\n\n\nFigure 7.4: In a Likert scale, the distances between categories are undefined.\n\n\n\nMost measurement in psychological assessment involves ordinal scales, though in many cases ordinal scales might appear to be other types of scales. Questionnaires that use Likert scales are clearly ordinal (e.g., Figure 7.4). Even though true/false items on questionnaires might seem like nominal scales, they are usually ordinal because the answer indicates whether a person has either more or less of an attribute. That is, more and less are inherently ordinal concepts. Likewise, ability test items are ordinal, even though correct vs. incorrect might seem like nominal categories. Ability tests are designed such that a correct response indicates more ability than an incorrect response. The ordinal nature of ability test items is especially clear in cases that allow for partial credit.\n\n\n\n\n\n\n\n\\small\\rm\\LaTeX Code for Figure 7.4\n\n\n\n\n\n% Likert Scale\n\\documentclass[tikz,border={60pt 10pt 60pt 0pt}]{standalone}\n\\usetikzlibrary{positioning, calc, arrows}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}]\n\\definecolor[named]{mycolor}{RGB}{39,64,139}\n\\begin{document}\n    \\begin{tikzpicture} [\n        font=\\normalsize,\n        xtext/.style={\n            align=center,\n            font=\\Large,\n            mycolor}\n        ]\n    \\foreach \\i [count = \\n] in {Strongly Disagree,\n                                 Disagree,\n                                 Neutral,\n                                 Agree,\n                                 Strongly Agree} {\n        \\node [xtext]  (n\\n) at (0, \\n * 2) {\\i};\n    }\n\n\n    \\foreach \\i [count=\\n] in {2,3,4,5}  {\n        \\coordinate (mid) at ($(n\\n)!.5!(n\\i)$);\n        \\draw[mycolor, very thick] (n\\n.north) -- (n\\i.south);\n        \\node[xshift = 6pt] at (mid) {?};\n}\n\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\nSome scales are only partially ordinal. For example, educational attainment is ordinal up to a certain point in most societies, but branches out as people acquire specialized training. For a career in psychology, the educational sequence is high school diploma, associate’s degree, bachelor’s degree, master’s degree, and doctoral degree.1 However, this is not the sequence for real estate agents, hair stylists, and pilots. If we wanted to compare educational degrees across professions, how would we rank them? For example, how would we compare a law degree with a doctorate in geology? Does one degree indicate higher educational attainment than the other? The answers depend on the criteria that we care about—and different people care about different things. Thus, it is difficult to say that we have an ordinal scale when we compare educational attainment across professions.\n1 Obviously, some of these degrees can be skipped, and the endpoint is different for different careers in psychology. Furthermore, not all degrees related to psychology fit neatly in this sequence (e.g., the school psychology specialist degree).2 Although paranoia is not traditionally considered a type of anxiety, it is clear that anxiety (about the possibly malevolent intentions of others) is a core feature of the trait.Like educational attainment, many psychological traits are more differentiated at some points in the continuum than at others. For example, as seen in Figure 7.5, it sometimes convenient to lump the various flavors of trait anxiety together at the low and middle range of distress and then to distinguish among them at the high end. It is difficult to say who is more anxious, a person who is extremely paranoid 2 or a person with a severe case of panic disorder. We can say that each is more anxious than the average person (an ordinal comparison) but each has a qualitatively different kind of anxiety. This problem is easily solved by simply talking about two different scales (paranoia and panic). However, there are different kinds of paranoia (e.g., different mixtures of hostility, fear, and psychosis) and different kinds of panic (e.g., panic vs. fear of panic). One can always divide psychological variables into ever narrower categories, making comparisons among and across related constructs problematic. At some point, we gloss over certain qualitative differences and treat them as if they were comparable, even though, strictly speaking, they are not.\n\n\n\n\n\n\n\n\nFigure 7.5: Anxiety is more differentiated at higher levels of distress\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 7.5\n\n\n\n\n\n# Anxiety is more differentiated at higher levels of distress\nbind_rows(\n  tibble(label = \"Paranoia\",\n         color = myfills[2],\n         x = c(0, 7, 8, 10),\n         y = c(4.8, 4.8, 3.5, 3.5)),\n  tibble(label = \"Obsession\",\n         color = colorspace::lighten(myfills[2], amount = 0.2),\n         x = c(0, 7, 8, 10),\n         y = c(4.9, 4.9, 4.5, 4.5)),\n  tibble(label = \"Worry\",\n         color = colorspace::lighten(myfills[1], amount = 0.2),\n         x = c(0, 7, 8, 10),\n         y = c(5.1, 5.1, 5.5, 5.5)),\n  tibble(label = \"Panic\",\n         color = myfills[1],\n         x = c(0, 7, 8, 10),\n         y = c(5.2, 5.2, 6.5, 6.5))) %&gt;% \n  # mutate(color = scales::col2hcl(color, c = 100 * ((x / 10) ^ 3))) %&gt;%\n  ggplot(aes(x,y)) + \n  ggforce::geom_bezier2(aes(group = label, color = color)) + \n  geom_text(aes(label = label, color = color), \n            hjust = 0,\n            data = . %&gt;% filter(x == 10), \n            nudge_x = 0.1,\n            size = ggtext_size(30)) + \n  geom_arrow(\n           linewidth = 1,\n           data = tibble(x = c(0,13.2), y = c(5,5)), \n           # xend = 10.5, \n           # yend = 5, \n           arrow_head = arrow_head_deltoid(),\n           linejoin = \"mitre\",\n           color = \"gray20\") + \n  annotate(geom = \"label\", \n           x = 7, \n           y = 5, \n           label = \"Distress Level\", \n           label.padding = margin(1,1,1,1, \"pt\"),\n           size = ggtext_size(26, 1), \n           label.size = 0, \n           color = \"gray20\") +\n  theme_void() + \n  theme(legend.position = \"none\") + \n  scale_color_identity() + \n  scale_x_continuous(expand = expansion(c(0, 0.21))) +\n  scale_y_continuous(expand = expansion(c(0.07, 0.03)))",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "variables.html#interval-scales",
    "href": "variables.html#interval-scales",
    "title": "\n7  Variables\n",
    "section": "\n7.3 Interval Scales",
    "text": "7.3 Interval Scales\nWith interval scales, not only are the numbers on the scale ordered, the distance between the numbers (i.e., intervals) is meaningful. A good example of an interval scale is the calendar year. The time elapsed from 1960 to 1970 is the same as the time elapsed from 1970 to 1980. By contrast, consider a standard Likert scale from a questionnaire. What is the distance between disagree and agree? Is it the same as the distance between agree and strongly agree? If it were, how would we know? In interval scales, all such mysteries disappear.In an interval scale the distance between numbers has a consistent meaning at every point on the scale.\nIt is not always easy to distinguish between an interval scale and an ordinal scale. Therapists sometimes ask clients to rate their distress “on a scale from 0 to 10.” Probably, in the mind of the therapist, the distance between each point of the scale is equal. In the mind of the client, however, it may not work that way. In Figure 7.6, a hypothetical client thinks of the distance between 9 and 10 as much greater than the distance between 0 and 1.\n\n\n\n\n\n\n\n\nFigure 7.6: Subjective units of distress may not be of equal length\n\n\n\n\n\n\n\n\n\n\n\\small\\rm\\LaTeX Code for Figure 7.6\n\n\n\n\n\n% Subjective Units of Distress\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{graphicx}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning, calc}\n\\usetikzlibrary{intersections}\n\\usetikzlibrary{decorations.pathreplacing}\n\\usetikzlibrary{decorations.text}\n\\usetikzlibrary{arrows,shapes,backgrounds, shadows,fadings}\n\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}]\n\n\\begin{document}\n\n\\begin{tikzpicture}[scale=1]\n\\definecolor{firebrick2}{RGB}{205,38,38};\n\\definecolor{royalblue2}{RGB}{67,110,238};\n\\pgfmathtruncatemacro{\\T}{10}\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=royalblue2,\n                        minimum size=5.5mm,\n                        white];\n\\foreach \\n in {0,...,\\T} \\node (\\n) at (0,\\n) {\\footnotesize{\\n}};\n\\foreach \\n [remember=\\n as \\lastn (initially 0)] in {0,...,\\T}\n\\draw (\\lastn) -- (\\n);\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=firebrick2,\n                        minimum size=5.5mm, white];\n\\def\\myarray{{0,0.63,1.45,2.36,3.33,4.35,5.42,6.52,7.65,8.81,11}};\n\\foreach \\x [count=\\xi] in {0,...,10}\n\\node (\\x) at (1,\\myarray[\\x]) {\\footnotesize{\\x}};\n\\foreach \\x [remember=\\x as \\lastx (initially 0)] in {0,...,\\T}\n\\draw (\\lastx) -- (\\x);\n\\tikzstyle{every node}=[align=left, font=\\small];\n\\node at (-1.8,5) {Scale you intend\\\\ your client to use\\\\ (equal intervals)};\n\\node at (2.8,5) {Scale your client\\\\ is actually using\\\\ (for now, at least)};\n\\end{tikzpicture}\n\\end{document}\n\n\n\n\nIt is doubtful that any subjectively scaled measurement is a true interval scale. Even so, it is clear that some ordinal scales are more interval-like than others. Using item response theory (Embretson & Reise, 2000), it is possible to sum many ordinal-level items and scale the total score such that it approximates an interval scale. It is important to note, however, that item response theory does not accomplish magic. The application of item response theory in this way is justified only if the ordinal items are measuring an underlying construct that is by nature at the interval (or ratio) level. No amount of statistical wizardry can alter the nature of the underlying construct. Sure, you can apply fancy math to the numbers, but a construct that is ordinal by nature will remain ordinal no matter what you do or convince yourself that you have done.\n\nEmbretson, S. E., & Reise, S. P. (2000). Item response theory for psychologists. Lawrence Erlbaum.\n\nMichell, J. (2004). Item response models, Pathological science and the shape of error: Reply to Borsboom and Mellenbergh. Theory & Psychology, 14(1), 121–129.\n\nMichell, J. (1997). Quantitative science and the definition of measurement in psychology. British Journal of Psychology, 88(3), 355–383. https://doi.org/10.1111/j.2044-8295.1997.tb02641.x\n\nMichell, J. (2008). Is psychometrics pathological science? Measurement, 6(1-2), 7–24.\nMost of the tools used in psychological assessment make use of ordinal scales and transform them such that they are treated as if they were interval scales. Is this defensible? Yes, a defense is possible (e.g., Michell, 2004) but not all scholars will be convinced by it (Michell, 1997, 2008). As an act of faith, I will assume that most of the scales used in psychological assessment (measures of abilities, personality traits, attitudes, interests, motivation, and so forth) are close enough to interval scales that they can be treated as such. In many instances, my faith may be misplaced, but where exactly can only be determined by high quality evidence. While I await such evidence, I try to balance my faith with moderate caution.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "variables.html#ratio-scales",
    "href": "variables.html#ratio-scales",
    "title": "\n7  Variables\n",
    "section": "\n7.4 Ratio Scales",
    "text": "7.4 Ratio Scales\nIn a ratio scale, zero represents something special: the absence of the quantity being measured. In an interval scale, there may be a zero, but the zero is just another number in the scale. For example, 0°C happens to be the freezing point of water at sea level but it does not represent the absence of heat.3 Ratio scales do not usually have negative numbers, but there are exceptions. For example, in a checking account balance, negative numbers indicate that the account is overdrawn. Still, a checking account balance is a true ratio scale because a zero indicates that there is no money in the account.A ratio scale has a true zero, in addition to all the properties of an interval scale.3 The absence of heat occurs at −273.15°C or 0°K.\nWhat does a true zero have to do with ratios? In interval scales, numbers can be added and subtracted but they cannot be sensibly divided. Why not? Because when you divide one number by another, you are creating a ratio. A ratio tells you how big one number is compared to another number. Well, how big is any number? The magnitude of a real number is its distance from zero (i.e., its absolute value). If zero is not a meaningful number on a particular scale, then ratios computed from numbers on that scale will not be meaningful. Therefore, because interval scales do not have a true zero, meaningful ratios are not possible. For example, although 20°C is twice as far from 0°C as 10°C, it does not mean that 20°C is twice as hot as 10°C. By contrast, these types of comparisons are possible on the Kelvin scale because 0°K is a true zero representing the complete absence of heat. That is, 20°K really is twice as hot as 10°K.\nIn psychological assessment, there are a few true ratio scales that are commonly used. Whenever anything is counted (e.g., counting how often a behavior occurs in a direct observation), it is a ratio scale. However, treating counts of behavior as ratio scales can be tricky. If I observe how many times a child speaks out of turn in class, and I use this as an index of impulsivity, it is no longer a ratio scale. Why? The actual variable, number of outbursts is a true ratio variable because 0 outbursts means the absence of outbursts. However, if I use the number of outbursts as a proxy variable for impulsivity, then 0 outbursts probably does not indicate the absence of impulsivity. At best it indicates lower levels of impulsivity. We can observed two children with 0 outbursts yet imagine that one child is still more impulsive than the other. This same problem exists for the measurement of reaction times. Reaction time is a true ratio scale because a reaction time of 0 means that no time has elapsed between the onset of the stimulus and the response. However, reaction time data used in clinical applications are often proxies for traits that are interval-level concepts, such as inattention on a continuous performance test. Why are psychological traits such as cognitive abilities, personality traits, and so forth interval-level concepts? Because we do not yet have any means of defining what, for example, zero intelligence or zero extroversion would look like. Attempts have been made (Jensen, 2006), but they have not yet proved persuasive.\n\nJensen, A. R. (2006). Clocking the mind: Mental chronometry and individual differences. Elsevier Science Limited.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "variables.html#sec:DiscreteVsContinuous",
    "href": "variables.html#sec:DiscreteVsContinuous",
    "title": "\n7  Variables\n",
    "section": "\n7.5 Discrete vs. Continuous Variables",
    "text": "7.5 Discrete vs. Continuous Variables\nInterval and ratio variables can be either discrete or continuous. Discrete variables can assume some values but not others. Once the list of acceptable values has been specified, there are no cases that fall between those values. For example, the number of bicycles a person owns is a discrete variable because the variable can assume only the non-negative integers. Fractions of bicycles are not considered. Discrete variables often take on integer values, but any set of exact, isolated values can be used in a discrete variable. For example, one could specify a discrete variable that is divided at every half point: 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, ….A discrete variable can only take on exact, isolated values from a specified list.\nWhen a variable can assume any value within a specified interval, the variable is said to be continuous. With a continuous variable, we can use fractions and decimals to achieve any level of precision that we desire. In Figure 7.7, blue circles present the set of integers from 0 to 10. No fractions between the integers occur in the variable. By contrast, the solid red line from 0 to 10 represents the values that can occur in a continuous variable. Any and all fractional values are possible, but in practice we must round to a feasibly useful level of precision.A continuous variable can take on any value within a specified range.\n\n\n\n\n\n\n\nFigure 7.7: Discrete variables have gaps whereas continuous variables have none.\n\n\n\n\n\n\n\n\n\n\n\n\\small\\rm\\LaTeX Code for Figure 7.7\n\n\n\n\n\n% Discrete Continuous\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{graphicx}\n\\usepackage{tikz}\n\\usetikzlibrary{positioning, calc}\n\\usetikzlibrary{intersections}\n\\usetikzlibrary{decorations.pathreplacing}\n\\usetikzlibrary{decorations.text}\n\\usetikzlibrary{arrows,shapes,backgrounds, shadows,fadings}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}[SmallCapsFont={Equity Caps A}, Numbers=Lining]\n\\begin{document}\n\n\\begin{tikzpicture}[xscale=0.8]\n\\definecolor{firebrick2}{RGB}{205,38,38};\n\\definecolor{royalblue2}{RGB}{67,110,238};\n\\node [anchor=west] at (10.5,1) {Discrete};\n\\node [anchor=west] at (10.5,1.75) {Continuous};\n\\foreach \\n in {0,...,10} {\n    \\node at (\\n,0.15) {\\n};\n    \\draw (\\n,0.35)--(\\n,0.45);\n}\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=royalblue2,\n                        minimum size=1mm];\n\\foreach \\n in {0,...,10} {\n    \\node at (\\n,1) {};\n}\n\\draw (0,0.45) --(10,0.45);\n\\tikzstyle{every node}=[draw=none,\n                        shape=circle,\n                        ball color=firebrick2,\n                        minimum size=1mm];\n\\draw [color=firebrick2,ultra thick] (0,1.75)--(10,1.75);                        \n\\node (r1) at (0,1.75) {};\n\\node  (r10)  at (10,1.75) {};\n\n\\end{tikzpicture}\n\\end{document}",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Variables</span>"
    ]
  },
  {
    "objectID": "distributions.html",
    "href": "distributions.html",
    "title": "\n8  Distributions\n",
    "section": "",
    "text": "8.1 Random Variables\nBecause we first learn about variables in an algebra class, we tend to think of variables as having values that can be solved for—if we have enough information about them. If I say that x is a variable and that x+6=8, we can use algebra to find that x must equal 2.\nFigure 8.1: Rolling a six-sided die is a process that creates a randomly ordered series of integers from 1 to 6.\nWhen we say that the throw of a six-sided die is a random variable, we are not talking about any particular throw of a particular die but, in a sense, every throw (that has ever happened or ever could happen) of every die (that has ever existed or could exist). Imagine an immense, roaring, neverending, cascading flow of dice falling from the sky. As each die lands and disappears, a giant scoreboard nearby records the relative frequencies of ones, twos, threes, fours, fives, and sixes. That’s a random variable.\nR Code for Figure 8.1\n\n\n\n\n\n# Function to make dice\nmakedice &lt;- function(i, id) {\n  x = switch(\n           i,\n           `1` = 0,\n           `2` = c(-1, 1),\n           `3` = c(-1, 1, 0),\n           `4` = c(-1, 1, -1, 1),\n           `5` = c(-1, 1, -1, 1, 0),\n           `6` = c(-1, 1, -1, 1, -1, 1)\n         )\n  y = switch(\n           i,\n           `1` = 0,\n           `2` = c(1,-1),\n           `3` = c(1,-1, 0),\n           `4` = c(1,-1,-1, 1),\n           `5` = c(1,-1,-1, 1, 0),\n           `6` = c(1,-1,-1, 1, 0, 0))\n  \n  tibble(id = id * 1,\n         i = i,\n         x = x,\n         y = y) %&gt;% \n    add_case(id = id + 0.5,\n             i = 0,\n             x = NA,\n             y = NA)\n}\n\n# Die radius\nr &lt;- 0.35\ndpos &lt;- 1.5\n\n# Die round arcs\ndround &lt;- tibble(x0 = c(-dpos, dpos, -dpos, dpos), \n                 y0 = c(dpos,dpos,-dpos,-dpos), \n                 r = r, \n                 start = c(-pi / 2, pi / 2, -pi, pi / 2) , \n                 end = c(0,0, -pi / 2,pi) )\n\n# Line segments\ndsegments &lt;- tibble(x = c(-dpos, dpos + r, dpos, -dpos - r), \n                    y = c(dpos + r,dpos,-dpos - r,-dpos),\n                    xend = c(dpos, dpos + r, -dpos,-dpos - r), \n                    yend = c(dpos + r,-dpos,-dpos - r, dpos))\n# Number of throws\nk &lt;- 50\n\n# Dot positions\nd &lt;- map2_df(sample(1:6,k, replace = T), 1:k, makedice)  \n\n# Plot\np &lt;- ggplot(d) + \n  geom_point(pch = 16, size = 50, aes(x,y))  + \n  geom_arc(aes(x0 = x0,\n               y0 = y0,\n               r = r,\n               start = start,\n               end = end,\n               linetype = factor(r)),\n           data = dround,\n           linewidth = 2) + \n  geom_segment(data = dsegments,\n               aes(x = x, y = y, xend = xend, yend = yend),\n               linewidth = 2) +\n  coord_equal() + \n  theme_void() + \n  theme(legend.position = \"none\") + \n  transition_manual(id) \n\n# Render animation\nanimate(p, \n        fps = 1,\n        device = \"svg\",\n        renderer = magick_renderer(), \n        width = 8,\n        height = 8)",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#random-variables",
    "href": "distributions.html#random-variables",
    "title": "\n8  Distributions\n",
    "section": "",
    "text": "Random variables are not like algebraic variables. Random variables simply take on values because of some random process. If we say that the outcome of a throw of a six-sided die is a random variable, there is nothing to “solve for.” There is no equation that determines the value of the die. Instead, it is determined by chance and the physical constraints of the die. That is, the outcome must be one of the numbers printed on the die, and the six numbers are equally likely to occur. This illustrates an important point. The word random here does not mean “anything can happen.” On a six-sided die, you will never roll a 7, 3.5, \\sqrt{\\pi}, −36,000, or any other number that does not appear on the six sides of the die. Random variables have outcomes that are subject to random processes, but those random processes do have constraints on them such that some outcomes are more likely than others—and some outcomes never occur at all.Random variables have values that are determined by a random process.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#sets",
    "href": "distributions.html#sets",
    "title": "\n8  Distributions\n",
    "section": "\n8.2 Sets",
    "text": "8.2 Sets\nA set refers to a collection of objects. Each distinct object in a set is an element.A set is a collection of distinct objects.An element is a distinct member of a set.\n\n8.2.1 Discrete Sets\nTo show that a list of discrete elements is a discrete set, we can use curly braces. For example, the set of positive single-digit even numbers is \\{2, 4, 6, 8\\}. With large sets with repeating patterns, it is convenient to use an ellipsis (“…”), the punctuation mark signifying an omission or pause. For example, rather than listing every two-digit positive even number, we can show the pattern like so:A discrete set has numbers that are isolated, meaning that each number has a range in which it is the only number in the overall set.\n\\{10, 12, 14,\\ldots, 98\\}\nIf we want the pattern to repeat forever, we can set an ellipsis on the left, right, or both sides. The set of odd integers extends to infinity in both directions:\n\\{\\ldots, -5, -3, -1, 1, 3, 5, \\ldots\\}\n\n8.2.2 Interval Sets\nWith continuous variables, we can define sets in terms of intervals. Whereas the discrete set \\{0,1\\} refers just to the numbers 0 and 1, the interval set (0,1) refers to all the numbers between 0 and 1.Intervals are a continous range of numbers.\n\n\n\n\n\n\n\n\nFigure 8.2: Interval Notation\n\n\n\nAs shown in Figure 8.2, some intervals include their endpoints and others do not. Intervals noted with square brackets include their endpoints and intervals written with parentheses exclude them. Some intervals extend to positive or negative infinity: (-\\infty,5] and (-8,+\\infty). Use a parenthesis with infinity instead of a square bracket because infinity is not a specific number that can be included in an interval.\n\n\n\n\n\n\n\nR Code for Figure 8.2\n\n\n\n\n\n# Interval notation\ntibble(lb = 1L,\n       ub = 5L,\n       y = 1:4,\n       meaning = c(\"includes 1 and 5\",\n                   \"excludes 1 and 5\",\n                   \"includes 1 but not 5\",\n                   \"includes 5 but not 1\"),\n       l_bracket = c(\"[\", \"(\", \"[\", \"(\"),\n       u_bracket = c(\"]\", \")\", \")\", \"]\")) %&gt;% \n  mutate(Interval = paste0(l_bracket, lb, \",\", ub, u_bracket) %&gt;% \n           fct_inorder() %&gt;% \n           fct_rev,\n         l_fill = ifelse(l_bracket == \"[\", myfills[1], \"white\"),\n         u_fill = ifelse(u_bracket == \"]\", myfills[1], \"white\")) %&gt;% \n  ggplot(aes(lb, Interval)) + \n  geom_segment(aes(xend = ub, yend = Interval), \n               linewidth = 2, \n               color = myfills[1]) +\n  geom_point(aes(fill = l_fill), \n             size = 5, \n             pch = 21, \n             stroke = 2, color = myfills[1]) + \n  geom_point(aes(fill = u_fill, x = ub), \n             size = 5, \n             pch = 21, \n             stroke = 2, \n             color = myfills[1]) +\n  geom_label(aes(label = paste(Interval, meaning), x = 3), \n             vjust = -.75, \n             label.padding = unit(0,\"lines\"), \n             label.size = 0, \n             family = bfont, \n             size = ggtext_size(27)) +\n  scale_fill_identity() +\n  scale_y_discrete(NULL, expand = expansion(c(0.08, 0.25))) +\n  scale_x_continuous(NULL, minor_breaks = NULL) +\n  theme_minimal(base_size = 27, \n                base_family = bfont) + \n  theme(axis.text.y = element_blank(), \n        panel.grid.major = element_blank(),\n        axis.line.x = element_line(linewidth = .5),\n        axis.ticks.x = element_line(linewidth = .25), \n        plot.margin = margin())",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#sec:SampleSpace",
    "href": "distributions.html#sec:SampleSpace",
    "title": "\n8  Distributions\n",
    "section": "\n8.3 Sample Spaces",
    "text": "8.3 Sample Spaces\nThe set of all possible outcomes of a random variable is the sample space. Continuing with our example, the sample space of a single throw of a six-sided die is the set \\{1,2,3,4,5,6\\}. Sample space is a curious term. Why sample and why space? With random variables, populations are infinitely large, at least theoretically. Random variables just keep spitting out numbers forever! So any time we actually observe numbers generated by a random variable, we are always observing a sample; actual infinities cannot be observed in their entirety. A space is a set that has mathematical structure. Most random variables generate either integers or real numbers, both of which are structured in many ways (e.g., order).A sample space is the set of all possible values that a random variable can assume.A population consists of all entities under consideration.A sample is a subset of a population.\nUnlike distributions having to do with dice, many distributions have a sample space with an infinite number of elements. Interestingly, there are two kinds of infinity we can consider. A distribution’s sample space might be the set of whole numbers: \\{0,1,2,...\\}, which extends to positive infinity. The sample space of all integers extends to infinity in both directions: \\{...-2,-1,0,1,2,...\\}.\nThe sample space of continuous variables is infinitely large for another reason. Between any two points in a continuous distribution, there is an infinite number of other points. For example, in the beta distribution, the sample space consists of all real numbers between 0 and 1: (0,1). Many continuous distributions have sample spaces that involve both kinds of infinity. For example, the sample space of the normal distribution consists of all real numbers from negative infinity to positive infinity: (-\\infty, +\\infty).",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#sec:ProbabilityDistribution",
    "href": "distributions.html#sec:ProbabilityDistribution",
    "title": "\n8  Distributions\n",
    "section": "\n8.4 Probability Distributions",
    "text": "8.4 Probability Distributions\nEach element of a random variable’s sample space occurs with a particular probability. When we list the probabilities of each possible outcome, we have specified the variable’s probability distribution. In other words, if we know the probability distribution of a variable, we know how probable each outcome is. In the case of a throw of a single die, each outcome is equally likely (Figure 8.3).In a probability distribution, there is an assignment of a probability to each possible element in a variable’s sample space.\n\n\n\n\n\n\n\n\nFigure 8.3: The probability distribution of a throw of a single die\n\n\n\n\n\n\n\n\n\n\n\\small\\rm\\LaTeX Code for Figure 8.3\n\n\n\n\n\n% Dice PMF\n\n\\documentclass[tikz = true, border = 2pt]{standalone}\n\\usepackage{tikz}\n\\usepackage{xfrac}\n\\usetikzlibrary{shapes,calc}\n\\usepackage{fontspec}\n\\setmainfont{Equity Text A}\n\\tikzset{\n    dot hidden/.style={},\n    line hidden/.style={},\n    dot colour/.style={dot hidden/.append style={color=#1}},\n    dot colour/.default=black,\n    line colour/.style={line hidden/.append style={color=#1}},\n    line colour/.default=black\n}\n\n\\usepackage{xparse}\n\\NewDocumentCommand{\\drawdie}{O{}m}{\n    \\begin{tikzpicture}[x=1em,y=1em,radius=0.1,#1]\n    \\draw[rounded corners=2,line hidden] (0,0) rectangle (1,1);\n    \\ifodd#2\n    \\fill[dot hidden] (0.5,0.5) circle;\n    \\fi\n    \\ifnum#2&gt;1\n    \\fill[dot hidden] (0.25,0.25) circle;\n    \\fill[dot hidden] (0.75,0.75) circle;\n    \\ifnum#2&gt;3\n    \\fill[dot hidden] (0.25,0.75) circle;\n    \\fill[dot hidden] (0.75,0.25) circle;\n    \\ifnum#2&gt;5\n    \\fill[dot hidden] (0.75,0.5) circle;\n    \\fill[dot hidden] (0.25,0.5) circle;\n    \\ifnum#2&gt;7\n    \\fill[dot hidden] (0.5,0.75) circle;\n    \\fill[dot hidden] (0.5,0.25) circle;\n    \\fi\n    \\fi\n    \\fi\n    \\fi\n    \\end{tikzpicture}\n}\n\n\\begin{document}\n    \\begin{tikzpicture}\n    \\foreach \\n in {1,...,6} {\n        \\node at ($(0,7)-(0,\\n)$) {\\drawdie [scale = 2]{\\n}};\n        \\node [fill=gray!50,\n               minimum height = 2cm,\n               minimum width = 0.1cm,\n               single arrow,\n               single arrow head extend =.15cm,\n               single arrow head indent =.08cm,\n               inner sep=1mm] at ($(1.55,7)-(0,\\n)$) {};\n        \\node  (p1) at (3,\\n) {\\large{$\\sfrac{\\text{1}}{\\text{6}}$}};\n    }\n    \\node [text centered,\n           anchor=south,\n           text height = 1.5ex,\n           text depth = .25ex] (p3) at (0,6.6) {\\large{{Sample Space}}};\n    \\node [text centered,\n           anchor = south,\n           text height = 1.5ex,\n           text depth = .25ex] (p4) at (3,6.6) {\\large{{Probability}}};\n    \\end{tikzpicture}\n\\end{document}\n\n\n\n\nThere is an infinite variety of probability distributions, but a small subset of them have been given names. Now, one can manage one’s affairs quite well without ever knowing what a Bernoulli distribution is, or what a \\chi{^2} distribution is, or even what a normal distribution is. However, sometimes life is a little easier if we have names for useful things that occur often. Most of the distributions with names are not really single distributions, but families of distributions. The various members of a family are unique but they are united by the fact that their probability distributions are generated by a particular mathematical function (more on that later). In such cases, the probability distribution is often represented by a graph in which the sample space is on the X-axis and the associated probabilities are on the Y-axis. In Figure 8.4, 16 probability distributions that might be interesting and useful to clinicians are illustrated. Keep in mind that what are pictured are only particular members of the families listed; some family members look quite different from what is shown in Figure 8.4.\n\n\n\n\n\n\n\nFigure 8.4: A gallery of useful distributions\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 8.4\n\n\n\n\n\n# A gallery of useful distributions\n# Run output file pdfIllustration.tex in LaTeX\n# pdflatex --enable-write18 --extra-mem-bot=10000000 --synctex=1 pdfIllustration.tex\ntikzpackages &lt;- paste(\n  \"\\\\usepackage{tikz}\",\n  \"\\\\usepackage{amsmath}\",\n  \"\\\\usepackage[active,tightpage,psfixbb]{preview}\",\n  \"\\\\PreviewEnvironment{pgfpicture}\",\n  collapse = \"\\n\"\n)\n\ntikzDevice::tikz('pdfIllustration.tex',\n                 standAlone = TRUE, \n                 packages = tikzpackages, \n                 width = 11, \n                 height = 11)\npar(\n  mar = c(1.75, 1.3, 1.75, 0),\n  mfrow = c(4, 4),\n  las = 1,\n  xpd = TRUE,\n  family = 'serif',\n  pty = \"s\",\n  mgp = c(2, 0.5, 0),\n  tcl = -0.3,\n  cex = 1.35\n)\n\n# Bernoulli\nplot(\n  c(0.2, 0.8) ~ seq(0, 1),\n  type = \"b\",\n  ylim = c(0, 1),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Bernoulli\",\n  lty = 3,\n  pch = 19,\n  xaxp = c(0, 1, 1),\n  xlim = c(-0.1, 1)\n)\ntext(x = .7, y = .8, \"$p=0.8$\")\n\n# Binomial\nplot(\n  dbinom(seq(0, 5), 5, 0.2) ~ seq(0, 5),\n  type = \"b\",\n  xlim = c(-0.1, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Binomial\",\n  lty = 3,\n  pch = 19\n)\ntext(\n  x = c(3.5, 3.5),\n  y = c(.35, .25),\n  c(\"$p=0.2$\", \"$n=5$\"),\n  adj = 0\n)\n\n# Poisson\nplot(\n  dpois(0:10, 3) ~ seq(0, 10),\n  type = \"b\",\n  xlim = c(-0.1, 10),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Poisson\",\n  lty = 3,\n  pch = 19\n)\ntext(x = 7, y = .15, r\"($\\lambda=3$)\")\n\n# Geometric\nplot(\n  dgeom(0:4, prob = 0.8) ~ seq(0, 4),\n  type = \"b\",\n  ylim = c(0, .8),\n  xlim = c(-0.1, 4),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Geometric\",\n  lty = 3,\n  pch = 19\n)\ntext(x = 2, y = .6, \"$p=0.8$\")\n\n# Discrete Uniform\nplot(\n  rep(1 / 4, 4) ~ seq(1, 4),\n  type = \"b\",\n  ylim = c(0, 1),\n  xlim = c(0, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 1,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Discrete Uniform\",\n  lty = 3,\n  pch = 19\n)\ntext(\n  x = c(1, 4),\n  y = c(.5, .5),\n  c(\"$a=1$\", \"$b=4$\"),\n  adj = 0.5\n)\n\n# Continuous\nplot(\n  c(0, 1 / 3, 1 / 3, 0) ~ c(1, 1, 4, 4),\n  type = \"n\",\n  ylim = c(0, 1),\n  xlim = c(0, 5),\n  bty = \"n\",\n  col = myfills[1],\n  lwd = 2,\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Continuous Uniform\"\n)\npolygon(\n  c(1, 1, 4, 4),\n  c(0, 1 / 3, 1 / 3, 0),\n  col = myfills[1],\n  xpd = FALSE,\n  border = NA\n)\ntext(\n  x = c(1, 4),\n  y = c(.5, .5),\n  c(\"$a=1$\", \"$b=4$\"),\n  adj = 0.5\n)\n\n# Normal\nx &lt;- seq(-4, 4, 0.02)\nplot(\n  dnorm(x) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Normal\",\n  lwd = 2,\n  bty = \"n\",\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dnorm(x), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\naxis(2)\n\ntext(\n  x = c(-1.5, -1.5),\n  y = c(.35, .25),\n  c(\"$\\\\mu=0$\", \"$\\\\sigma^2=1$\"),\n  adj = 1\n)\n\n\ncenter_neg &lt;- function(x) {\n  signs &lt;- sign(x)\n  paste0(ifelse(signs &lt; 0,\"$\",\"\"), x, ifelse(signs &lt; 0,\"\\\\phantom{-}$\",\"\"))\n}\n\nall_tick_labels &lt;- function(side = 1, at, labels = at) {\n  axis(side, labels = rep(\"\",length(at)), at = at)\nfor (i in 1:length(at)) {\n  axis(side, \n       at = at[i], \n       labels = labels[i],\n       tick = F)\n  }\n}\naxis_ticks &lt;- seq(-4,4,2)\naxis_labs &lt;- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels = axis_labs)\n\n\n# Student t\nx &lt;- seq(-6, 6, 0.02)\nplot(\n  dt(x, 2) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Student's $\\\\boldsymbol{t}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.4),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dt(x, 2), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\ntext(x = 3, y = .3, \"$\\\\nu=2$\")\naxis(2)\naxis_ticks &lt;- seq(-6,6,2)\naxis_labs &lt;- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels = axis_labs)\n\n# Chi-Square\nx &lt;- seq(0, 40, 0.05)\nplot(\n  dchisq(x, 13) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"$\\\\boldsymbol{\\\\chi^2}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.1)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dchisq(x, 13), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = FALSE,\n  border = NA\n)\ntext(x = 20, y = .08, \"$k=2$\", adj = 0)\n\n# F\nx &lt;- seq(0, 6, 0.01)\nplot(\n  df(x, 3, 120) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"$\\\\boldsymbol{F}$\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.8)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, df(x, 3, 120), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(2, 2),\n  y = c(.6, .4),\n  c(\"$d_1=3$\", \"$d_2=120$\"),\n  adj = 0\n)\n\n# Weibull\nx &lt;- seq(6.5, 11.5, 0.01)\nplot(\n  dweibull(x, 20, 10) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Weibull\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.8)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dweibull(x, 20, 10), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(7, 7),\n  y = c(.6, .4),\n  c(\"$k=20$\", \"$\\\\lambda=10$\"),\n  adj = 0\n)\n\n# Beta\nx &lt;- seq(0, 1, 0.01)\nplot(\n  dbeta(x, 2, 2.5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Beta\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 2),\n  xaxp = c(0, 1, 1)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dbeta(x, 2, 2.5), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(0.75, 0.75),\n  y = c(1.75, 1.25),\n  c(\"$\\\\alpha=2$\", \"$\\\\beta=2.5$\"),\n  adj = 0\n)\n\n# Log Normal\nx &lt;- c(seq(0, .999, 0.001), seq(1, 15, .05))\nplot(\n  dlnorm(x, 2, .5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Log Normal\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.4)\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dlnorm(x, 1, 0.5), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(7, 7),\n  y = c(.3, .2),\n  c(\"$\\\\mu=2$\", \"$\\\\sigma=0.5$\"),\n  adj = 0\n)\n\n\n# Skew Normal\nx &lt;- seq(-4, 4, 0.01)\nplot(\n  sn::dsn(x, 2, 2.5) ~ x,\n  type = \"n\",\n  col = myfills[1],\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Skew Normal\",\n  lwd = 2,\n  bty = \"n\",\n  ylim = c(0, 0.5),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, sn::dsn(\n    x,\n    xi = 1,\n    omega = 1.5,\n    alpha = -4\n  ), 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(-3.9, -3.9, -3.9),\n  y = c(.45, .35, .25),\n  c(\"$\\\\xi=1$\", \"$\\\\omega=1.5$\", \"$\\\\alpha=-4$\"),\n  adj = 0\n)\n\naxis(2)\naxis_ticks &lt;- seq(-4,4,2)\naxis_labs &lt;- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels =axis_labs)\n\n# Normal Mixture\nx &lt;- seq(-4, 4, 0.01)\ny &lt;- (dnorm(x, -2, 0.5) * .25 + dnorm(x))\nplot(\n  y ~ x,\n  type = \"n\",\n  col = \"violet\",\n  xlab = \"\",\n  ylab = \"\",\n  main = \"Normal Mixture\",\n  lwd = 4,\n  bty = \"n\",\n  ylim = c(0, 0.5),\n  axes = F\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, y, 0),\n  col = myfills[1],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\npolygon(\n  c(min(x), x, max(x)),\n  c(0, dnorm(x, -2, 0.5) * .25, 0),\n  col = myfills[2],\n  lwd = 1,\n  xpd = TRUE,\n  border = NA\n)\ntext(\n  x = c(-0.85, -0.85),\n  y = c(.45, .35),\n  c(\"$\\\\mu_1=-2$\", \"$\\\\sigma_1^2=0.5$\"),\n  adj = 1\n)\ntext(\n  x = c(3.5, 3.5),\n  y = c(.45, .35),\n  c(\"$\\\\mu_2=0$\", \"$\\\\sigma_2^2=1$\"),\n  adj = 1\n)\naxis(2)\naxis_ticks &lt;- seq(-4,4,2)\naxis_labs &lt;- center_neg(axis_ticks)\nall_tick_labels(1, at = axis_ticks, labels =axis_labs)\n# Bivariate Normal\n\nx = seq(-4, 4, 0.1)\nX = fMultivar::grid2d(x)\nz = fMultivar::dnorm2d(X$x, X$y, rho = 0.6)\nZ = list(x = x,\n         y = x,\n         z = matrix(z, ncol = length(x)))\npersp(\n  Z,\n  theta = 20,\n  phi = 25,\n  col = \"royalblue1\",\n  xlab = \"\\nX\",\n  ylab = \"\\nY\",\n  zlab = \"\",\n  zlim = c(0, .20),\n  border = NA,\n  expand = .7,\n  box = FALSE,\n  ticktype = \"simple\",\n  ltheta = 0,\n  shade = 0.5,\n  main = \"Bivariate Normal\",\n  lwd = 0.5\n)\ntext(c(-.11, .32), c(-.42, -.25), c(\"X\", \"Y\"))\ntext(0, 0.25, \"$\\\\rho_{XY}=0.6$\")\ndev.off()",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#continuous-distributions",
    "href": "distributions.html#continuous-distributions",
    "title": "\n8  Distributions\n",
    "section": "\n9.1 Continuous Distributions",
    "text": "9.1 Continuous Distributions\n\n9.1.1 Probability Density Functions\nAlthough there are many more discrete distribution families, we will now consider some continuous distribution families. Most of what we have learned about discrete distributions applies to continuous distributions. However, there is a need of a name change for the probability mass function. In a discrete distribution, we can calculate an actual probability for a particular value in the sample space. In continuous distributions, doing so can be tricky. We can always calculate the probability that a score in a particular interval will occur. However, in continuous distributions, the intervals can become very small, approaching a width of 0. When that happens, the probability associated with that interval also approaches 0. Yet, some parts of the distribution are more probable than others. Therefore, we need a measure of probability that tells us the probability of a value relative to other values: the probability density functionThe probability density function is function that can show relative likelihoods of sample space elements of a continuous random variable.\nConsidering the entire sample space of a discrete distribution, all of the associated probabilities from the probability mass function sum to 1. In a probability density function, it is the area under the curve that must sum to 1. That is, there is a 100% probability that a value generated by the random variable will be somewhere under the curve. There is nowhere else for it to go!\nHowever, unlike probability mass functions, probability density functions do not generate probabilities. Remember, the probability of any value in the sample space of a continuous variable is infinitesimal. We can only compare the probabilities to each other. To see this, compare the discrete uniform distribution and continuous uniform distribution in Figure 8.4. Both distributions range from 1 to 4. In the discrete distribution, there are 4 points, each with a probability of ¼. It is easy to see that these 4 probabilities of ¼ sum to 1. Because of the scale of the figure, it is not easy to see exactly how high the probability density function is in the continuous distribution. It happens to be ⅓. Why? First, it does not mean that each value has a ⅓ probability. There are an infinite number of points between 1 and 4 and it would be absurd if each of them had a ⅓ probability. The distance between 1 and 4 is 3. In order for the rectangle to have an area of 1, its height must be ⅓. What does that ⅓ mean, then? In the case of a single value in the sample space, it does not mean much at all. It is simply a value that we can compare to other values in the sample space. It could be scaled to any value, but for the sake of convenience it is scaled such that the area under the curve is 1.\nNote that some probability density functions can produce values greater than 1. If the range of a continuous uniform distribution is less than 1, at least some portions of the curve must be greater than 1 to make the area under the curve equal 1. For example, if the bounds of a continuous distribution are 0 and ⅓, the average height of the probability density function would need to be 3 so that the total area is equal to 1.\n\n9.1.2 Continuous Uniform Distributions\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\nLower Bound\na  \\in (-\\infty,\\infty)\n\n\nUpper Bound\nb \\in (a,\\infty)\n\n\nSample Space\nx \\in \\lbrack a,b\\rbrack\n\n\nMean\n\\mu = \\frac{a+b}{2}\n\n\nVariance\n\\sigma^2 = \\frac{(b-a)^2-1}{12}\n\n\nSkewness\n\\gamma_1 = 0\n\n\nKurtosis\n\\gamma_2 = -\\frac{6}{5}\n\n\nProbability Density Function\nf_X(x;a,b) = \\frac{1}{b-a}\n\n\nCumulative Distribution Function\nF_X(x;a,b) = \\frac{x-a}{b-a}\n\n\n\n\n\n\nTable 9.6: Features of Continuous Discrete Distributions\n\n\n\nUnlike the discrete uniform distribution, the uniform distribution is continuous.6 In both distributions, there is an upper and lower bound and all members of the sample space are equally probable.\n6 For the sake of clarity, the uniform distribution is often referred to as the continuous uniform distribution.\n9.1.2.1 Generating random samples from the continuous uniform distribution\nTo generate a sample of n numbers with a continuous uniform distribution between a and b, use the runif function like so:\n\n# Sample size\nn &lt;- 1000\n# Lower and upper bounds\na &lt;- 10\nb &lt;- 30\n# Sample\nx &lt;- runif(n, min = a, max = b)\n\n\n\n\n\n\n\n\n\nFigure 9.23: Random sample (n = 1000) of a continuous uniform distribution between 10 and 30. Points are randomly jittered to show the distribution more clearly.\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.23\n\n\n\n\n\n# Plot\ntibble(x) %&gt;% \nggplot(aes(x, y = 0.5)) + \n  geom_jitter(size = 0.5, \n              pch = 16,\n              color = myfills[1], \n              height = 0.45) +\n  scale_x_continuous(NULL) +\n  scale_y_continuous(NULL, \n                     breaks = NULL, \n                     limits = c(0,1), expand = expansion()) + \n  theme_minimal(base_family = bfont, base_size = bsize)\n\n\n\n\n\n9.1.2.2 Using the continuous uniform distribution to generate random samples from other distributions\nUniform distributions can begin and end at any real number but one member of the uniform distribution family is particularly important—the uniform distribution between 0 and 1. If you need to use Excel instead of a statistical package, you can use this distribution to generate random numbers from many other distributions.\nThe cumulative distribution function of any continuous distribution converts into a continuous uniform distribution. A distribution’s quantile function converts a continuous uniform distribution into that distribution. Most of the time, this process also works for discrete distributions. This process is particularly useful for generating random numbers with an unusual distribution. If the distribution’s quantile function is known, a sample with a continuous uniform distribution can easily be generated and converted.\nFor example, the RAND function in Excel generates random numbers between 0 and 1 with a continuous uniform distribution. The BINOM.INV function is the binomial distribution’s quantile function. Suppose that n (number of Bernoulli trials) is 5 and p (probability of success on each Bernoulli trial) is 0.6. A randomly generated number from the binomial distribution with n=5 and p=0.6 is generated like so:\n=BINOM.INV(5,0.6,RAND())\nExcel has quantile functions for many distributions (e.g., BETA.INV, BINOM.INV, CHISQ.INV, F.INV, GAMMA.INV, LOGNORM.INV, NORM.INV, T.INV). This method of combining RAND and a quantile function works reasonably well in Excel for quick-and-dirty projects, but when high levels of accuracy are needed, random samples should be generated in a dedicated statistical program like R, Python (via the numpy package), Julia, STATA, SAS, or SPSS.\n\n9.1.3 Normal Distributions\n(Unfinished)\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\nSample Space\nx \\in (-\\infty,\\infty)\n\n\nMean\n\\mu = \\mathcal{E}\\left(X\\right)\n\n\nVariance\n\\sigma^2 = \\mathcal{E}\\left(\\left(X - \\mu\\right)^2\\right)\n\n\nSkewness\n\\gamma_1 = 0\n\n\nKurtosis\n\\gamma_2 = 0\n\n\nProbability Density Function\nf_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\n\nCumulative Distribution Function\nF_X(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} {\\displaystyle \\int_{-\\infty}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}\n\n\n\n\n\n\nTable 9.7: Features of Normal Distributions\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.24: Carl Friedrich Gauss (1777–1855)Image Credits\n\n\n\nThe normal distribution is sometimes called the Gaussian distribution after its discoverer, Carl Friedrich Gauss Figure 9.24. It is a small injustice that most people do not use Gauss’s name to refer to the normal distribution. Thankfully, Gauss is not exactly languishing in obscurity. He made so many discoveries that his name is all over mathematics and statistics.\nThe normal distribution is probably the most important distribution in statistics and in psychological assessment. In the absence of other information, assuming that an individual difference variable is normally distributed is a good bet. Not a sure bet, of course, but a good bet. Why? What is so special about the normal distribution?\nTo get a sense of the answer to this question, consider what happens to the binomial distribution as the number of events (n) increases. To make the example more concrete, let’s assume that we are tossing coins and counting the number of heads (p=0.5). In Figure 9.25, the first plot shows the probability mass function for the number of heads when there is a single coin (n=1)). In the second plot, n=2 coins. That is, if we flip 2 coins, there will be 0, 1, or 2 heads. In each subsequent plot, we double the number of coins that we flip simultaneously. Even with as few as 4 coins, the distribution begins to resemble the normal distribution, although the resemblance is very rough. With 128 coins, however, the resemblance is very close.\n\n\n\n\n\n\n\nFigure 9.25: The binomial distribution begins to resemble the normal distribution when the number of events is large.\n\n\n\n\nThis resemblance to the normal distribution in the example is not coincidental to the fact that p=0.5, making the binomial distribution symmetric. If p is extreme (close to 0 or 1), the binomial distribution is asymmetric. However, if n is large enough, the binomial distribution eventually becomes very close to normal.\nMany other distributions, such as the Poisson, Student’s T, F, and \\chi^2 distributions, have distinctive shapes under some conditions but approximate the normal distribution in others (See Figure 9.26). Why? In the conditions in which non-normal distributions approximate the normal distribution, it is because, like in Figure 9.25, many independent events are summed.\n\n\n\n\n\n\n\nFigure 9.26: Many distributions become nearly normal when their parameters are high.\n\n\n\n\n\n9.1.3.1 Notation for Normal Variates\nStatisticians write about variables with normal distributions so often that a compact notation for specifying a normal variable’s parameters was useful to develop. If I want to specify that X is a normally variable with a mean of \\mu and a variance of \\sigma^2, I will use this notation:\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\nX\nA random variable.\n\n\n\\sim\nIs distributed as\n\n\n\\mathcal{N}\nHas a normal distribution\n\n\n\\mu\nThe population mean\n\n\n\\sigma^2\nThe population variance\n\n\n\n\n\n\nTable 9.8: Features of Half-Normal Distributions\n\n\n\nMany authors list the standard deviation \\sigma instead of the variance \\sigma^2. When I specify normal distributions with specific means and variances, I will avoid ambiguity by always showing the variance as the standard deviation squared. For example, a normal variate with a mean of 10 and a standard deviation of 3 will be written as X \\sim \\mathcal{N}(10,3^2).\n\n\n\n\n\n\n\nFigure 9.27: Percentiles convert a distribution into a uniform distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.28: Evenly spaced percentile ranks are associated with unevenly spaced scores.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.29: Evenly spaced scores are associated with unevenly spaced percentiles\n\n\n\n\n\n9.1.3.2 Half-Normal Distribution\n(Unfinished)\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\nSample Space\nx \\in [\\mu,\\infty)\n\n\nMu\n\\mu \\in (-\\infty,\\infty)\n\n\nSigma\n\\sigma \\in [0,\\infty)\n\n\nMean\n\\mu + \\sigma\\sqrt{\\frac{2}{\\pi}}\n\n\nVariance\n\\sigma^2\\left(1-\\frac{2}{\\pi}\\right)\n\n\nSkewness\n\\sqrt{2}(4-\\pi)(\\pi-2)^{-\\frac{3}{2}}\n\n\nKurtosis\n8(\\pi-3)(\\pi-2)^{-2}\n\n\nProbability Density Function\nf_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi \\sigma ^ 2}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\n\nCumulative Distribution Function\nF_X(x;\\mu,\\sigma) = \\sqrt{\\frac{2}{\\pi\\sigma}} {\\displaystyle \\int_{\\mu}^{x} e ^ {-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}dx}\n\n\n\n\n\n\nTable 9.9: Features of Half-Normal Distributions\n\n\n\n\n\n\n\n\n\n\nFigure 9.30: The half-normal distribution is the normal distribution with the left half of the distribution stacked on top of the right half of the distribution.\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.30\n\n\n\n\n\n# Half normal distribution\nxlim &lt;- 4\nn &lt;- length(seq(-xlim, 0, 0.01))\nt1 &lt;- tibble(\n  x = c(0,-xlim,\n        seq(-xlim, 0, 0.01),\n        0,\n        0,\n        seq(0, xlim, 0.01),\n        xlim,\n        0),\n  y = c(0,\n        0,\n        dnorm(seq(-xlim, 0, 0.01)),\n        0,\n        0,\n        dnorm(seq(0, xlim, 0.01)),\n        0,\n        0),\n  side = c(rep(F, n + 3), rep(T, n + 3)),\n  Type = 1\n)\nt2 &lt;- t1 %&gt;%\n  mutate(y = if_else(side, y, 2 * y)) %&gt;%\n  mutate(x = abs(x),\n         Type = 2)\n\nbind_rows(t1, t2) %&gt;%\n  mutate(Type = factor(Type)) %&gt;%\n  ggplot(aes(x, y, fill = side)) +\n  geom_polygon() +\n  geom_text(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0.14,\n      Type = factor(c(1,2)),\n      label = c(\n        \"Normal\",\n        \"Half-Normal\"),\n      side = T),\n    aes(label = label),\n    family = bfont, fontface = \"bold\",\n    size = ggtext_size(30), \n    vjust = 1\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = 0,\n      y = dnorm(0) * c(1, 2) + 0,\n      Type = factor(c(1,2)),\n      label = c(\n        paste0(\"*X* ~ \",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(*\",\n               span_style(\"&mu;\"),\n               \"*, *\",\n               span_style(\"&sigma;\"),\n               \"*&lt;sup&gt;2&lt;/sup&gt;)\"),\n        paste0(\"*X* ~ |\",\n               span_style(\"N\", style = \"font-family:'Lucida Calligraphy'\"),\n               \"(0, *\",\n               span_style(\"&sigma;\"),\n               \"*&lt;sup&gt;2&lt;/sup&gt;)| + *\",\n               span_style(\"&mu;\"),\n               \"*\")),\n      side = T),\n    aes(label = label),\n    family = c(\"Equity Text A\"),\n    size = ggtext_size(30), \n    vjust = 0, \n    label.padding = unit(0,\"lines\"), \n    label.color = NA,\n    fill = NA) +\n  theme_void(base_size = 30,\n                base_family = bfont) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_blank()\n  ) +\n  scale_fill_manual(values = myfills) +\n  facet_grid(rows = vars(Type), space = \"free_y\", scales = \"free_y\") \n\n\n\n\nSuppose that X is a normally distributed variable such that\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\nVariable Y then has a half-normal distribution such that Y = |X-\\mu|+\\mu. In other words, imagine that a normal distribution is folded at the mean with the left half of the distribution now stacked on top of the right half of the distribution (See Figure 9.30).\n\n9.1.3.3 Truncated Normal Distributions\n(Unfinished)\n\n9.1.3.4 Multivariate Normal Distributions\n(Unfinished)\n\n9.1.4 Chi Square Distributions\n(Unfinished)\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\nSample Space\nx \\in [0,\\infty)\n\n\nDegrees of freedom\n\\nu \\in [0,\\infty)\n\n\nMean\n\\nu\n\n\nVariance\n2\\nu\n\n\nSkewness\n\\sqrt{8/\\nu}\n\n\nKurtosis\n12/\\nu\n\n\nProbability Density Function\nf_X(x;\\nu) = \\frac{x^{\\nu/2-1}}{2^{\\nu/2}\\;\\Gamma(\\nu/2)\\,\\sqrt{e^x}}\n\n\nCumulative Distribution Function\nF_X(x;\\nu) = \\frac{\\gamma\\left(\\frac{\\nu}{2},\\frac{x}{2}\\right)}{\\Gamma(\\nu/2 )}\n\n\n\n\n\n\nTable 9.10: Features of Chi-Square Distributions\n\n\n\nI have always thought that the \\chi^2 distribution has an unusual name. The chi part is fine, but why square? Why not call it the \\chi distribution?7 As it turns out, the \\chi^2 distribution is formed from squared quantities.\n7 Actually, there is a \\chi distribution. It is simply the square root of the \\chi^2 distribution. The half-normal distribution happens to be a \\chi distribution with 1 degree of freedom.Notation note: A \\chi^2 distribution with \\nu degrees of freedom can be written as \\chi^2_\\nu or \\chi^2(\\nu).\nThe \\chi^2 distribution has a straightforward relationship with the normal distribution. It is the sum of multiple independent squared normal variates. That is, if z is a standard normal variate— z\\sim\\mathcal{N}(0,1^2)—then z^2 has a \\chi^2 distribution with 1 degree of freedom (\\nu):\nz^2\\sim \\chi^2_1\nIf z_1 and z_2 are independent standard normal variates, the sum of their squares has a \\chi^2 distribution with 2 degrees of freedom:\nz_1^2+z_2^2 \\sim \\chi^2_2 If \\{z_1,z_2,\\ldots,z_{\\nu} \\} is a series of \\nu independent standard normal variates, the sum of their squares has a \\chi^2 distribution with \\nu degrees of freedom:\n\\sum^\\nu_{i=1}{z_i^2} \\sim \\chi^2_\\nu\n\n9.1.4.1 Clinical Uses of the \\chi^2 distribution\nThe \\chi^2 distribution has many applications, but the mostly likely of these to be used in psychological assessment is the \\chi^2 Test of Goodness of Fit and the \\chi^2 Test of Independence.\nSuppose we suspect that a child’s temper tantrums are more likely to occur on weekdays than on weekends. The child’s mother has kept a record of each tantrum for the past year and was able to count the frequency of tantrums. If tantrums were equally likely to occur on any day, 5 of 7 tantrums should occur on weekdays, and 2 of 7 tantrums should occur on weekends. The observed frequencies are compared with the expected frequencies below.\n\\begin{array}{r|c|c|c}\n& \\text{Weekday} & \\text{Weekend} & \\text{Total} \\\\\n\\hline\n\\text{Observed Frequency}\\, (o) & 14 & 13 & n=27\\\\\n\\text{Expected Proportion}\\,(p) & \\frac{5}{7} & \\frac{2}{7} & 1\\\\\n\\text{Expected Frequency}\\, (e = np)& 27\\times \\frac{5}{7}= 19.2857& 27\\times \\frac{2}{7}= 7.7143& 27\\\\\n\\text{Difference}\\,(o-e) & -5.2857 & 5.2857&0\\\\\n\\frac{(o-e)^2}{e} & 1.4487 & 3.6217 & \\chi^2 = 5.07\n\\end{array}\nIn the table above, if the observed frequencies (o_i) are compared to their respective expected frequencies (e_i), then:\n\\chi^2_{k-1}=\\sum_{i=1}^k{\\frac{(o_i-e_i)^2}{e_i}}=5.07\nUsing the \\chi^2 cumulative distribution function, we find that the probability of observing the frequencies listed is low under the assumption that tantrums are equally likely each day.\n\nobserved_frequencies &lt;- c(14, 13)\nexpected_probabilities &lt;- c(5,2) / 7\n\nfit &lt;- chisq.test(observed_frequencies, p = expected_probabilities)\nfit\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed_frequencies\nX-squared = 5.0704, df = 1, p-value = 0.02434\n\n# View expected frequencies and residuals\nbroom::augment(fit)\n\n# A tibble: 2 × 6\n  Var1  .observed .prop .expected .resid .std.resid\n  &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 A            14 0.519     19.3   -1.20      -2.25\n2 B            13 0.481      7.71   1.90       2.25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n\n\n\nB\n0\n1\n\n\n\n\n0\n36\n39\n\n\n1\n5\n20\n\n\n\n\n\n\n# A tibble: 4 × 9\n  A     B     .observed .prop .row.prop .col.prop .expected .resid .std.resid\n  &lt;fct&gt; &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 0     0            36  0.36     0.878      0.48      30.8  0.947       2.47\n2 1     0            39  0.39     0.661      0.52      44.2 -0.789      -2.47\n3 0     1             5  0.05     0.122      0.2       10.2 -1.64       -2.47\n4 1     1            20  0.2      0.339      0.8       14.8  1.37        2.47\n\n\n\n9.1.5 Student’s t Distributions\n\n\n\n\n\n\n\n\n\n\nFeature\nSymbol\n\n\n\nSample Space\nx \\in (-\\infty,\\infty)\n\n\nDegrees of Freedom\n\\nu \\in (0,\\infty)\n\n\nMean\n\\left\\{\n\\begin{array}{ll}\n      0 & \\nu \\gt  1 \\\\\n      \\text{Undefined} & \\nu \\le 1 \\\\\n\\end{array}\n\\right.\n\n\nVariance\n\\left\\{\n\\begin{array}{ll}\n      \\frac{\\nu}{\\nu-2} & \\nu\\gt 2 \\\\\n      \\infty & 1 \\lt \\nu \\le 2\\\\\n      \\text{Undefined} & \\nu \\le 1 \\\\\n\\end{array}\n\\right.\n\n\nSkewness\n\\left\\{\n\\begin{array}{ll}\n      0 & \\nu \\gt  3 \\\\\n      \\text{Undefined} & \\nu \\le 3 \\\\\n\\end{array}\n\\right.\n\n\nKurtosis\n\\left\\{\n\\begin{array}{ll}\n      \\frac{6}{\\nu-4} & \\nu \\gt 4 \\\\\n      \\infty & 2 \\lt \\nu \\le 4\\\\\n      \\text{Undefined} & \\nu \\le 2 \\\\\n\\end{array}\n\\right.\n\n\nProbability Density Function\nf_X(x; \\nu) = \\frac{\\Gamma(\\frac{\\nu+1}{2})} {\\sqrt{\\nu\\pi}\\,\\Gamma(\\frac{\\nu}{2})} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}\n\n\nCumulative Distribution Function\nF_X(x; \\nu)=\\frac{1}{2} + x \\Gamma \\left( \\frac{\\nu+1}{2} \\right)  \\frac{\\phantom{\\,}_{2}F_1 \\left(\\frac{1}{2},\\frac{\\nu+1}{2};\\frac{3}{2};-\\frac{x^2}{\\nu} \\right)} {\\sqrt{\\pi\\nu}\\,\\Gamma \\left(\\frac{\\nu}{2}\\right)}\n\n\n\n\n\n\nTable 9.11: Features of t DistributionsNotation note: \\Gamma is the gamma function. _2F_1 is the hypergeometric function.\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.31: “Student” statistician, William Sealy Gosset (1876–1937)Image Credit\n\n\n\n(Unfinished)\nGuinness Beer gets free advertisement every time the origin story of the Student t distribution is retold, and statisticians retell the story often. The fact that the original purpose of the t distribution was to brew better beer seems too good to be true.\nWilliam Sealy Gosset (1876–1937), self-trained statistician and head brewer at Guinness Brewery in Dublin, continually experimented on small batches to improve and standardize the brewing process. With some help from statistician Karl Pearson, Gosset used then-current statistical methods to analyze his experimental results. Gosset found that Pearson’s methods required small adjustments when applied to small samples. With Pearson’s help and encouragement (and later from Ronald Fisher), Gosset published a series of innovative papers about a wide range of statistical methods, including the t distribution, which can be used to describe the distribution of sample means.\nWorried about having its trade secrets divulged, Guinness did not allow its employees to publish scientific papers related to their work at Guinness. Thus, Gosset published his papers under the pseudonym, “A Student.” The straightforward names of most statistical concepts need no historical treatment. Few of us who regularly use the Bernoulli, Pareto, Cauchy, and Gumbell distributions could tell you anything about the people who discovered them. But the oddly named “Student’s t distribution” cries out for explanation. Thus, in the long run, it was Gosset’s anonymity that made him famous.\n\n\n\n\n\n\n\n\nFigure 9.32: The t distribution approaches the standard normal distribution as the degrees of freedom (df) parameter increases.\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.32\n\n\n\n\n\n# The t distribution approaches the normal distribution\nd &lt;- crossing(x = seq(-6,6,0.02), \n         df = c(seq(1,15,1),\n                seq(20,45,5),\n                seq(50,100,10),\n                seq(200,700,100))) %&gt;%\n  mutate(y = dt(x,df),\n         Normal = dnorm(x)) \n\nt_size &lt;- 40\n\nd_label &lt;- d %&gt;% \n  select(df) %&gt;% \n  unique() %&gt;% \n  mutate(lb = qt(.025, df),\n         ub = qt(0.975, df)) %&gt;% \n  pivot_longer(c(lb, ub), values_to = \"x\", names_to = \"bounds\") %&gt;% \n  mutate(label_x = signs::signs(x, accuracy = .01),\n         y = 0,\n         yend = dt(x, df))\n\np &lt;- ggplot(d, aes(x, y)) + \n  geom_area(aes(y = Normal), alpha = 0.25, fill = myfills[1]) +\n  geom_line() +\n  geom_area(data = . %&gt;% filter(x &gt;= 1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_area(data = . %&gt;% filter(x &lt;= -1.96), \n            alpha = 0.25, \n            fill = myfills[1],\n            aes(y = Normal)) +\n  geom_text(data = d_label, \n            aes(label = label_x), \n            family = bfont, \n            vjust = 1.25,\n            size = ggtext_size(t_size)) + \n  geom_text(data = d_label %&gt;% select(df) %&gt;% unique,\n            aes(x = 0, y = 0, label = paste0(\"df = \", df)), \n            vjust = 1.25, \n            family = bfont,\n            size = ggtext_size(t_size)) + \n  geom_segment(data = d_label, aes(xend = x, yend = yend)) +\n  transition_states(states = df, \n                    transition_length =  1, \n                    state_length = 2) +\n  theme_void(base_size = t_size, base_family = bfont) +\n  # labs(title = \"df = {closest_state}\") +\n  annotate(x = qnorm(c(0.025, 0.975)), \n           y = 0, \n           label = signs::signs(qnorm(c(0.025, 0.975)), accuracy = .01), \n           geom = \"text\", \n           size = ggtext_size(t_size),\n           color = myfills[1],\n           vjust = 2.6, \n           family = bfont) + \n  coord_cartesian(xlim = c(-6,6), ylim = c(-.045, NA)) \n\nanimate(p, \n        renderer = magick_renderer(), \n        device = \"svglite\", \n        fps = 2, \n        height = 6, \n        width = 10)\ngganimate::anim_save(\"tdist_norm.gif\")\n\n\n\n\n\n9.1.5.1 The t distribution’s relationship Relationship to the normal distribution.\nSuppose we have two independent standard normal variates Z_0 \\sim \\mathcal{N}(0, 1^2) and Z_1 \\sim \\mathcal{N}(0, 1^2).\nA t distribution with one degree of freedom is created like so:\n\nT_1 = z_0\\sqrt{\\frac{1}{z_1^2}}\n\nA t distribution with two degrees of freedom is created like so:\n\nT_2 = z_0\\sqrt{\\frac{2}{z_1^2 + z_2^2}}\n\nWhere z_0, z_1 and z_2 are independent standard normal variates.\nA t distribution with \\nu degrees of freedom is created like so:\n\nT_v = z_0\\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}\n\nThe sum of \\nu squared standard normal variates \\left(\\sum_{i=1}^\\nu z_i^2\\right) has a \\chi^2 distribution with \\nu degrees of freedom, which has a mean of \\nu. Therefore, \\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}}, on average, equals one. However, the expression \\sqrt{\\frac{\\nu}{\\sum_{i=1}^\\nu z_i^2}} has a variability approaches 0 as \\nu increases. When \\nu is high, z_0 is being multiplied by a value very close to 1. Thus, T_\\nu is nearly normal at high levels of nu.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "distributions.html#additional-distributions",
    "href": "distributions.html#additional-distributions",
    "title": "\n8  Distributions\n",
    "section": "\n9.2 Additional Distributions",
    "text": "9.2 Additional Distributions\n\n9.2.1 F Distributions\nSuppose that X is the ratio of two independent \\chi^2 variates U_1 and U_2 scaled by their degrees of freedom \\nu_1 and \\nu_2, respectively:\nX=\\frac{\\frac{U_1}{\\nu_1}}{\\frac{U_2}{\\nu_2}}\nThe random variate X will have an F distribution with parameters, \\nu_1 and \\nu_2.\nThe primary application of the F distribution is to test the equality of variances in ANOVA. I am unaware of any direct applications of the F distribution in psychological assessment.\n\n9.2.2 Weibull Distributions\nHow long do we have to wait before an event occurs? With Weibull distributions, we model wait times in which the probability of the event changes depending on how long we have waited. Some machines are designed to last a long time, but defects in a part might cause it fail quickly. If the machine is going to fail, it is likely to fail early. If the machine works flawlessly in the early period, we worry about it less. Of course, all physical objects wear out eventually, but a good design and regular maintenance might allow a machine to operate for decades. The longer machine has been working well, the less risk that it will irreparably fail on any particular day.\nFor some things, the risk of failure on any particular day becomes increasingly likely the longer it has been used. Biological aging causes increasing risk of death over time such that the historical records have no instances of anyone living beyond\nFor some events, there is a constant probability that the event will occur. For others, the probability is higher at first but becomes steadily less likely over time\nthe longer we wait the greater the probability will occur. For example, as animals age the probability of death accelerates such that beyond a certain age no individual as been observed to survive.\n\n9.2.3 Unfinished\n\nGumbel Distributions\nBeta Distributions\nExponential Distributions\nPareto Distributions",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "descriptives.html",
    "href": "descriptives.html",
    "title": "\n9  Descriptives\n",
    "section": "",
    "text": "9.1 Frequency Distribution Tables\nX\nFrequency\nCumulativeFrequency\nProportion\nCumulativeProportion\n\n\n\n3\n1\n1\n.125\n.125\n\n\n4\n3\n4\n.375\n.500\n\n\n6\n2\n6\n.250\n.750\n\n\n10\n2\n8\n.250\n1\n\n\n\n\n\nTable 9.1: Frequency Distribution TableThe median is 5, halfway between the two middle scores of 4 and 6.\nR Code for Table 9.1\n\n\n\n\n\nX &lt;- c(3,4,6,10)\nreps &lt;- c(1,3,2,2)\nd &lt;- map2(X, reps, rep) %&gt;% unlist %&gt;% \n  tibble(X = .) %&gt;% \n  group_by(X) %&gt;% \n  summarise(f = n()) %&gt;% \n  mutate(cf = cumsum(f),\n         p = (f / sum(f)) , \n         cp = cumsum(p) %&gt;% \n           prob_label(accuracy = .001)) %&gt;% \n  mutate(p = prob_label(p, accuracy = .001))\n\nsixes &lt;- reps[X == 6]\nsample_size = sum(reps)\n\nknitr::kable(\n  d,\n  col.names = c(\n    \"*X*\",\n    \"Frequency\",\n    \"Cumulative&lt;br&gt;Frequency\" ,\n    \"Proportion\",\n    \"Cumulative&lt;br&gt;Proportion\"\n  ),\n  align = \"rrrrr\",\n  escape = F\n)\nIt is common to include alongside the frequencies of each value the proportion (or percentage) of times a value occurs. If the frequency of sample space element i is f_i, and the total sample size is n, then the proportion of sample space element i is\np_i = \\frac{f_i}{n}\nIn Table 9.1, the frequency of sixes is f=2 and there are n = 8 numbers in the distribution, thus the proportion of sixes is p = \\frac{2}{8} = .25.\ncf_i= \\sum_{j=1}^{i}{f_j}\nOrdinal, interval, and ratio variables can have cumulative frequencies, but not nominal variables. To calculate cumulative frequencies, the sample space needs to be sorted in a meaningful way, which is not possible with true nominal variables. That is, there are no scores “below” any other scores in nominal variables.\nThe cumulative proportion (cp) is the proportion of scores less than or equal to a particular sample space element.\ncp_i = \\frac{cf_i}{n}",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#frequency-distribution-tables",
    "href": "descriptives.html#frequency-distribution-tables",
    "title": "\n9  Descriptives\n",
    "section": "",
    "text": "A simple way to describe a distribution is to list how many times each value in the distribution occurs. For example, in this distribution: \\{10, 3, 4, 10, 6, 4, 6, 4\\}, there is 1 three, 3 fours, 2 sixes, and 2 tens. The value that occurs most often is four. A frequency distribution table displays the number of times each value occurs, as in Table 9.1.A frequency distribution table summarises a sample by showing the frequency counts of each member of the sample space.\n\n\n\n\n\nIt is also common to supplement frequency distribution tables with additional information such as the cumulative frequency. For each sample space element, the cumulative frequency (cf) is the sum of the frequencies (f) of the current and all previous sample space elements.The cumulative frequency tells us the number of scores in a distribution that are equal to or lower than a particular sample space element.\n\n\n\n\n\n9.1.1 Frequency Distribution Tables in R\nLet’s start with a data set from Garcia et al. (2010), which can accessed via the psych package.\n\nGarcia, D. M., Schmitt, M. T., Branscombe, N. R., & Ellemers, N. (2010). Women’s reactions to ingroup members who protest discriminatory treatment: The importance of beliefs about inequality and response appropriateness. European Journal of Social Psychology, 40(5), 733–745.\n\n# Get the Garcia data set from the psych package\nd &lt;- psych::Garcia\n\nThe sjmisc package (Lüdecke, 2021) provides a quick and easy way to create a frequency distribution table with the frq function.\n\nLüdecke, D. (2021). Sjmisc: Data and variable transformation functions. https://strengejacke.github.io/sjmisc/\n\nsjmisc::frq(d$anger)\n\nx &lt;numeric&gt; \n# total N=129 valid N=129 mean=2.12 sd=1.66\n\nValue |  N | Raw % | Valid % | Cum. %\n-------------------------------------\n    1 | 73 | 56.59 |   56.59 |  56.59\n    2 | 24 | 18.60 |   18.60 |  75.19\n    3 |  4 |  3.10 |    3.10 |  78.29\n    4 |  8 |  6.20 |    6.20 |  84.50\n    5 | 12 |  9.30 |    9.30 |  93.80\n    6 |  7 |  5.43 |    5.43 |  99.22\n    7 |  1 |  0.78 |    0.78 | 100.00\n &lt;NA&gt; |  0 |  0.00 |    &lt;NA&gt; |   &lt;NA&gt;\n\n\nTypically we use frequency distribution tables to check whether the values of a variable are correct and that the distribution makes sense to us. Thus the frq function is all we need most of the time. However, if you need a publication-ready frequency distribution table, you will probably have to make it from scratch (See Table 9.2).\n\n\n\n\n\n\nX\nf\ncf\np\ncp\n\n\n\n\n1\n73\n73\n.57\n.57\n\n\n\n2\n24\n97\n.19\n.75\n\n\n\n3\n4\n101\n.03\n.78\n\n\n\n4\n8\n109\n.06\n.84\n\n\n\n5\n12\n121\n.09\n.94\n\n\n\n6\n7\n128\n.05\n.99\n\n\n\n7\n1\n129\n.01\n1.00\n\n\n\n\n\n\n\nTable 9.2: Frequency Distribution Table for Angerf = Frequency,cf = Cumulative Frequency, p = Proportion, and cp = Cumulative Proportion\n\n\n\n\n\n\n\n\n\n\nR Code for Table 9.2\n\n\n\n\n\n# Publication-quality frequency table\nd %&gt;% \n  rename(X = anger) %&gt;% \n  count(X, name = \"f\") %&gt;% \n  mutate(cf = cumsum(f),\n         p = f / sum(f),\n         cp = cumsum(p)) %&gt;% \n  mutate(across(\n    .cols = p:cp,\n    .fns = function(x) scales::number(x, .01) %&gt;% \n      str_remove(\"^0\"))) %&gt;% \n  rename_with(\n    .fn = function(x) paste0(\"*\",x,\"*\")) %&gt;% \n  mutate(` ` = \"\") %&gt;% \n  kable(\n    digits = 2,\n    align = \"r\"\n  ) %&gt;%\n  html_table_width(c(30, rep(100, 4), 20))\n\n\n\n\n\n9.1.2 Frequency Distribution Bar Plots\n\n\n\n\n\n\n\n\nFigure 9.1: Frequency Distribution Bar Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: Cumulative Frequency Stacked Bar Plot\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3: Cumulative Frequency Step Plot\n\n\n\nIn Figure 9.1, the frequency distribution from Table 9.2 is translated into a standard bar plot in which each bar is proportional to the frequency of each response. A column bar plot allows for easy comparison of the frequency of each category. For example, in Figure 9.1, the most frequent response to the Anger question—1 (low)—draws your attention immediately. In contrast to the mental effort needed to scan frequencies listed in a table, the relative height of each frequency in the bar plot is perceived, compared, and interpreted almost effortlessly. With a single glance at Figure 9.1, no calculation is required to know that none of the other responses is even half as frequent as 1.\n\n\n\n\n\n\n\nR Code for Figure 9.1\n\n\n\n\n\n# Make frequency data\nd_freq &lt;- d %&gt;% \n  rename(Anger = anger) %&gt;% \n  count(Anger, name = \"f\") %&gt;% \n  mutate(cf = cumsum(f),\n         p = f / sum(f),\n         cp = cumsum(p))\n\n# Frequency Bar Plot\nd_freq %&gt;% \n  ggplot(aes(Anger, f)) + \n  geom_col(fill = myfills[1]) + \n  geom_text_fill(aes(label = f), \n                 vjust = -0.5, \n                 size = ggtext_size(30)) +\n  scale_x_continuous(breaks = 1:7, \n                     minor_breaks = NULL) + \n  scale_y_continuous(\"Frequency\", \n                     expand = expansion(c(0, 0.09))) +\n  theme_minimal(base_size = 30, \n                base_family = bfont) + \n  theme(panel.grid.major.x = element_blank())       \n\n\n\n\n\n9.1.3 Frequency Distribution Stacked Bar Plots\nIn a standard bar plot, one may easily compare frequencies to each other, but that may not be what you wish the reader to notice first. A stacked bar plot emphasizes the proportions of each category as it relates to the whole. It also allows for the visual display of the cumulative frequencies and proportions. For example, in Figure 9.2, it is easy to see that more than half of participants have an anger level of 1, and three quarters have an anger level of 2 or less.\n\n\n\n\n\n\n\nR Code for Figure 9.2\n\n\n\n\n\n# Stacked Frequency Bar Plot\n\nd_freq %&gt;% \n  ggplot(aes(factor(\"Anger\"), cf - f / 2)) + \n  geom_tile(aes(height = f, fill = factor(Anger)), \n            width = 1.2) +\n  geom_text(aes(label = paste0(\n    Anger, \n    \" (\", \n    f, \n    \", \",\n    scales::percent(p, accuracy = 1),\n    \")\"))) + \n  scale_y_continuous(\n    \"Cumulative Frequency\",\n    breaks = c(0, d_freq$cf),\n    minor_breaks = NULL,\n    expand = expansion(c(0, .04)),\n    sec.axis = sec_axis(\n      trans = ~ .x,\n      labels = scales::percent(c(0, d_freq$cp), \n                               accuracy = 1),\n      breaks = c(0, d_freq$cf),\n      name = \"Cumulative Percentage\"))  +\n  scale_fill_manual(values = tinter::tinter(myfills[1], 9)) +\n  scale_x_discrete(NULL) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.text.y = element_text(vjust = c(rep(0.5, 7),-0.3))\n  ) \n\n\n\n\n\n9.1.4 Frequency Distribution Step Line Plots\nA step line plot can show the cumulative frequency’s relationship with the variable. For example, in Figure 9.3, it appears that the cumulative frequency rises quickly at first but then rises slowly and steadily thereafter.\n\n\n\n\n\n\n\nR Code for Figure 9.3\n\n\n\n\n\n# Frequency Step Plot\nd_freq %&gt;%\n  ggplot(aes(Anger, cf)) +\n  geom_step(direction = \"mid\",\n            color = myfills[1],\n            linewidth = 0.5) +\n  geom_text_fill(\n    aes(label = cf),\n    vjust = -0.5,\n    color = myfills[1],\n    size = ggtext_size(30)\n  ) +\n  geom_text_fill(\n    aes(label = signs::signs(\n      f, accuracy = 1, add_plusses = T\n    )),\n    vjust = 1.5,\n    color = \"gray40\",\n    size = ggtext_size(30)\n  ) +\n  scale_x_continuous(breaks = 1:7,\n                     expand = expansion()) +\n  scale_y_continuous(\n    \"Cumulative Frequency\",\n    limits = c(0, NA),\n    breaks = 0,\n    minor_breaks = NULL,\n    expand = expansion(mult = c(0.001, 0.07))\n  ) +\n  theme_minimal(base_size = 30, base_family = bfont) +\n  theme(panel.grid.major.x = element_blank()) +\n  annotate(\n    geom = \"segment\",\n    x = 0.5,\n    y = 0,\n    xend = 0.5,\n    yend = 73,\n    color = myfills[1],\n    linewidth = 1\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 0.5,\n    y = 73,\n    xend = 1,\n    yend = 73,\n    color = myfills[1],\n    linewidth = 0.5\n  ) +\n  annotate(\n    geom = \"segment\",\n    x = 7,\n    y = 129,\n    xend = 7.5,\n    yend = 129,\n    color = myfills[1],\n    linewidth = 0.5\n  )",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#measures-of-central-tendency",
    "href": "descriptives.html#measures-of-central-tendency",
    "title": "\n9  Descriptives\n",
    "section": "\n9.2 Measures of Central Tendency",
    "text": "9.2 Measures of Central Tendency\nIf we need to summarize an entire distribution with a single number with the least possible information loss, we use a measure of central tendency—usually the mode, the median, or the mean. Although these numbers are intended to represent the entire distribution, they often require accompaniment from other statistics to perform this role with sufficient fidelity.\nOur choice of which measure of central tendency we use to summarize a distribution depends on which type of variable we are summarizing (i.e., nominal, ordinal, interval, or ratio) and also a consideration of each central tendency measure’s strengths and weaknesses in particular situations.\n\n9.2.1 Mode\nThe mode is the most frequent score in a distribution. Suppose we have a distribution that looks like this:The mode is the value in a distribution that occurs most often.\n\\{1,2,2,2,2,3,3\\}\nBecause 2 occurs more frequently than the other values in the distribution, the mode is 2.\n\n\n\nIn a frequency distribution table, the mode is the value with the highest value in the f (frequency) column. In Table 9.2, the mode is 1 because it has the highest frequency (f = 73).\nIn a bar plot, histogram, or probability density plot, the mode is the value that corresponds to the highest point in the plot. For example, in Figure 9.1, the modal value is 1 because its frequency of 73 is the highest point in the bar plot. In Figure 9.4, the mode is −1 because that is the highest point in the density plot.\nIf two values tie, both values are the mode and the distribution is bimodal. Sometimes a distribution has two distinct clusters, each with its own local mode. The greater of these two modes is the major mode, and the lesser is the minor mode (See Figure 9.4).A bimodal distribution has two modes.\n\n\n\n\n\n\n\n\nFigure 9.4: A Bimodal Distribution\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.4\n\n\n\n\n\n# A bimodal distribution\ntibble(x = seq(-3, 3.5, .01),\n       y = dnorm(x,-1, 0.5) / 0.8 + \n         dnorm(x, 1, 0.75)) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_area(fill = myfills[1]) +\n  geom_text(\n    data = . %&gt;% dplyr::filter(x == -1),\n    vjust = -0.25,\n    label = \"Major\\nMode\",\n    size = 8,\n    lineheight = 0.9\n  ) +\n  geom_text(\n    data = . %&gt;% dplyr::filter(x == 1),\n    vjust = -0.25,\n    label = \"Minor\\nMode\",\n    size = 8,\n    lineheight = 0.9\n  ) +\n  scale_x_continuous(NULL,\n                     minor_breaks = NULL,\n                     breaks = seq(-3, 3)) +\n  scale_y_continuous(NULL,\n                     breaks = NULL,\n                     expand = expansion(c(0, .25))) +\n  theme_minimal(base_size = 30,\n                base_family = bfont) +\n  theme(panel.grid.major.x = element_blank()) \n\n\n\n\nThe mode is the only measure of central tendency that be computed for all variable types and is the only choice for nominal variables (See Table 9.4).\nTo compute the mode of a variable, use the mfv (most frequent value) function from the modeest package (Poncet, 2019). In this example, the 2 occurs four times.\n\nPoncet, P. (2019). Modeest: Mode estimation. https://github.com/paulponcet/modeest\n\n# Data\nx &lt;- c(1,2,2,2,2,3,3)\n# Mode\nmodeest::mfv(x)\n\n[1] 2\n\n\nThe mfv function will return all modes if there is more than one. In this example, the 1, 3, and 4 all occur twice.\n\n# Data\nx &lt;- c(1,1,3,3,4,4,5,6)\n# Mode\nmodeest::mfv(x)\n\n[1] 1 3 4\n\n\n\n9.2.2 Median\nThe median is midpoint of a distribution, the point that divides the lower half of the distribution from the upper half. To calculate the median, you first need to sort the scores. If there is an odd number of scores, the median is the middle score. If there an even number of scores, it is the mean of the two middle scores. There are other definitions of the median that are a little more complex, but rarely is precision needed for calculating the median.The median is the point that divides the lower 50 percent of a distribution from the upper 50 percent.\nTo find the median using a frequency distribution table, find the first sample space element with a cumulative proportion greater than 0.5. For example, in the distribution shown in Table 9.3, the first cumulative proportion greater than 0.5 occurs at 5, which is therefore the median.\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\nFrequency\nCumulative Frequency\nProportion\nCumulative Proportion\n\n\n\n1\n1\n1\n.14\n.14\n\n\n5\n3\n4\n.43\n.57\n\n\n7\n1\n5\n.14\n.71\n\n\n9\n2\n7\n.29\n1.00\n\n\n\n\n\n\nTable 9.3: Finding the Median in a Frequency Distribution Table.In this case, the median is 5 because it has the first cumulative proportion that is greater than 0.5.\n\n\n\nIf a sample space element’s cumulative proportion is exactly 0.5, average that sample space element with the next highest value. For example, in the distribution in Table 9.1, the cumulative proportion for 4 is exactly 0.5 and the next value is 6. Thus the median is\n\\frac{4+6}{2}=5\nThe median can be computed for ordinal, interval, and ratio variables, but not for nominal variables (See Table 9.4). Because nominal variables have no order, no value is “between” any other value. Thus, because the median is the middle score and nominal variables have no middle, nominal variables cannot have a median.\nFor ordinal variables, the median is the preferred measure of central tendency because it is usually more stable from one sample to the next compared to the mode.\nIn R, the median function can compute the median:\n\nmedian(c(1,2,3))\n\n[1] 2\n\n\n\n9.2.3 Mean\nThe arithmetic mean is the sum of all values of a distribution divided by the size of the distribution.The arithmetic mean is the balance point of a disribution.\n\\mu_X = \\frac{\\sum_{i = 1}^n {X_i}}{n}\nWhere \\begin{align*}\n  \\mu_X &= \\text{The population mean of } X\\\\\n  n &= \\text{The number of values in } X\n\\end{align*}\nThe arithmetic mean can only be calculated with interval or ratio variables. Why? The formula for the mean requires adding numbers, and the operation of addition is not defined for ordinal and nominal values.\nThe arithmetic mean is usually the preferred measure of central tendency for interval and ration variables because it is usually more stable from sample to sample than the median and the mode. In Figure 9.5, it can be seen that the sampling distributions of the mean is narrower than that of the median and the mode. In other words, it has a smaller standard error.The standard error is the standard deviation of a sampling distribution.\n\n\n\n\n\n\n\n\nFigure 9.5: Sampling Distributions of Central Tendency Measures.The standard normal distribution, \\mathcal{N}(0,1), was used to generate 10,000 samples with a sample size of 100. The distribution of sample means is slightly narrower than the distribution of sample medians, meaning that the mean is slightly more stable than the median. The distribution of sample modes is very wide, meaning that the mode is much less stable than the mean and median.\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.5\n\n\n\n\n\n# Central tendency function\nct &lt;- function(sample_size = 100) {\n  x &lt;- rnorm(sample_size)\n  mo &lt;- DescTools::Mode(round(x,2))\n  \n  c(Mean = mean(x), \n       Median = median(x), \n       Mode = sample(mo, 1))\n}\n\n# Replicate samples\nd_ct &lt;- replicate(10000, ct(100)) %&gt;% \n  t() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(cols = everything(), \n               names_to = \"CentralTendency\",\n               values_to = \"Value\") %&gt;% \n  mutate(y = recode(CentralTendency, \n                    `Mode` = 0.03, \n                    `Median` = 0.07, \n                    `Mean` = 0.135))\n\n# Summary data\nd_sum &lt;- d_ct %&gt;% \n  filter(!is.na(Value)) %&gt;% \n  group_by(CentralTendency) %&gt;% \n  summarise(x = mean(Value),\n            y = mean(y),\n            ub = quantile(Value, 0.975),\n            lb = quantile(Value, 0.025)) %&gt;% \n  rename(Value = x)\n\n# Plot\nd_ct %&gt;% \n  filter(!is.na(Value)) %&gt;% \n  ggplot(aes(Value, y)) +\n  stat_function(geom = \"area\", \n                fun = function(x) dnorm(x, 0, 1), \n                n = 1000, \n                color = NA, \n                fill = \"gray50\", \n                alpha = 0.2)  +\n  ggdist::stat_halfeye(aes(fill = CentralTendency), \n                       scale = 1.2, \n                       color = \"gray20\") +\n  geom_text(data = d_sum,\n            aes(label = CentralTendency),\n            vjust = 1.5,\n            size = ggtext_size(22),\n            color = \"gray20\") +\n  scale_x_continuous(name = NULL, \n                     limits = c(-3.5,3.5), \n                     breaks = seq(-4,4),\n                     labels = \\(x) signs::signs(x, accuracy = 1)) +\n  theme_minimal(base_size = 22, \n                base_family = bfont) +\n  theme(legend.position = \"none\", \n        panel.grid = element_blank()) +\n  scale_y_continuous(NULL, breaks = NULL, \n                     expand = expansion()) + \n  scale_fill_manual(values = scales::muted(\n    rep(myfills[1], 3),\n    l = c(65, 55, 40)))\n\n\n\n\nIn R, the mean function can compute the median:\n\nmean(c(1,2,3))\n\n[1] 2\n\n\nWatch out for missing values in R. If the distribution has even a single missning value, the mean function will return NA, as will most other summary functions in R (e.g., median, sd, var, and cor).\n\nmean(c(1,NA,3))\n\n[1] NA\n\n\nTo calculate the mean of all non-missing values, specify that all missing values shoule be removed prior to calculation like so:\n\nmean(c(1,NA,3), na.rm = TRUE)\n\n[1] 2\n\n\n\n9.2.4 Comparing Central Tendency Measures\nWhich measure of central tendency is best depends on what kind of variable is needed and also what purpuse it serves. Table 9.4 has a list of comparative features of each of the three major central tendency measures.\n\n\n\n\n\n\nFeature\nMode\nMedian\nMean\n\n\n\nComputable for Nominal Variables\nYes\nNo\nNo\n\n\nComputable for Ordinal Variables\nYes\nYes\nNo\n\n\nComputable for Interval Variables\nYes\nYes\nYes\n\n\nComputable for Ratio Variables\nYes\nYes\nYes\n\n\nAlgebraic Formula\nNo\nNo\nYes\n\n\nUnique Value\nNo\nYes\nYes\n\n\nSensitive to Outliers/Skewness\nNo\nNo\nYes\n\n\nStandard Error\nLarger\nSmaller\nSmallest\n\n\n\n\n\n\nTable 9.4: Comparing Central Tendency Measures",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#expected-values",
    "href": "descriptives.html#expected-values",
    "title": "\n9  Descriptives\n",
    "section": "\n9.3 Expected Values",
    "text": "9.3 Expected Values\n(Unfinished)\nAt one level, the concept of the expected value of a random variable is really simple; it is just the population mean of the variable. So why don’t we just talk about population means and be done with this “expected value” business? It just complicates things! True. In this case, however, there is value in letting some simple things appear to become complicated for a while so that later we can show that some apparently complicated things are actually simple.The expected value of a random variable is the population mean of the values that the random variable generates.\nWhy can’t we just say that the expected value of a random variable is the population mean? You are familiar, of course, with the formula for a mean. You just add up the numbers and divide by the number of numbers n:\n\nm_X=\\frac{\\sum_{i=1}^{n} {x_i}}{n}\n\nFine. Easy. Except…hmm…random variables generate an infinite number of numbers. Dividing by infinity is tricky. We’ll have to approach this from a different angle…\nThe expected value of a random variable is a weighted mean. A mean of what? Everything in the sample space. How are the sample space elements weighted? Each element in the sample space is multiplied by its probability of occurring.\n\n\n\n\n\n\n\n\nFigure 9.6: Probability Distribution of a Hypothetical Random Variable\n\n\n\nSuppose that the sample space of a random variable X is {2, 4, 8} with respective probabilities of {0.3, 0.2, 0.5}, as shown in Figure 9.6.\n\n\n\n\n\n\n\nR Code for Figure 9.6\n\n\n\n\n\ntibble(x = factor(c(2,4,8), levels = 1:8),\n       p = c(0.3, 0.2, 0.5)) %&gt;% \n  ggplot(aes(x,p)) + \n  geom_col(fill = myfills[1]) + \n  geom_text(aes(label = prob_label(p)), \n            vjust = -0.4, \n            family = bfont, \n            size = ggtext_size(18)) + \n  theme_minimal(base_family = bfont, \n                base_size = 18) + \n  scale_y_continuous(\"Probability\", \n                     expand = expansion(mult = c(.01, .10)),\n                     breaks = seq(0,1,.1),\n                     labels = prob_label\n                     ) + \n  scale_x_discrete(\"Sample Space\", drop = F ) + \n  theme(panel.grid.major.x = element_blank())\n\n\n\n\nThe notation for taking the expected value of a random variable X is \\mathcal{E}(X). Can we find the mean of this variable X even if we do not have any samples it generates? Yes. To calculate the expected value of X, multiply each sample space element by its associated probability and then take the sum of all resulting products. Thus,\n\n\\begin{align*}\n\\mathcal{E}(X)&=\\sum_{i=1}^{3}{p_i x_i}\\\\\n&= p_1x_1+p_2x_2+p_3x_3\\\\\n&= (.3\\times 2)+(.2\\times 4)+(.5\\times 8)\\\\\n&=5.4\n\\end{align*}\n\nThe term expected value might be a little confusing. In this case, 5.4 is the expected value of X but X never once generates a value of 5.4. So the expected value is not “expected” in the sense that we expect to see it often. It is expected to be close to the mean of any randomly selected sample of the variable that is sufficiently large:\n\n\n\n\n\n\n\n\nFigure 9.7: Slicing the Standard Normal Distribution into Ever Thinner Bins\n\n\n\n\n\\mathcal{E}(X)=\\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^{n} {x_i}\n\nIf a random variable X is discrete, its expected value \\mathcal{E}(X) is the sum of each member of the sample space x_i multiplied by its probability of occurring p_i. The probability of occurring is the output of X’s probability density function at that location: p_i=f_X(x_i). Thus,\n\n\\mathcal{E}(X)=\\sum_{i=-\\infty}^{\\infty}{x_i f_X(x_i)}\n\nWith continuous variables, the number of elements in a sample is infinite. Fortunately, calculus was designed to deal with this kind of infinity. The trick is to imagine that the continuous variable is sliced into bins and that the bins are sliced ever more thinly. If a continuous random variable has probability density function f_X(x), the expected value is\n\n\\mathcal{E}(X)=\\int_{-\\infty}^{\\infty} {x f_X(x)\\,\\mathrm{d}x}\n\nIf we multiply each value of X by the height of its bin (p), we get the mean of the binned distribution. If the bins become ever thinner, as in Figure 9.7, the product of X and p approximates the expected value of the smooth continuous distribution.\n\n\n\n\n\n\n\nR Code for Figure 9.7\n\n\n\n\n\n# Slicing the standard normal distribution into ever thinner bins\nmake_bins &lt;- function(binPower,\n                      binWidth,\n                      LowerBound,\n                      UpperBound) {\n  tibble(x = seq(LowerBound, UpperBound, binWidth), binPower, binWidth)\n}\n\npmap_df(tibble(\n  binPower = 0:4,\n  binWidth = 2 ^ (-1 * binPower),\n  LowerBound = -4,\n  UpperBound = 4\n),\nmake_bins) %&gt;%\n  mutate(\n    p = pnorm(x + binWidth / 2) - pnorm(x - binWidth / 2),\n    width_label = factor(\n      2 ^ binPower,\n      levels = 2 ^ (0:4),\n      labels = c(\"Width = 1\",\n                 paste0(\"Width = 1/\",\n                        2 ^ (1:4)))\n    )\n  ) %&gt;%\n  ggplot(aes(x, p)) +\n  geom_col(\n    aes(width = binWidth),\n    fill = myfills[1],\n    color = \"white\",\n    lwd = 0.1\n  ) +\n  facet_grid(width_label ~ .,\n             scales = \"free\") +\n  theme_light(base_size = 24,\n              base_family = bfont) +\n  scale_x_continuous(\n    NULL,\n    breaks = -4:4,\n    labels = function(x)\n      signs::signs(x, accuracy = 1),\n    expand = c(0.01, 0)\n  ) +\n  scale_y_continuous(NULL,\n                     breaks = NULL) +\n  theme(\n    panel.grid = element_blank(),\n    # strip.text.y = element_blank(),\n    strip.placement = \"outside\",\n    strip.text.y = element_text(angle = 0),\n    axis.text.x = element_text(hjust = c(rep(.75, 4), rep(0.5, 5)))\n  )",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#measures-of-variability",
    "href": "descriptives.html#measures-of-variability",
    "title": "\n9  Descriptives\n",
    "section": "\n9.4 Measures of Variability",
    "text": "9.4 Measures of Variability\n\n9.4.1 Variability of Nominal Variables\nFor most purposes, the visual inspection of a frequency distribution table or bar plot is all that is needed to understand a nominal variable’s variability. I have never needed a statistic that measures the variability of a nominal variable, but if you need one, there are many from which to choose. For example, Wilcox (1973) presented this analog to variance for nominal variables:\n\nWilcox, A. R. (1973). Indices of qualitative variation and political measurement. The Western Political Quarterly, 26(2), 325. https://doi.org/10.2307/446831\n\n\\text{VA} = 1-\\frac{1}{n^2}\\frac{k}{k-1}\\sum_{i=1}^k\\left(f_i-\\frac{n}{k}\\right)^2\n\nThe qualvar package (Gombin, 2018) can compute the primary indices of qualitative variation presented by Wilcox.\n\nGombin, J. (2018). Qualvar: Implements indices of qualitative variation proposed by wilcox (1973). http://joelgombin.github.io/qualvar/\n\n\n\n\n\n\n\n\nFigure 9.8: The Variance Analog (VA) index of qualitative variation ranges from 0 to 1. It equals 0 when every data point is assigned to the same category and 1 when each category has the same frequency.\n\n\n\n\nlibrary(qualvar)\n\n# Frequencies\nfrequencies =  c(A = 60, B = 10, C = 25, D = 5)\n\n# VA\nVA(frequencies)\n\n[1] 0.7533333\n\n\nIn all of these indices of qualitative variation, the lowest value is 0 when every data point belongs to the same category (See Figure 9.8, left panel). Also, the maximum value is 1 when the data points are equally distributed across categories (See Figure 9.8, right panel).\n\n\n\n\n\n\n\nR Code for Figure 9.8\n\n\n\n\n\n# The Variance Analog (VA) index of qualitative variation\nlow_var &lt;- c(A = 100, B = 0, C = 0, D = 0)\nmid_var =  c(A = 60, B = 10, C = 25, D = 5)\nhigh_var = c(A = 25, B = 25, C = 25, D = 25)\n\n\ntibble(\n  Variability = c(\"Low\", \"Middle\", \"High\"),\n  Frequency = list(low_var, mid_var, high_var),\n  VA = map_dbl(Frequency, VA)\n) %&gt;%\n  mutate(\n    Frequency = map(Frequency, function(d)\n      as.data.frame(d) %&gt;%\n        tibble::rownames_to_column(\"Category\")),\n    Variability = paste0(Variability,\n                         \"\\nVA = \",\n                         prob_label(VA)) %&gt;%\n      fct_inorder()\n  ) %&gt;%\n  unnest(Frequency) %&gt;%\n  rename(Frequencies = d) %&gt;%\n  ggplot(aes(Category, Frequencies)) +\n  geom_col(aes(fill = Variability)) +\n  geom_text_fill(\n    aes(label = Frequencies),\n    vjust = -.3,\n    color = \"gray30\",\n    size = ggtext_size(30)\n  ) +\n  scale_y_continuous(\n    expand = expansion(mult = c(0, 0.08)),\n    breaks = seq(0, 100, 20),\n    minor_breaks = seq(0, 100, 10)\n  ) +\n  scale_fill_manual(values = myfills) +\n  facet_grid(cols = vars(Variability)) +\n  theme_light(base_family = bfont, base_size = 30) +\n  theme(panel.grid.major.x = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n9.4.2 Interquartile Range\nAs with nominal variables, a bar plot or frequency distribution table can tell you most of what you want to know about the variability of an ordinal variable. If you need a quantitative measure of how much an ordinal variable varies, you have many options. The most important of these is the interquartile range.The interquartile range (IQR) is the distance from the score at the 25th percentile to the score at the 75th percentile.\n\n\n\n\n\n\n\n\nFigure 9.9: In a normal distribution with a mean of 100 and a standard deviation of 15, the interquartile range is about 20.2, the distance between 89.9 and 110.1.\n\n\n\nWhen median is a good choice for our central tendency measure, the interquartile range is usually a good choice for our variability measure. Whereas the median is the category that contains the 50th percentile in a distribution, the interquartile range is the distance between the categories that contain the 25th and 75th percentile. That is, it is the range of the 50 percent of data in the middle of the distribution. For example, in Figure 9.9, the shaded region is the space between the 25th and 75th percentiles in a normal distribution. The IQR is the width of the shaded region, about 20.2.\n\n\n\n\n\n\n\nR Code for Figure 9.9\n\n\n\n\n\nIQR_bounds &lt;- qnorm(c(.25, .75), mean = 100, sd = 15)\nl_height = .05\nggplot(data = tibble(x = c(40, 160), y = pnorm(x, 100, 15)), aes(x)) +\n  stat_function(fun = \\(x) dnorm(x, 100, 15),\n                geom = \"area\",\n                alpha = 0.1) +\n  stat_function(\n    fun = \\(x) dnorm(x, 100, 15),\n    geom = \"area\",\n    xlim = qnorm(c(.25, .75), mean = 100, sd = 15),\n    fill = myfills[1],\n    alpha = 0.5\n  ) +\n  scale_y_continuous(NULL, breaks = NULL, expand = expansion()) +\n  scale_x_continuous(NULL, breaks = seq(40, 160, 15)) +\n  theme_minimal(base_size = 28, base_family = bfont) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.x = element_blank(),\n    axis.ticks.x = element_line(color = \"gray30\"), plot.margin = margin()\n  ) +\n  annotate(\n    x = IQR_bounds,\n    y = 0,\n    size = ggtext_size(28),\n    geom = \"richtext\", label.color = NA, fill = NA,\n    label = paste0(\"**\", round(IQR_bounds, 1),\"**&lt;br&gt;&lt;span style='font-size:22pt'&gt;\", c(25, 75), \"&lt;sup&gt;th&lt;/sup&gt;&lt;br&gt;percentile\"),\n    hjust = c(1, 0),\n    lineheight = .9,\n    vjust = 0\n  ) + \n  geom_arrow_segment(\n    data = tibble(\n      x = IQR_bounds[1], \n      y = 0, \n      xend = IQR_bounds[2], \n      yend = 0),\n    aes(x = x, y = y, xend = xend, yend = yend),\n    arrow_head = arrow_head_deltoid(),\n    arrow_fins = arrow_head_deltoid()\n    ) +\n  annotate(x = 100, \n           y = 0,\n           size = ggtext_size(28),\n           label = paste0(\"*IQR*&lt;br&gt;=\", round(IQR_bounds[2] - IQR_bounds[1], 1)),\n           geom = \"richtext\", fill = NA, label.color = NA,\n           vjust = 0) + \n  coord_cartesian(clip = \"off\")\n\n\n\n\nIn ordinal data, there is no distance between categories, thus we cannot report the interquartile range per se. However, we can report the categories that contain the 25th and 75th percentiles. In Figure 9.10, the interquartile range has its lower bound at Disagree and its upper bound at Slightly Agree.”\n\n\n\n\n\n\n\n\nFigure 9.10: In this ordinal variable, the interquartile range has a lower bound at Disagree (which contains the 25th percentile) and an upper bound at Slightly Agree (which contains the 75th percentile).\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.10\n\n\n\n\n\nd &lt;- tibble(\n  Agreement = c(\n    \"Strongly Disagree\",\n    \"Disagree\",\n    \"Slightly Disagree\",\n    \"Slightly Agree\",\n    \"Agree\",\n    \"Strongly Agree\"\n  ),\n  n = c(23, 85, 93, 121, 20, 26),\n  p = n / sum(n),\n  cp = cumsum(p),\n  ymin = lag(cp, default = 0),\n  ytext = ymin + p / 2\n)\n\nd %&gt;%\n  mutate(Agreement = fct_inorder(Agreement) %&gt;% fct_rev()) %&gt;%\n  ggplot(aes(p, cp)) +\n  geom_rect(aes(\n    ymin = ymin,\n    ymax = cp,\n    xmin = 0,\n    xmax = 1,\n    fill = Agreement\n  )) +\n  geom_label(\n    aes(x = 1, label = paste0(round(cp * 100), \"%\")),\n    hjust = 0,\n    label.size = 0,\n    color = \"gray30\"\n  ) +\n  geom_text(\n    aes(\n      x = 0.5,\n      y = ytext,\n      label = paste0(Agreement,\n                     \" (\",\n                     round(100 * p), \"%)\")\n    ),\n    size = WJSmisc::ggtext_size(18),\n    color = \"gray10\"\n  ) +\n  scale_y_continuous(\n    \"Cumulative Proportion\",\n    minor_breaks = NULL,\n    labels = \\(x) paste0(round(x * 100), \"%\"),\n    expand = expansion(),\n    limits = c(0, 1)\n  ) +\n  scale_x_continuous(\"Agreement\", breaks = NULL, expand = expansion(add = c(0, .2))) +\n  scale_fill_manual(values = rev(c(\n    rev(tinter(myfills[1], steps = 4)[1:3]),\n    tinter(myfills[2], steps = 4)[1:3]\n  ))) +\n  theme(\n    legend.position = \"none\",\n    axis.ticks.y = element_line(\"gray30\"),\n    panel.grid.major.y = element_blank()\n  ) +\n  coord_cartesian(clip = \"off\")\n\n\n\n\nThe median and the interquartile range are displayed in box and whiskers plots like Figure 9.11. The height of the box is the interquartile range, and the horizontal line is the median. The top “whisker” extends no higher than 1.5 × IQR above the 75th percentile. The bottom “whisker” extends no lower than 1.5 × IQR below the 25th percentile. Any data points outside the whiskers can be considered outliers.\n\n\n\n\n\n\n\nFigure 9.11: A Tukey-style Box and Whiskers Plot with Medians and Interquartile Ranges.\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.11\n\n\n\n\n\nset.seed(2)\nd &lt;-\n  tibble(\n    A = rnorm(100, 50, 10),\n    C = 1.5 * rchisq(100, 4) + 50,\n    B = rbeta(100, 4.5, .5) * 80\n  ) %&gt;%\n  pivot_longer(cols = everything(),\n               names_to = \"grp\",\n               values_to = \"x\") %&gt;%\n  mutate(grp = factor(grp)) %&gt;%\n  group_by(grp) %&gt;%\n  mutate(\n    md = median(x),\n    IQR = IQR(x),\n    q25 = quantile(x, .25),\n    q75 = quantile(x, .75)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(outlier = ifelse(x &gt; md,\n                          x - q75 &gt; IQR * 1.5,\n                          q25 - x &gt;  IQR * 1.5))\n\nd_stats &lt;- d %&gt;%\n  group_by(grp) %&gt;%\n  summarise(md = median(x),\n            q25 = quantile(x, .25),\n            q75 = quantile(x, 0.75)) %&gt;%\n  pivot_longer(cols = -grp,\n               names_to = \"stats\",\n               values_to = \"x\") %&gt;%\n  mutate(\n    st = case_when(\n      stats == \"md\" ~ \" (Median)\",\n      stats == \"q25\" ~ \" (1&lt;sup&gt;st&lt;/sup&gt; Quartile)\",\n      stats == \"q75\" ~ \" (3&lt;sup&gt;rd&lt;/sup&gt; Quartile)\"\n    )\n  )\n\nwidth = .3\n\nd %&gt;%\n  ggplot(aes(grp, x)) +\n  geom_boxplot(aes(fill = grp), \n               outlier.color = NA, \n               width = width * 2) +\n  geom_richtext(\n    data = d_stats,\n    aes(label = paste0(\n      scales::number(x, .1), \n      ifelse(grp == \"A\", st, \"\")\n    )),\n    nudge_x = width + .01,\n    label.color = NA,\n    hjust = 0,\n    color = \"gray20\"\n  ) +\n  geom_arrow_segment(\n    data = select(d, grp, q25, q75) %&gt;% \n      unique(),\n    aes(\n      yend = q75,\n      y = q25,\n      x = as.numeric(grp) - width - 0.05,\n      xend = as.numeric(grp) - width - 0.05\n    ),\n    arrow_head = arrow_head_deltoid(),\n    arrow_fins = arrow_head_deltoid(),\n  ) +\n  geom_richtext(\n    data = select(d, grp, q25, q75, IQR) %&gt;% \n      unique(),\n    aes(\n      x = as.numeric(grp) - width - 0.05,\n      y = (q25 + q75) / 2,\n      label = paste0(ifelse(grp == \"A\", \"*IQR* = \", \"\"), scales::number(IQR, .1))\n    ),\n    angle = 90,\n    vjust = -0.2,\n    label.color = NA,\n    label.padding = margin(t = 3)\n  ) +\n  ggbeeswarm::geom_quasirandom(pch = 16,\n                               size = 1,\n                               aes(color = outlier),\n                               width = .3) +\n  scale_x_discrete(\"Group\") +\n  scale_y_continuous(NULL) +\n  scale_fill_manual(values = myfills %&gt;% \n                      scales::alpha(.5)) +\n  scale_color_manual(values = c(\"gray20\", \"firebrick\")) +\n  theme(legend.position = \"none\") \n\n\n\n\n\n9.4.3 Variance\nA deviation is computed by subtracting a score from its mean:A deviation is the distance of a score from the score’s mean.\nX-\\mu\nWe would like to know the typical size of the deviation X-\\mu. To do so, it might seem intuitively correct to take the average (i.e., expected value) of the deviation, but this quantity is always 0:\n\n\\begin{aligned}\n\\mathcal{E}(X-\\mu)&=\\mathcal{E}(X)-\\mathcal{E}(\\mu)\\\\\n&=\\mu-\\mu\\\\\n&=0\n\\end{aligned}\n\nBecause the average deviation is always 0, it has no utility as a measure of variability. It would be reasonable to take the average absolute value of the deviations, but absolute values often cause algebraic difficulties later when we want to use them to derive other statistics. A more mathematically tractable solution is to make each deviation positive by squaring them.\nVariance \\left(\\sigma^2\\right) is the expected value of squared deviations from the mean \\left(\\mu\\right):Variance is a measure of variability that gives the size of the average squared deviation from the mean.\n\n\\begin{aligned}\n\\sigma^2&=\\mathcal{E}\\!\\left(\\left(X-\\mu\\right)^2\\right)\\\\\n&=\\mathcal{E}\\!\\left(X^2-2X\\mu+\\mu^2\\right)\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-\\mathcal{E}\\!\\left(2X\\mu\\right)+\\mathcal{E}\\!\\left(\\mu^2\\right)\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-2\\mu\\,\\mathcal{E}\\!\\left(X\\right)+\\mu^2\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-2\\mu\\mu+\\mu^2\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-2\\mu^2+\\mu^2\\\\\n&=\\mathcal{E}\\!\\left(X^2\\right)-\\mu^2\\\\\n\\end{aligned}\n\nIf all elements of a population with mean \\mu are known, the population variance is calculated like so:\n\n\\sigma^2=\\frac{\\sum_i^n{\\left(x_i-\\mu\\right)^2}}{n}\n\nNotice that the population variance’s calculation requires knowing the precise value of the population mean. Most of the time, we need to estimate the population mean \\mu using a sample mean m. A sample variance \\left(s^2\\right) for a sample size n can be calculated like so:\n\ns^2=\\frac{\\sum_i^n{\\left(x_i-m\\right)^2}}{n-1}\n\n\n\n\n\n\n\n\n\nFigure 9.12: Friedrich Wilhelm Bessel (1784–1846)Image Credits\n\n\n\nUnlike with the population variance, we do not divide by the sample size n to calculate the sample variance. If we divided by n, the sample variance would be negatively biased (i.e., it is likely to underestimate the population variance). In what is known as Bessel’s correction (i.e, dividing by n-1 instead of by n), we get an unbiased estimate of the variance.\nVariance is rarely used for descriptive purposes because it is a squared quantity with no direct connection to the width of the distribution it describes. We mainly use variance as a stepping stone to compute other descriptive statistics (e.g., standard deviations and correlation coefficients) and as an essential ingredient in inferential statistics (e.g., analysis of variance, multiple regression, and structural equation modeling). However, Figure 9.13 attempts a visualization of what variance represents. Along the X-axis, the values of a normally distributed variable X are plotted as points. The Y-axis represents the deviations of variable X from \\mu, the mean of X. For each value of X, we can create a square with sides as long as the deviations from \\mu. The red squares have a positive deviation and the blue squares have a negative deviation. The darkness of the color represents the magnitude of the deviation. The black square has an area equal to the average area of all the squares. Its sides have a length equal to the standard deviation, the square root of variance.\n\n\n\n\n\n\n\nFigure 9.13: Visualizing Variance.The values of variable X are plotted with the deviations of X. Each square is a deviation from the mean of X. Darker squares have larger deviations. The area of the thick black square is the variance—the average size of the squared deviations.\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.13\n\n\n\n\n\n# Visualizing Variance\n\nset.seed(1)\nx &lt;- rnorm(70, 10, 3)\nmu &lt;- mean(x)\nsigma &lt;- sd(x)\nxbreaks &lt;- pretty(x)\nybreaks &lt;- pretty(x - mu)\n\ntick_width &lt;- .04 * sigma\n\ntibble(x = x,\n       deviations = x - mu,\n       abval = abs(deviations)) |&gt;\n  arrange(-abval) |&gt;\n  ggplot(aes(x, deviations)) +\n  geom_rect(\n    aes(\n      xmax = x,\n      xmin = mu,\n      ymax = deviations,\n      ymin = 0,\n      fill = deviations\n    ),\n    color = \"gray95\",\n    linewidth = .1\n  ) +\n  annotate(\n    \"segment\",\n    yend = max(ybreaks) + tick_width,\n    y = min(ybreaks) - tick_width,\n    x = mu,\n    xend = mu,\n    color = \"gray30\"\n  ) +\n  annotate(\n    \"segment\",\n    xend = max(xbreaks) + tick_width,\n    x = min(xbreaks) - tick_width,\n    y = 0,\n    yend = 0,\n    color = \"gray30\"\n  ) +\n  coord_equal() +\n  annotate(\n    \"rect\",\n    xmax = mu + sigma,\n    xmin = mu,\n    ymax = sigma,\n    ymin = 0,\n    fill = NA,\n    color = \"gray10\",\n    linewidth = 1\n  ) + \n  annotate(\n    \"richtext\",\n    x = mu + sigma / 2,\n    y = sigma,\n    label = paste0(\"*\",span_style(\"&sigma;\"),\"* = \", \n                   scales::number(sigma, accuracy = .01)),\n    label.color = NA,\n    fill = NA,\n    vjust = 0,\n    family = bfont,\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) + \n  annotate(\n    \"richtext\",\n    x = mu + sigma,\n    y = sigma / 2,\n    angle = 90,\n    label = paste0(\"*\",span_style(\"&sigma;\"),\"* = \", \n                   scales::number(sigma, accuracy = .01)),\n    label.color = NA,\n    fill = NA,\n    vjust = 1.1,\n    family = bfont,\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = xbreaks,\n      deviations = 0,\n      vjust = ifelse(xbreaks &lt; mu, 0, 1.05)\n    ),\n    aes(label = x, vjust = vjust),\n    family = bfont,\n    label.color = NA,\n    label.padding = unit(c(.2, 0, .2, 0), \"lines\"),\n    fill = NA,\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) +\n  geom_richtext(\n    data = tibble(\n      deviations = ybreaks,\n      x = mu,\n      hjust = ifelse(ybreaks &lt; 0, 0, 1.05)\n    ) |&gt;\n      dplyr::filter(deviations != 0),\n    aes(\n      label = signs::signs(deviations, accuracy = 1),\n      hjust = hjust\n    ),\n    family = bfont,\n    label.color = NA,\n    fill = NA,\n    label.padding = unit(c(0, .4, 0, .4), \"lines\"),\n    size = ggtext_size(16),\n    color = \"gray20\"\n  ) + annotate(\"point\",\n               x = mu,\n               y = 0,\n               size = 3) +\n  annotate(\n    \"richtext\",\n    x = mu,\n    y = 0,\n    label = paste0(\n      \"*\",\n      span_style(\"&mu;\"),\n      \"* = \",\n      scales::number(mu, accuracy = .01),\n      \"\"\n    ),\n    label.color = NA,\n    fill = NA,\n    vjust = 0.5,\n    hjust = -.1,\n    angle = -45,\n    family = bfont,\n    size = ggtext_size(bsize),\n    color = \"gray20\"\n  ) +\n  geom_segment(\n    data = tibble(x = xbreaks),\n    aes(\n      x = xbreaks,\n      y = tick_width,\n      yend = -tick_width,\n      xend = xbreaks\n    ),\n    color = \"gray20\"\n  ) +\n  geom_segment(\n    data = tibble(y = ybreaks) |&gt; dplyr::filter(y != 0),\n    aes(\n      y = y,\n      x = mu + tick_width,\n      xend = mu - tick_width,\n      yend = y\n    ),\n    color = \"gray20\"\n  ) +\n  theme_void(base_family = bfont, base_size = bsize) +\n  theme(legend.position = \"right\",\n        legend.title = element_text(angle = 90, \n                                    margin = margin(l = -15, r = -24))) +\n  scale_fill_gradient2(\n    \"Deviations\",\n    midpoint = 0,\n    low = myfills[1],\n    high = myfills[2],\n    mid = \"white\",\n    breaks = ybreaks,\n    limits = c(-8, 8),\n    labels = \\(x) signs(x, accuracy = 1)\n  ) +\n  guides(\n    fill = guide_colorbar(\n      title = \"Deviations\",\n      label.position = \"right\",\n      title.position = \"left\",\n      title.hjust = 0.5,\n      direction = \"vertical\",\n      barheight = unit(.9, \"npc\")\n    )\n  ) +\n  annotate(\n    \"text\",\n    x = min(xbreaks) - tick_width * 4,\n    y = 0,\n    label = \"X\",\n    family = bfont,\n    color = \"gray20\",\n    fontface = \"italic\",\n    size = ggtext_size(bsize, 1)\n  ) + \n  annotate(\"richtext\",\n           label.color = NA,\n           fill = NA,\n           x = mu + sigma / 2,\n           y = sigma / 2,\n           vjust = 0.5,\n           size = ggtext_size(bsize),\n           label = paste0(\"*\", \n                          span_style(\"&sigma;\"), \n                          \"*&lt;sup&gt;2&lt;/sup&gt; = \", \n                          scales::number(sigma ^ 2, accuracy = .01)),\n           family = bfont,\n           color = \"gray20\") +\n  geom_point(pch = 16, size = 1.7, color = \"black\", y = 0, alpha = .8) \n\n\n\n\n\n9.4.4 Standard Deviation\nThe standard deviation is by far the most common measure of variability. The standard deviation \\sigma is the square root of the variance \\sigma^2.The standard deviation is a measure of variabily that estimates the typical distance of a score from its own mean.\n\n\\begin{aligned}\n\\sigma&=\\sqrt{\\sigma^2}=\\sqrt{\\frac{\\sum_i^n{\\left(x_i-\\mu\\right)^2}}{n}}\\\\\ns&=\\sqrt{s^2}=\\sqrt{\\frac{\\sum_i^n{\\left(x_i-m\\right)^2}}{n-1}}\n\\end{aligned}\n\nAlthough it is not an arithmetic average of the deviations, it can be thought of as representing the typical size of the deviations. Technically, it is the square root of the average squared deviation.\nIn a normal distribution, the standard deviation is the distance from the mean to the two inflection points in the probability density curve (see Figure 9.14).An inflection point in a curve is the point at which the curvature changes from upward to downward or vice versa.\n\n\n\n\n\n\n\nFigure 9.14: The inflection points in the normal curve are 1 standard deviation from the mean.\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 9.14\n\n\n\n\n\n# The inflection points in the normal curve \n# are 1 standard deviation from the mean.\n\nd_text &lt;- tibble(x = c(-1, 1,-1.4, 1.4, -.65,.65),\n                 y = dnorm(x),\n                 angle = c(0,0,68,-68, 67,-67),\n                 hjust = c(1,0,rep(.5, 4)),\n                 vjust = c(0.5,0.5, 0.1, 0.1, 0.05, 0.05),\n                 label = c(paste0(\"Inflection point at &minus;1&lt;em&gt;\", span_style(\"&sigma;\"), \"&lt;/em&gt;\"),\n                           paste0(\"Inflection point at +1&lt;em&gt;\", span_style(\"&sigma;\"), \"&lt;/em&gt;\"),\n                           \"Upward curve\",\n                           \"Upward curve\",\n                           \"Downward curve\",\n                           \"Downward curve\"),\n                 color = c(\"black\", \"black\", myfills[1], myfills[1], myfills[2], myfills[2])\n                 )\n\nggplot(tibble(x = c(-4,4)), aes(x)) + \n  stat_function(fun = dnorm, geom = \"area\", alpha = 0.4, fill = myfills[1], xlim = c(-4,-1)) +\n  stat_function(fun = dnorm, geom = \"area\", alpha = 0.4, fill = myfills[2], xlim = c(-1,1)) +\n  stat_function(fun = dnorm, geom = \"area\", alpha = 0.4, fill = myfills[1], xlim = c(1,4)) +\n  annotate(x = c(-1,1), y = dnorm(c(-1,1)), geom = \"point\") + \n  geom_richlabel(aes(color = color, label = label, y = y, angle = angle, hjust = hjust, vjust = vjust), data = d_text, text_size = 18) + \n  scale_color_identity() +\n  scale_x_continuous(\"Standard Deviation Units\", \n                     breaks = -4:4, \n                     labels = paste0(signs::signs(-4:4, \n                                                  accuracy = 1, \n                                                  label_at_zero = \"blank\", \n                                                  add_plusses = T),\n                                     \"&lt;em&gt;\",\n                                     span_style(c(rep(\"&sigma;\", 4), \"&mu;\", rep(\"&sigma;\", 4))), \n                                     \"&lt;/em&gt;\")) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0,NA), expand = expansion()) +\n  coord_fixed(12)  +\n  geom_vline(xintercept = 0, color = \"gray30\") +\n  theme(panel.grid = element_blank(), \n        axis.text.x = element_markdown())\n\n\n\n\n\n9.4.5 Average Absolute Deviations\nAverage absolute deviations summarize the absolution values of deviations from a central tendency measure. There are many kinds of average absolute deviations, depending which central tendency each value deviates from and then which measure of central tendency summarizes the absolute deviations. With three central tendency measures, we can imagine nine different “average” absolute deviations:\n\\text{The}~\\begin{bmatrix}mean\\\\ median\\\\ modal\\end{bmatrix}~\\text{absolute deviation around the}~\\begin{bmatrix}mean\\\\ median\\\\ mode\\end{bmatrix}.\nThat said, two of these average absolute values are used more than the others: the median deviation around the median and mean deviation around the mean. One struggles to imagine what uses one might have for the others (e.g, the modal absolute deviation around the median or the mean absolute deviation around the mode).\n\n9.4.5.1 Median Absolute Deviation (around the Median)\nSuppose that we have a random variable X, which has a median of \\tilde{X}. The median absolute deviation (MAD) is the median of the absolute values of the deviations from the median:\n\n\\text{Median Absolute Deviation (around the Median)}=\\mathrm{median}\\left(\\left|X-\\tilde{X}\\right|\\right)\n\nA primary advantage of the MAD over the standard deviation is that it is robust to the presence of outliers. For symmetric distributions, the MAD is half the distance of the interquartile range.\n\n9.4.5.2 Mean Absolute Deviation (around the Mean)\nIf the mean of random variable X is \\mu_X, then:\n\n\\text{Mean Absolute Deviation (around the Mean)}=\\mathrm{mean}\\left(\\left|X-\\mu_X\\right|\\right)\n\nThe primary advantage of this statistic is that it is easy to explain to people with no statistical training. In a straightforward manner, it tells us how far each score is from the mean, on average.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#skewness",
    "href": "descriptives.html#skewness",
    "title": "\n9  Descriptives\n",
    "section": "\n9.5 Skewness",
    "text": "9.5 Skewness\n(Unfinished)",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#kurtosis",
    "href": "descriptives.html#kurtosis",
    "title": "\n9  Descriptives\n",
    "section": "\n9.6 Kurtosis",
    "text": "9.6 Kurtosis\n(Unfinished)",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#raw-moments",
    "href": "descriptives.html#raw-moments",
    "title": "\n9  Descriptives\n",
    "section": "\n10.1 Raw Moments",
    "text": "10.1 Raw Moments\nThe first raw moment \\mu'_1 of a random variable X is its expected value:\n\\mu'_1=\\mathcal{E}(X)=\\mu_X\nThe second raw moment \\mu'_2 is the expected value of X^2:\n\\mu'_2=\\mathcal{E}(X^2)\nThe nth raw moment \\mu'_n is the expected value of X^n:\n\\mu'_n=\\mathcal{E}(X^n)",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#central-moments",
    "href": "descriptives.html#central-moments",
    "title": "\n9  Descriptives\n",
    "section": "\n10.2 Central Moments",
    "text": "10.2 Central Moments\nThe first raw moment, the mean, has obvious significance and is easy to understand. The remaining raw moments do not lend themselves to easy interpretation. We would like to understand the higher moments after accounting for the lower moments. For this reason, we can discuss central moments, which are like raw moments after subtracting mean.\nOne can evaluate a moment “about” a constant c like so:1\n1 Alternately, we can say that this is the nth moment referred to c.\n\\text{The }n\\text{th moment of }X\\text{ about }c=\\mathcal{E}\\left(\\left(X-c\\right)^n\\right)\n\nA central moment \\mu_n is a moment about the mean (i.e., the first raw moment):A central moment is a raw moment after the variable’s mean has been subracted.\n\n\\mu_n=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^n\\right)\n\nThe first central moment \\mu_1 is not very interesting, because it always equals 0:\n\n\\begin{aligned}\\mu_1&=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^1\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)\\right)\\\\\n&=\\mathcal{E}\\left(X\\right)-\\mathcal{E}\\left(\\mu_X\\right)\\\\\n&=\\mu_X-\\mu_X\\\\\n&=0\n\\end{aligned}\n\nOf note, the second central moment \\mu_2 is the variance:\n\n\\mu_2=\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^2\\right)=\\sigma_X^2",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "descriptives.html#standardized-moments",
    "href": "descriptives.html#standardized-moments",
    "title": "\n9  Descriptives\n",
    "section": "\n10.3 Standardized Moments",
    "text": "10.3 Standardized Moments\nA standardized moment2 is the raw moment of a variable after it has been “standardized” (i.e., converted to a z-score):\n2 Standardized moments are also called normalized central moments.\nStandardizing a variable by converting it to z-score is accomplished like so: \nz=\\frac{X-\\mu_X}{\\sigma_X}\n\n\n\n\\text{The }n\\text{th standardized moment} = \\mathcal{E}\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^n\\right)=\\frac{\\mu_n}{\\sigma^n}\n\nThe first two standardized moments have little use because they are always the same for every variable. The first standardized moment is the expected value of a z-score, which is always 0.\n\n\\mathcal{E}\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^1\\right)=\\mathcal{E}\\left(z\\right) = 0\n The second standardized moment is the expected value of a z-score squared, which is always 1.\n\n\\mathcal{E}\\left(\\left(\\frac{X-\\mu_X}{\\sigma_X}\\right)^2\\right)=\\frac{\\mathcal{E}\\left(\\left(X-\\mu_X\\right)^2\\right)}{\\sigma_X^2} =\\frac{\\sigma_X^2}{\\sigma_X^2}= 1\n\nThe third standardized moment is the expected value of a z-score cubed, which is one an important measure of skewness.\nThe idea that skewness is the third standardized moment (i.e., the expected value of the z-score cubed) allows for an interesting intrepretation of skewness. To begin, the z-score by itself is a measure of the overall level of the score. The z-score squared is a measure of variability. Thus, skewness can be seen as the relationship between deviation and variability.\n\n\\mathcal{E}\\left(z^3\\right) = \\mathcal{E}\\left(\\underbrace{z}_{\\text{Level}}\\cdot \\underbrace{z^2}_{\\text{Variability}}\\right)\n\nThus a positively skewed variable can be described having a tendency to be become more variable (more sparse) as its value increases.\n\n\n\n\n\n\n\nFigure 10.1: Positive skewness can be interpeted as a tendency for the data to become more sparse as the scores increase.\n\n\n\n\n\n\n\n\n\n\n\nR Code for Figure 10.1\n\n\n\n\n\ntibble(x = seq(0, 15, .01), y = dchisq(x, 3)) %&gt;%\n  ggplot(aes(x, y)) +\n  geom_area(fill = myfills[1]) +\n  theme_void() +\n  coord_fixed(30) +\n  annotate(\n    \"segment\",\n    x = 3,\n    y = 0,\n    xend = 3,\n    yend = dchisq(3, 3) + .01,\n    color = \"white\"\n  ) +\n  geom_arrow(\n    data = tibble(x = c(3, 0), y = c(0.01, 0.01)),\n    arrow_head =  arrow_head_deltoid(),\n    color = \"white\",\n    resect = 1\n  ) +\n  geom_arrow(\n    data = tibble(x = c(3, 8), y = c(0.01, 0.01)),\n    arrow_head =  arrow_head_deltoid(),\n    color = \"white\",\n    resect = 1\n  ) +\n  geom_richtext(\n    data = tibble(\n      x = 3,\n      y = .01,\n      l = c(\"Lower Values&lt;br&gt;Less Sparse\", \"Higher Values&lt;br&gt;More Sparse\"),\n      hj = c(1,0)\n    ),\n    aes(label = l, hjust = hj),\n    vjust = 0,\n    fill = NA,\n    color = \"white\",\n    label.color = NA,\n    size = ggtext_size(16),\n    label.padding = margin(r = 6, l = 6, b = 3.5)\n  ) +\n  geom_richtext(\n    data = tibble(x = 3, y = 0, l = c(\"Mean\")),\n    aes(label = l),\n    vjust = 1,\n    label.color = NA,\n    size = ggtext_size(16),\n    color = myfills[1]\n  ) +\n  geom_blank(data = tibble(x = 0, y = -.02))",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Descriptives</span>"
    ]
  },
  {
    "objectID": "compositescores.html",
    "href": "compositescores.html",
    "title": "\n10  Composite Scores\n",
    "section": "",
    "text": "10.1 The Mean of the Sum Is the Sum of the Means\nThis section is going to make a simple idea look complicated. If you get lost, this is what I am trying to say:\nI’m sorry for what comes next, but the work we put into it will pay off later. Okay, now let’s make that simple idea formal and hard to understand:\nWe can start with the example of two variables: X_1 and X_2. The sum of these variables, which we will call X_S, has a mean, which is the sum of the means of X_1 and X_2. That is, \\mu_{S}=\\mu_{1}+\\mu_{2}.\nWhat if we have three variables? or four? or five? It would be tedious to illustrate each case one by one. We need a way of talking about the sum of a bunch of random variables but without worrying about how many variables there are. Here we go:",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "compositescores.html#the-mean-of-the-sum-is-the-sum-of-the-means",
    "href": "compositescores.html#the-mean-of-the-sum-is-the-sum-of-the-means",
    "title": "\n10  Composite Scores\n",
    "section": "",
    "text": "If we create a new random variable by adding a bunch of random variables together, the mean of that new variable is found by adding together the means of all the variables we started with.\n\n\n\n\n\n10.1.1 Calculating a Sum\nSuppose k is a positive integer greater than 1. So if there are k random variables, the notation for the set of all them is \\{X_1,...,X_k\\}. However, it is even more compact to use matrix notation such that \\boldsymbol{X}=\\{X_1,...,X_k\\}.\nNow, \\boldsymbol{X} is a set of random variables in their entirety, without referencing any particular values those variables might generate. A set of particular values of these variables would be shown as \\boldsymbol{x} or \\{x_1,...,x_k\\}. In regular notation, the sum of these particular values would be:\nx_S=\\sum_{i=1}^{k}{x_i}\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\nx_S\nThe sum of all k scores in \\{x_1,...,x_k\\}\n\n\n\nx_i\nA particular score generated by variable X_i\n\n\n\nk\nThe number of variables in \\{X_1,...,X_k\\}, (k \\in \\mathbb{N}_1)\n\n\n\n\n\n\nThe same formula is more compact in matrix notation:\n\nx_S=\\boldsymbol{x'1}\n\nWhere\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\\boldsymbol{x}\nA k \\times 1 vector of scores \\{x_1,x_2,...,x_k\\}\n\n\n\n\\boldsymbol{1}\nA k \\times 1 vector of ones \\{1_1,1_2,...,1_k\\}\n\n\n\n\n\n\nThe \\boldsymbol{1} symbol may be a bit confusing. It is a column of ones that has the same length (number of elements) as \\boldsymbol{x}. Suppose that \\boldsymbol{x} has a length of three. In this case:\n\\boldsymbol{1}_3=\\begin{bmatrix}1\\\\ 1\\\\ 1 \\end{bmatrix}\nAlso, \\boldsymbol{x'}, is \\boldsymbol{x} transposed.To transpose means to make all columns of a matrix into rows (or all rows into columns).\n\n\nTransposition is noted with a prime symbol (\\boldsymbol{^\\prime}). If\n\n\\boldsymbol{A}= \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\n then\n\n\\boldsymbol{A'}= \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\\\\\n\\end{bmatrix}\n\nA column of 3 ones:\n\n\\boldsymbol{1}_3=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\n\nTransposed, a column of 2 ones is a row of 2 ones.\n\n\\boldsymbol{{1'}}_2=\\begin{bmatrix}1&1\\end{bmatrix}\n\nTypically, the number of ones is implied such that the length of the column or row will be compatible with the adjacent matrix. For example, post-multiplying by a vector ones will create a vector of row totals:\n\n\\begin{align*}\n\\boldsymbol{A1}&=\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1+2+3\\\\4+5+6\\end{bmatrix}\\\\\n&=\\begin{bmatrix}6\\\\15\\end{bmatrix}\n\\end{align*}\n\nPre-multiplying by a vector of ones will create column totals:\n\n\\begin{align*}\n\\boldsymbol{1'A}&=\\begin{bmatrix}1\\\\1\\end{bmatrix}'\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1&1\\end{bmatrix}\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1+4 & 2+5 & 3+6\\end{bmatrix}\\\\&=\\begin{bmatrix}5 & 7 & 9\\end{bmatrix}\n\\end{align*}\n\nTo create a sum of the entire matrix, multiply by \\boldsymbol{1} on both sides:\n\\boldsymbol{1'A1}=21\nTherefore, \\boldsymbol{x'} is a row vector. A row vector multiplied by a column vector is the sum of the product of each analogous element in the pair of vectors. Thus,\n\n\\begin{align*}\n\\boldsymbol{x'1}&=\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3 \\end{bmatrix}'\n\\begin{bmatrix}1\\\\ 1\\\\ 1 \\end{bmatrix} \\\\\n&=\\begin{bmatrix} x_1 & x_2 & x_3\\end{bmatrix}\n\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}\\\\\n&=x_1 \\times 1 + x_2 \\times 1 + x_3 \\times 1\\\\\n&=  x_1 + x_2 + x_3\n\\end{align*}\n\nLet’s do some calculations in R with a particular example. Suppose that there are three variables: \\boldsymbol{X}=\\{X_1, X_2, X_3\\}. In R, we will create a vector of the names of variables in \\boldsymbol{X}:\n\nXnames &lt;- c(\"X1\",\"X2\",\"X3\")\n# Notes:\n# Xnames is a vector of variable names\n# The c function combines numbers (or other objects) into a vector.\n\nNow suppose that there are three particular scores: \\boldsymbol{x}=\\{100,120,118\\}\n\n# x = vector of particular scores from variables X1, X2, X3\nx &lt;- c(110,120,118)\n\n# Applying Xnames to x (to make output easier to read)\nnames(x) &lt;- Xnames \n\n# Notes: \n# The `names` function returns or sets the names of vector elements.\n# We can set names and values in a single line like so:\nx &lt;- c(X1 = 110, X2 = 120, X3 = 118)\n\nThe sum of these three scores can be calculated in a variety of ways. Here is the easiest:\n\n# x_S = The sum of scores x1, x2, and x3\nx_S &lt;- sum(x)\n\nHowever, if we want to be matrix algebra masochists (and, apparently, at least one of us does!), we could do this:\n\n# A vector of ones the same length as x\nones &lt;- rep(1,length(x))\n\n# Notes: \n# The rep function creates vectors of repeated elements. \n# For example, rep(5,3) is the same as c(5,5,5).\n# \n# The length function returns the number of elements in a vector. \n# For example, length(c(3,3)) returns `2`.\n\n\n# Calculating x_S with matrix algebra\nx_S &lt;- t(x) %*% ones\n\n# Notes: \n# The t function transposes a vector or matrix.\n# The operator %*% multiplies compatible matrices.\n\nEither way that we calculate it, x_S = 348.\n\n10.1.2 Calculating the Mean of a Sum\nThe mean of X_S is:\n\n\\mu_{S}=\\sum_{i=1}^{k}{\\mu_i}=\\boldsymbol{\\mu'1}\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\\mu_S\nThe mean of X_S\n\n\n\n\\mu_i\nThe mean of X_i\n\n\n\n\\boldsymbol{\\mu}\nA k \\times 1 vector of means of the variables in \\boldsymbol{X}\n\n\n\n\n\n\nSuppose that the means of X_1, X_2, and X_3 are all 100.\n\n# m = vector of means of X\nm &lt;- c(100,100,100)\n\nAgain, the mean of X_S (\\mu_S) can be calculated in two ways:\n\n# m_S = The mean of S\n# The easy way\nm_S &lt;- sum(m)\n\n# With matrix algebra\nm_S &lt;- t(m) %*% ones\n\nRunning this code, we can see that \\mu_S = 300.\n\n10.1.3 Calculating the Mean of a Weighted Sum\nThe mean of a weighted sum is the weighted sum of the means. That is, if\n\\begin{equation}\nx_S=\\sum_{i=1}^{k}{x_i w_i}=\\boldsymbol{x'w}\n\\end{equation}\nWhere\nthen\n\\begin{equation}\n\\mu_S=\\sum_{i=1}^{k}{\\mu_i w_i}=\\boldsymbol{\\mu'w}\n\\end{equation}\n\n\nNote that the calculation of X_S and \\mu_S with matrix algebra is the same as it was with an equally weighted sum except that instead of post-multiplying by a vector of ones (\\boldsymbol{x^\\prime 1}), we post-multiply by a vector of weights (\\boldsymbol{x^\\prime w}). In truth, an equally weighted sum is a special case of a weighted sum in which \\boldsymbol{w} consists entirely of ones.\nSuppose that \\boldsymbol{w} = \\{0.5,1,2\\}. That is, the weight for X_1 is 0.5, the weight for X_2 is 1, and the weight for X_3 is 2. We will continue to use the same values for \\boldsymbol{x} and \\boldsymbol{\\mu} as before:\n\n# w = The vector of weight for variables X1, X2, and X3\nw = c(0.5, 1, 2)\n\n# The easy way\nx_S_weighted &lt;- sum(x * w)\nm_S_weighted &lt;- sum(m * w)\n\n# Notes:\n# The multiplication operator * multiplies analogous elements \n# of vectors  and matrices. In the example, `x * w` is \n# c(110 * 0.5, 120 * 1, 118 * 2)\n\n# With matrix algebra\nx_S_weighted &lt;- t(x) %*% w\nm_S_weighted &lt;- t(m) %*% w\n\nRunning the code shows that x_S = 411 and that \\mu_S = 350.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "compositescores.html#the-variance-of-the-sum-is-the-sum-of-the-covariance-matrix",
    "href": "compositescores.html#the-variance-of-the-sum-is-the-sum-of-the-covariance-matrix",
    "title": "\n10  Composite Scores\n",
    "section": "\n10.2 The Variance of the Sum Is the Sum of the Covariance Matrix",
    "text": "10.2 The Variance of the Sum Is the Sum of the Covariance Matrix\n\n\nUnfortunately, the notation for a covariance matrix is a bold capital sigma \\boldsymbol{\\Sigma}, which is easily confused with the summation symbol, which is generally larger and not bold: \\sum.\nIf variables are uncorrelated, the variance of their sum is the sum of their variances. However, this is not true when variables are substantially correlated. The formula for the variance of a sum looks more complex than it is. It is just the sum of the covariance matrix.\n\n\\sigma_{X_S}^2=\\sum_{i=1}^{k}{\\sum_{j=1}^{k}{\\sigma_{ij}}}=\\boldsymbol{1'\\Sigma 1}\n(\\#eq:VarianceOfASum)\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\\sigma_{X_S}^2\nThe variance of X_S\n\n\n\n\\sigma_{ij}\nThe covariance between X_i and X_j (\\sigma_{ij}=\\sigma_i^2 if i=j)\n\n\n\n\\boldsymbol{\\Sigma}\nThe k \\times k covariance matrix of all the variables in \\boldsymbol{X}\n\n\n\n\n\n\n\n\nThe symbol for a sample correlation is the Roman lowercase r, and a matrix of such correlations is an uppercase \\boldsymbol{R}. Therefore, the population correlation coefficient is a Greek lowercase rho: \\rho. This, unfortunately means that a matrix of correlations should be an uppercase rho: \\boldsymbol{P}. Somehow, statisticians are okay with \\rho looking a lot like an italicized Roman letter p. However, using an uppercase rho (\\boldsymbol{P}) for a correlation matrix is too weird even for statisticians! You know, hobgoblins of little minds and all…\nSuppose that the standard deviations of X_1, X_2, and X_3 are all 15. Thus, \\sigma=\\{15,15,15\\}. The correlations among the three variables are shown in matrix \\boldsymbol{R}:\n\n\\boldsymbol{R} = \\begin{bmatrix}\n1 & 0.5 & 0.6\\\\\n0.5 & 1 & 0.7\\\\\n0.6 & 0.7 & 1\n\\end{bmatrix}\n\nTo create this matrix in R:\n\n# R = correlation matrix of variables in X\nR &lt;- matrix(c(1, 0.5, 0.6, \n              0.5, 1, 0.7,  \n              0.6, 0.7, 1),\n    nrow = 3,\n    ncol = 3,\n    byrow = TRUE)\n\nR\n\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.6\n[2,]  0.5  1.0  0.7\n[3,]  0.6  0.7  1.0\n\n\nThe covariance matrix \\boldsymbol{\\Sigma} can be computed from the correlation matrix and the standard deviations like so:\n\n\\sigma_{ij} = \\sigma_{i} \\sigma_{j} \\rho_{ij}\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\\sigma_{ij}\nThe covariance between X_i and X_j\n\n\n\n\\rho_{ij}\nThe correlation between X_i and X_j\n\n\n\n\\sigma_{i}\nThe standard deviation of variable X_i\n\n\n\n\\sigma_{j}\nThe standard deviation of variable X_j\n\n\n\n\n\n\nUnfortunately, computing a covariance matrix like this in a computer program is inelegant because we have to make use of looping:\n\n# R = Correlation matrix of variables in X\nR &lt;- matrix(c(1,0.5,0.6,\n              0.5,1,0.7,\n              0.6,0.7,1), nrow = 3)\nrownames(R) &lt;- colnames(R) &lt;- Xnames #Apply names\n\n# s = The standard deviations of variables in X\ns &lt;- c(15,15,15)\n\n# k = The number of variables\nk &lt;- length(s)\n\n# CM = Covariance Matrix\n# Initialize k by k matrix of zeroes\nCM &lt;- matrix(0, k, k)\nrownames(CM) &lt;- colnames(CM) &lt;- Xnames\nfor (i in seq(1, k)) {\n  for (j in seq(1, k)) {\n    CM[i, j] = s[i] * s[j] * R[i, j]\n  }\n}\n\nCM\n\n      X1    X2    X3\nX1 225.0 112.5 135.0\nX2 112.5 225.0 157.5\nX3 135.0 157.5 225.0\n\n\n\n\nThe \\mathtt{diag} function has three purposes. First, it can extract the diagonal vector from a matrix: \nA_{kk} =\n\\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  a_{k1} & a_{k2} & \\cdots & a_{kk}\n\\end{bmatrix}\n Then \\mathtt{diag}(\\boldsymbol{A}) = \\{a_{11},a_{22},...,a_{kk}\\} Second, the diag function inserts a vector into the the diagonal of a k \\times k matrix of zeros: \n\\mathtt{diag}(\\boldsymbol{a}) =\n\\begin{bmatrix}\n  a_{1} & 0 & \\cdots & 0 \\\\\n  0 & a_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & a_{k}\n\\end{bmatrix}\n Third, the diag function converts integer k into a k \\times k identity matrix: \n\\mathtt{diag}(k) =\n\\begin{bmatrix}\n  1 & 0 & \\cdots & 0 \\\\\n  0 & 1 & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & 1\n\\end{bmatrix}\n\nTo calculate the covariance matrix in matrix notation, we can make use of the \\mathtt{diag} function.\n\n\\begin{align*}\n\\boldsymbol{\\Sigma}&=\\mathtt{diag}(\\boldsymbol{\\sigma}) \\boldsymbol{R}\\, \\mathtt{diag}(\\boldsymbol{\\sigma})\\\\\n&=\\begin{bmatrix}\n  \\sigma_{1} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & \\sigma_{k}\n\\end{bmatrix}\n  \\begin{bmatrix}\n  1 & \\rho_{12} & \\cdots & \\rho_{1k} \\\\\n  \\rho_{21} & 1 & \\cdots & \\rho_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\rho_{k1} & \\rho_{k2} & \\cdots & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\sigma_{1} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & \\sigma_{k}\n\\end{bmatrix}\\\\\n  &=  \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{1}\\sigma_{2}\\rho_{12} & \\cdots & \\sigma_{1}\\sigma_{k}\\rho_{1k} \\\\\n  \\sigma_{2}\\sigma_{1}\\rho_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2}\\sigma_{k}\\rho_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k}\\sigma_{1}\\rho_{k1} & \\sigma_{k}\\sigma_{2}\\rho_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\\\\\n   &=  \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\n\\end{align*}\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\\boldsymbol{\\Sigma}\nThe covariance matrix of variables in \\boldsymbol{X}\n\n\n\n\\boldsymbol{\\sigma}\nThe standard deviations of variables in \\boldsymbol{X}\n\n\n\n\\boldsymbol{R}\nThe correlation matrix of variables in \\boldsymbol{X}\n\n\n\n\n\n\nSo this is where matrix algebra starts to shine. We do not need to initialize the variable that contains the covariance matrix, or calculate k, or do any looping.\n\n# CM = Covariance matrix of variables in X\nCM &lt;- diag(s) %*% R %*% diag(s)\n\nBeautiful!\nRunning the code, we get: \n\\boldsymbol{\\Sigma}=\\begin{bmatrix}\n225 & 112.5 & 135\\\\\n112.5 & 225 & 157.5\\\\\n135 & 157.5 & 225\n\\end{bmatrix}\n\nTo calculate the variance of X_S if it is an unweighted sum we apply Equation @ref(eq:VarianceOfASum):\n\n\n\nHere we see that \\sigma_X^2=1485",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "compositescores.html#calculating-the-variance-of-a-weighted-sum",
    "href": "compositescores.html#calculating-the-variance-of-a-weighted-sum",
    "title": "\n10  Composite Scores\n",
    "section": "\n10.3 Calculating the Variance of a Weighted Sum",
    "text": "10.3 Calculating the Variance of a Weighted Sum\nIf X_S is a weighted sum, its variance is the weighted sum of the covariance matrix.\n\n\\sigma_S^2=\\sum_{i=1}^{k}{\\sum_{j=1}^{k}{w_i w_j \\sigma_{ij}}}=\\boldsymbol{w'\\Sigma w}\n(\\#eq:VarianceOfAWeightedSum)\n\nContinuing with the same variables in our example, we see that things are starting to become clumsy and ugly without matrix algebra:\n\n# First we initialize var_S as 0\nvar_S_weighted &lt;- 0\n# Now we loop through k rows and k columns of CM\nfor (i in seq(1, k)) {\n  for (j in seq(1, k)) {\n    var_S_weighted &lt;- var_S_weighted + w[i] * w[j] * CM[i, j]\n  }\n}\n\nWith matrix algebra, this all happens with a single line of code:\n\nvar_S_weighted &lt;- t(w) %*% CM %*% w\n\nIf we don’t need to know the covariance matrix, we can skip its calculation:\n\nvar_S_weighted &lt;- t(w * s) %*% R %*% (w * s)\n\nAll three methods give the same answer: \\sigma_X^2 = 2193.75",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "compositescores.html#calculating-a-composite-score",
    "href": "compositescores.html#calculating-a-composite-score",
    "title": "\n10  Composite Scores\n",
    "section": "\n10.4 Calculating a Composite Score",
    "text": "10.4 Calculating a Composite Score\nWe now have all of the information needed to make a composite score. First we will make an unweighted composite.\n\\begin{equation}\nC = \\frac{x_S-\\mu_S}{\\sigma_{X_S}}\\sigma_C + \\mu_C\n(\\#eq:Composite)\n\\end{equation}\n\n\n\n\nSymbol\nMeaning\n\n\n\nC\nThe composite variable C\n\n\n\nx_S\nThe sum of all k scores in \\{x_1,...,x_k\\}\n\n\n\n\\sigma_{X_S}\nThe standard deviation of X_S\n\n\n\n\\sigma_C\nThe standard deviation of C\n\n\n\n\\mu_C\nThe mean of C\n\n\n\n\n\n\nContinuing with our example, suppose that \\mu_C=100 and \\sigma_C=15.\n\n# m_C = Composite mean\nm_C &lt;- 100\n# s_C = Composite standard deviation\ns_C &lt;- 15\n# C = The composite score\nC &lt;- ((x_S - m_S) / sqrt(var_S)) * s_C + m_C\n\n\nC=119\n\n\n10.4.1 Calculating a Weighted Composite Score\nIn matrix notation, an unweighted composite score is calculated like so:\n\nC=\\boldsymbol{\\frac{1'(x-\\mu_x)}{\\sqrt{1'\\Sigma 1}}}\\sigma_C+\\mu_C\n Replacing each vector of ones \\boldsymbol{1} with a weight vector \\boldsymbol{w} gives us the formula for computing a weighted composite score:\n\nC_w=\\boldsymbol{\\frac{w'(x-\\mu_x)}{\\sqrt{w'\\Sigma w}}}\\sigma_C+\\mu_C\n\n\n\nIn clinical practice, the most common kind of composite score is an equally weighted composite consisting of scores with the same mean and standard deviation. Sometimes scores are weighted by the degree to which they correlate with the construct the composite is intended to measure. Other weighting schemes are also possible. In most cases, it makes sense to first convert all scores to z-scores and then weight the z-scores. Failing to convert the scores to a common metric such as z-scores will result in an implicit weighting by standard deviations. That is, the score with the largest standard deviation will have the most weight in the composite score. Converting to z-scores first will equalize the each score’s influence on the composite score. We can convert all the scores in \\boldsymbol{x} to z-scores like so:\n\n\\boldsymbol{z} = \\frac{\\boldsymbol{x}-\\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}\n\nThe nice thing about z-scores is that their means are zeros, their standard deviations are ones, and their covariances are correlations. Thus, the formula for a weighted composite score consisting of z-scores is fairly simple, especially if the composite score is a z-score itself:\n\nC = \\frac{\\boldsymbol{w^\\prime z}}{\\sqrt{\\boldsymbol{w^\\prime Rw}}}\\sigma_C+\\mu_C\n\nContinuing with our example, computing a composite score with X_1=110, X_2=120, and X_3=118:\n\n# C = The composite score\nC &lt;- (t(w) %*% (x - m) / sqrt(t(w) %*% CM %*% w)) * 15  + 100\n\n# With index scores, we round to the nearest integer\nC &lt;- round(C)\n\n# Notes:\n# The round function rounds to the nearest integer by default,\n# but you can round to any number of digits you wish. For example, \n# rounding to 2 significant digits (i.e., the hundredths place), \n# would be round(x,2).\n\nHere we see, that the weighted composite score C = 120. If we had wanted an equally weighted composite score, the w vector would be set to equal ones. If we had done so, C would have been 119 instead of 120.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "compositescores.html#the-reliability-coefficient-of-a-composite",
    "href": "compositescores.html#the-reliability-coefficient-of-a-composite",
    "title": "\n10  Composite Scores\n",
    "section": "\n10.5 The Reliability Coefficient of a Composite",
    "text": "10.5 The Reliability Coefficient of a Composite\nCalculating the reliability coefficient of a composite score is much easier than it might appear at first. Remember that reliability is the ratio of a score’s true score variance to its total variance:\n\nr_{XX} = \\frac{\\sigma_T^2}{\\sigma_X^2}\n\nThe variance of a composite score is the sum of the covariance matrix of the composite’s component scores.\n\n\\sigma_X^2=\\boldsymbol{1}'\\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\\boldsymbol{1}\n\nThe covariance matrix of the component true scores is exactly the same as the covariance matrix of the component observed scores, except that the variances on the diagonal are multiplied by the reliability coefficients of each of the component scores like so:\n\n\\sigma_T^2=\\boldsymbol{1}'\\begin{bmatrix}\n  {\\color{firebrick}r_{11}}\\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & {\\color{firebrick}r_{22}}\\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & {\\color{firebrick}r_{kk}}\\sigma_{k}^2\n\\end{bmatrix}\\boldsymbol{1}\n\n\n# r_XX = Reliability coefficients for X1, X2, and X3\nr_XX &lt;- c(0.88,0.80,0.76)\n\n# CM_T = Covariance matrix of true scores\nCM_T &lt;- CM\n\n# Replace diagonal with true score variances\ndiag(CM_T) &lt;- diag(CM) * r_XX\n\n# r_C = Reliability coefficient of composite score\nr_C &lt;- sum(CM_T) / sum(CM)\n\nThe reliability coefficient of the unweighted composite is 0.92.\nFor a weighted composite, the reliability is calculated by relplacing each vector of ones \\boldsymbol{1} with a weight vector \\boldsymbol{w}:\n\n\\rho_{XX}=\\frac{\\sigma_T^2}\n               {\\sigma_X^2}\n         =\\frac{\\boldsymbol{w}^{\\prime}\n               \\begin{bmatrix}\n  {\\color{firebrick}r_{11}}\\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & {\\color{firebrick}r_{22}}\\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & {\\color{firebrick}r_{kk}}\\sigma_{k}^2\n                \\end{bmatrix}\n                \\boldsymbol{w}}\n          {\\boldsymbol{w}^{\\prime}\n               \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n\\end{bmatrix}\\boldsymbol{w}}\n\n\n# Reliability coefficient of the weighted composite\nr_C_w &lt;- t(w) %*% CM_T %*% w / t(w) %*% CM %*% w\n\nThe reliability coefficient of the weighted composite is 0.88.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "compositescores.html#composite-scores-and-their-correlations",
    "href": "compositescores.html#composite-scores-and-their-correlations",
    "title": "\n10  Composite Scores\n",
    "section": "\n10.6 Composite Scores and Their Correlations",
    "text": "10.6 Composite Scores and Their Correlations\nIf the population correlations among all of the components are known, it is possible to calculate the correlations among composite scores made from these components. Such correlations can be used in many practical applications, including in prediction models and in the evaluation of difference scores.\nSuppose that Composite A is calculated from the sum of two component tests, A_1 and A_2. Composite B is calculated from the sum of B_1 and B_2. Suppose that the correlation matrix for the four components is:\n\n\n\n\n\\boldsymbol{R} = \\begin{array}{r|cccc}\n& \\color{royalblue}{A_1} & \\color{royalblue}{A_2} & \\color{firebrick}{B_1} & \\color{firebrick}{B_2}\\\\\n\\hline \\color{royalblue}{A_1} & \\color{royalblue}{1} & \\color{royalblue}{.30} & \\color{purple}{.35} & \\color{purple}{.40}\\\\\n\\color{royalblue}{A_2} & \\color{royalblue}{.30} & \\color{royalblue}{1} & \\color{purple}{.42} & \\color{purple}{.48}\\\\\n\\color{firebrick}{B_1} & \\color{purple}{.35} & \\color{purple}{.42} & \\color{firebrick}{1} & \\color{firebrick}{.56}\\\\\n\\color{firebrick}{B_2} & \\color{purple}{.40} & \\color{purple}{.48} & \\color{firebrick}{.56} & \\color{firebrick}{1}\n\\end{array}\n\n\n# Make correlation matrix R\nR &lt;- matrix(c(1, .30, .35, .40,\n              .30, 1, .42, .48,\n              .35, .42, 1, .56,\n              .40, .48, .56, 1),\n            nrow = 4,\n            ncol = 4)\n\n# Display R\nR\n\n     [,1] [,2] [,3] [,4]\n[1,] 1.00 0.30 0.35 0.40\n[2,] 0.30 1.00 0.42 0.48\n[3,] 0.35 0.42 1.00 0.56\n[4,] 0.40 0.48 0.56 1.00\n\n\nThe correlation between Composite A and Composite B is calculating by adding up the numbers is all three shaded regions of the correlation matrix and then dividing the sum of “between” correlations in purple by the geometric mean of the sums from the “within” correlations in the blue and red regions in the correlation matrix \\boldsymbol{R} like so:\nr_{AB}=\\dfrac{\\color{purple}{\\text{Sum of Correlations between A and B}}}{\\sqrt{\\color{royalblue}{\\text{Sum of Correlations Within A}}\\times\\color{firebrick}{{\\text{Sum of Correlations Within B}}}}}\nTo calculate these sums using matrix algebra, we first construct a “weight matrix” \\boldsymbol{W} that tells us which tests are in which composite. The 1s in the first two rows of column 1 tell us that tests A_1 and A_2 belong to composite A. Likewise, the 1s in the last two rows of column 2 tell us that B_1 and B_2 belong to composite B.\n\n\\boldsymbol{W}=\n\\begin{array}{r|cc}\n& \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A_1} & \\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{A_2} & \\color{royalblue}{1} & 0\\\\\n\\color{firebrick}{B_1} & 0 & \\color{firebrick}{1}\\\\\n\\color{firebrick}{B_2} & 0 & \\color{firebrick}{1}\n\\end{array}\n\nHere is one way to make the weight matrix in R:\n\n# Make a 4 by 2 matrix of 0s\nW &lt;- matrix(0, nrow = 4, ncol = 2)\n\n# Assign 1s to rows 1 and 2 to column 1\nW[1:2,1] &lt;- 1\n# Assign 1s to rows 3 and 4 to column 2\nW[3:4,2] &lt;- 1\n\n# Display W\nW\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    0\n[3,]    0    1\n[4,]    0    1\n\n\n\n\n\nThe covariance matrix \\boldsymbol{\\Sigma_{AB}} is calculated by pre- and post-mulitplying \\boldsymbol{R} by the wieght matrix .\n\n\\begin{aligned}\\boldsymbol{\\Sigma_{AB}} &= \\boldsymbol{W'RW}\\\\[2ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{1} & 0\\\\\n0 & \\color{firebrick}{1}\\\\\n0 & \\color{firebrick}{1}\n\\end{array}\\right]'\\left[\\begin{array}{cccc}\n\\color{royalblue}{1} & \\color{royalblue}{.30} & \\color{purple}{.35} & \\color{purple}{.40}\\\\\n\\color{royalblue}{.30} & \\color{royalblue}{1} & \\color{purple}{.42} & \\color{purple}{.48}\\\\\n\\color{purple}{.35} & \\color{purple}{.42} & \\color{firebrick}{1} & \\color{firebrick}{.56}\\\\\n\\color{purple}{.40} & \\color{purple}{.48} & \\color{firebrick}{.56} & \\color{firebrick}{1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{1} & 0\\\\\n0 & \\color{firebrick}{1}\\\\\n0 & \\color{firebrick}{1}\n\\end{array}\\right]\\\\[2ex]\n&=\\begin{array}{r|cc}\n& \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A} & \\color{royalblue}{1 + .30 + .30 + 1} & \\color{purple}{.35 + .42 + .40 + .48}\\\\\n\\color{firebrick}{B} & \\color{purple}{.35 + .40 + .42 + .48} & \\color{firebrick}{1 + .56 + .56 + 1}\n\\end{array}\\\\[2ex]\n&=\\begin{array}{r|cc}\n& \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A} & \\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{firebrick}{B} & \\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\n\\end{aligned}\n\n\n# Covariance matrix of composites A and B\nSigma_AB &lt;- t(W) %*% R %*% W\n\n# Display Sigma_AB\nSigma_AB\n\n     [,1] [,2]\n[1,] 2.60 1.65\n[2,] 1.65 3.12\n\n\nNow we need to extract the variance diagonal from the covariance matrix so that we can use them to convert the covariance matrix to a correlation matrix. The variances are put on a diagonal matrix, and then taking the square root converts the variances to standard deviations in matrix \\boldsymbol{\\sigma}.\n\n\\begin{aligned}\n\\boldsymbol{\\sigma}\n&=\\mathtt{diag}(\\mathtt{diag}(\\boldsymbol{\\Sigma_{AB}}))^{\\frac{1}{2}}\\\\\n&=\\mathtt{diag}\\left(\n\\mathtt{diag}\\left({\n\\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]\n   }\\right)\n   \\right)^{\\frac{1}{2}}\\\\\n   &=\\mathtt{diag}\\left(\n   \\left[\\begin{array}{c}\n\\color{royalblue}{2.60}\\\\\n\\color{firebrick}{3.12}\n\\end{array}\\right]\n   \\right)^{\\frac{1}{2}}\\\\\n   &=\\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & 0\\\\\n0 & \\color{firebrick}{3.12}\n\\end{array}\\right]^{\\frac{1}{2}}\\\\\n   &=\\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]\n\\end{aligned}\n\n\n# Standard deviations\nsigma &lt;- diag(diag(Sigma_AB)^(0.5))\n\n# Display sigma\nsigma\n\n         [,1]     [,2]\n[1,] 1.612452 0.000000\n[2,] 0.000000 1.766352\n\n\nPre- and post-multiplying the covariance matrix \\boldsymbol{\\Sigma_{AB}} by the inverted standard devation matrix \\boldsymbol{\\sigma} to yield the correlations between composites A and B.\n\n\\begin{aligned}\n\\boldsymbol{R_{AB}}&=\\boldsymbol{\\sigma}^{-1}\\boldsymbol{\\Sigma_{AB}}\\boldsymbol{\\sigma}^{-1}\\\\[2ex]\n&={\\boldsymbol{\\sigma}^{-1} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]^{-1}}\n   {\\boldsymbol{\\Sigma_{AB}} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]}\n   {\\boldsymbol{\\sigma}^{-1} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]^{-1}}\\\\[1ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{.6202} & 0\\\\\n0 & \\color{firebrick}{.5661}\n\\end{array}\\right]\n   \\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]\n   \\left[\\begin{array}{cc}\n\\color{royalblue}{.6202} & 0\\\\\n0 & \\color{firebrick}{.5661}\n\\end{array}\\right]\\\\[1ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & \\color{purple}{.58}\\\\\n\\color{purple}{.58} & \\color{firebrick}{1}\n\\end{array}\\right]\n\\end{aligned}\n\n\n\nThe solve function inverts a square matrix. Inverting a single number x gives its reciprocal. If you multiply the inverse of a number by the nunbe itself, you get 1, the identity for the multiplication operator. x^{-1}x=1, multiplying a matrix by its inverse creates an identity matrix (a matrix with ones on the diagonal and zeroes elsewhere):\n\\boldsymbol{A^{-1}A = I}\n\n# Correlations between composites A and B\nR_AB &lt;- solve(sigma) %*% Sigma_AB %*% solve(sigma)\nR_AB\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5793219\n[2,] 0.5793219 1.0000000\n\n\nThese calculations in R can be greatly simplified using the cov2cor function, which converts covariances to correlations:\n\n# Correlation matrix of composites A and B\nR_AB &lt;- cov2cor(t(W) %*% R %*% W)\n\nR_AB\n\n          [,1]      [,2]\n[1,] 1.0000000 0.5793219\n[2,] 0.5793219 1.0000000",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Composite Scores</span>"
    ]
  },
  {
    "objectID": "unfinished.html",
    "href": "unfinished.html",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "",
    "text": "11.1 Probability, Odds, and Logits",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#probability-odds-and-logits",
    "href": "unfinished.html#probability-odds-and-logits",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "",
    "text": "11.1.1 Odds\nMost of us are comfortable with the notion of probability, but we find odds to be a little harder to understand. In everyday language, people use probability and odds interchangeably. Technically, they are related, but not at all the same thing.Probability is the ratio of the number outcomes in which the event of interest occurrs over the total number of outcomes.Odds refers to the ratio of the number of outcomes in which the event occurs to the number of outcomes in which the event does not occur.\nWhen the odds of winning are 3 to 1, there will be 3 wins for every loss.\n\n\n\n\n\n\n\n\nFigure 11.1: The relationship between probability and odds. Note that probability is bounded by 0 and 1, whereas odds can range from 0 to positive infinity.\n\n\n\nAs seen in Figure 11.1, the relationship between probability and odds is not linear. To convert an odds ratio to probability:\n\np=\\frac{odds}{1 + odds}\n\n\n\n\n\n\n\n\n\nThus, 3 to 1 odds is:\n\nodds &lt;- 3 / 1\n\np &lt;- odds / (1 + odds)\np\n\n[1] 0.75\n\n\nOr, 2 to 3 odds is:\n\nodds &lt;- 2 / 3\np &lt;- odds / (1 + odds)\np\n\n[1] 0.4\n\n\nTo convert probability to odds, one can simply use this formula:\n\nOdds = \\frac{p}{1-p}\n\nIf the probability of an event is .8, then the odds are:\n\np &lt;- .8\nodds &lt;- p / (1 - p)\nodds\n\n[1] 4\n\n\nProbabilities bounded by 0 and 1, inclusive. Odds have a minimum of 0 but have no upper bound (See Figure 11.1).\nIn many statistical applications, we need to convert a probability to a value that has neither upper nor lower bounds. A logit maps probabilities onto real numbers from negative to positive infinity (See Figure 11.2).A logit is a portmanteau of log unit. In item-response theory, ability and item difficulty are expressed in terms of logits. Logits are sometimes called the log-odds because they are calculated as the (natural) log of the odds: \\\\text{logit}(p)=\\\\ln\\\\left(\\\\frac{p}{1-p}\\\\right)\n\n\n\n\n\n\n\n\nFigure 11.2: The relationship between probability and logits. Whereas probability is bounded by 0 and 1, logits range from negative to positive infinity.\n\n\n\nA logit is the log-odds of probability.\n\n\\text{logit}\\left(p\\right)=\\ln\\left(\\frac{p}{1-p}\\right)\n\n\n\n\n\n\n\n\n\n\n11.1.2 W Scores\nIn the same way that we transform z-scores to various kinds of standard scores, we can transform logits to other kinds of scales. The most prominent in psychological assessment is the W-score (AKA Growth Score Values), developed by Woodcock & Dalh (1971). An accessible discussion of its derivation can by found in Benson et al. (2018).\n\nWoodcock, R. W., & Dalh, M. N. (1971). A common scale for the measurement of person ability and item difficulty (AGS Paper No. 10). American Guidance Service.\n\nBenson, N. F., Beaujean, A. A., Donohue, A., & Ward, E. (2018). W Scores: Background and derivation. Journal of Psychoeducational Assessment, 36(3), 273–277. https://doi.org/10.1177/0734282916677433\nIf ability \\theta is in logits, then W is calculated like so:\n\nW = \\frac{20}{\\ln(9)}\\theta+500\n\nAlthough the coefficient \\frac{20}{\\ln(9)} may seem strange, it has some desirable features. It is equivalent to 20 times the base-9 logarithm of e:\n\n20 \\log_9(e) = \\frac{20}{\\ln(9)} \\approx 9.1024\n\n\n\n\n\n\n\n\n\nFigure 11.3: The relationship between ability-difficulty differences and probability of answering correctly. Note that at −20, −10, 0, +10, and +20 the probabilities are exactly .10, .25, .50, .75, and .90, respectively.\n\n\n\nThis unusual coefficient yields some nice round probabilities of answering an item correctly when the person’s ability and the item’s difficulty differ by 0, 10, or 20 points (See Figure 11.3).\nThe probability p of answering an item correctly depends on the difference between the item’s difficulty W_D and the person’s ability W_A. Specifically, the relationship in Figure 11.3 works according to this equation:\n\np = \\left(1 + 9^{\\left(W_D-W_A\\right)/20}\\right)^{-1}",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#relative-proficiency",
    "href": "unfinished.html#relative-proficiency",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "\n11.2 Relative Proficiency",
    "text": "11.2 Relative Proficiency\nOne of the benefits of item response theory models is that ability is expressed on the same scale as item difficulties. This feature allows us to make predictions about how like a person with a particular ability level will correctly answer an item of a particular difficulty.\nThe Relative Proficiency Index (RPI) was first used in the Woodcock Reading Mastery Tests (Woodcock, 1973). The RPI answers the following question:The Relative Proficiency Index (RPI) tells us the proability a person with of ability \\\\theta will answer an item correctly given that a person with ability \\\\mu_\\\\theta has probability p of answering it correctly.\nWoodcock, R. W. (1973). Woodcock Reading Mastery Tests. American Guidance Service.\n\n\nWhen the average same-age peer with ability \\mu_\\theta has probability p of answering an item, what is the probability of answering correctly for a person of ability \\theta?\n\nThe RPI can be calculated like so:\n\nRPI = \\left(1+e^{-\\left(\\theta-\\mu_{\\theta}+\\text{logit}(p)\\right)}\\right)^{-1}\n \n\\begin{align*}\n  \\theta &= \\text{Ability level of person (in logits)}\\\\\n  \\mu_{\\theta} &= \\text{Average ability level (in logits) of a reference group}\\\\\n  p &= \\text{Probability a person with ability } \\mu_\\theta \\text{ will answer correctly}\\\\\n  \\text{logit}(p) &= \\ln\\left(\\frac{p}{1-p}\\right) = p \\text{ converted to logits}\n\\end{align*}\n\nThe psycheval package can calculate the RPI using the the rpi function. By default, the primary inputs x and mu are assumed to be on the W scale, and the criterion p is .90.\nSuppose a person’s ability corresponds to a W-score of 460 and same-age peers have an average W score of 500:\n\nlibrary(psycheval)\nrpi(x = 460, mu = 500)\n\n0.1\n\n\nThis difference of 40 W score points means that when typical same-age peers have a 90% chance of answering an item correctly (a common benchmark for mastery), this person has a 10% chance of answering the item correctly.\nIf x and mu are logits, then you can specify scale = 1 like so:\n\nrpi(x = 1, mu = 0, scale = 1)\n\n0.9607297\n\n\nThe RPI works nicely for documenting deficits, but for gifted students, the RPI is quite high, often near 1. In such cases, we can also calculate the probability a person with ability \\mu_\\theta can answer an item that a person with ability \\theta has a probability p_\\theta of answering correctly:\n\nRPI_{\\text{reversed}} = \\left(1+e^{-\\left(\\mu_\\theta-\\theta+\\text{logit}(p_\\theta)\\right)}\\right)^{-1}\n\nSuppose a person has a W score of 550, which is 50 points higher than typical same-age peers. The standard RPI will give a value close to 1:\n\nrpi(x = 550, mu = 500)\n\n0.999543\n\n\nThis means that when same-age peers have a 90% chance of answering the item correctly, this person is almost certain to answer it correctly. Unfortunately, this fact does not convey the degree of giftedness in an evocative manner.\nTo get a better sense of how far advanced this person is compared to the performance of typical same-age peers, we can reverse the RPI like so.\n\nrpi(x = 550, mu = 500, reverse = TRUE)\n\n0.03571429\n\n\nThis means that when this person has a .9 probability of answering an item correctly, the typical same-age peer has about a .036 probability of answering it. Thus, this person is capable of completing tasks that are quite difficult for typical same-age peers.\nThe standard RPI refers to a proficiency level of .9, but the rpi function can calculate the relative proficiency index at any criterion level. For example:\n\nrpi(x = 550, mu = 500, criterion = .10)\n\n0.9642857\n\n\nThis means that when a typical same-age peer has a .10 probability of answering an item correctly, this person will answer it correctly with a .96 probability.\nAn Excel spreadsheet that calculates this “generalized” RPI can be found here.",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#thresholds",
    "href": "unfinished.html#thresholds",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "\n11.3 Thresholds",
    "text": "11.3 Thresholds\nThe traditional threshold for diagnosing intellectual disability is 70. If a person’s observed IQ \\left(r_{XX}=.97\\right) is 68, what is the probability that the person’s true score is 70 or less?\nMore generally, given an observed score X with reliability coefficient r_{XX}, what is the probability that the associated true score T is less than or equal to threshold \\tau?\nWhen we predict the true score T with a specific value of X, the estimated true score \\hat{T} is:\n\\hat{T}=r_{XX}\\left(X-\\mu_X\\right)+\\mu_X\nThe standard error of the estimate \\sigma_{T-\\hat{T}} in this prediction is:\n\\sigma_{T-\\hat{T}}=\\sigma_X\\sqrt{r_{XX}-r_{XX}^2}\nIf we have reason to assume that the prediction errors are normally distributed, the probability that the true score T is less than or equal to threshold \\tau can be calculated using the standard normal cumulative distribution function \\Phi like so:\nP(T\\le\\tau)=\\Phi\\left(\\frac{\\tau-\\hat{T}}{\\sigma_{T-\\hat{T}}}\\right)\nUsing this formula, we can see that our hypothetical person with IQ = 68, the probability that the true score is 70 or lower is about .66. Figure 11.4 shows the probabilities for all values near the diagnostic threshold of 70.\n\n\n\n\n\n\n\nFigure 11.4: The Probability a True Score Will Be Less Than or Equal to 70\n\n\n\n\n\n\n\n\n\n\n\n\nAs a shortcut to using the formulas displayed above\n\n11.3.1 Multivariate Thresholds\nTo diagnose intellectual disability, we need a standardized measure of intellectual functioning (usually an IQ test) and well-validated measure of adaptive functioning. Suppose our two measures correlate at r = .40. The reliability coefficient of the IQ is r_{iq}=.97, and the reliability coefficient of the adaptive behavior composite is r_{ab}=.95. Both measures have a mean of \\mu=100 and a standard deviation of \\sigma=15.\nSuppose that a person with IQ = 68 has an adaptive behavior composite of 67. What is the probability that both true scores are 70 or lower?\nThe vector of observed scores is:\nX=\\{68, 67\\}\nThe vector of reliability coefficients:\nr_{XX}=\\{.97,.95\\}\nThe correlation matrix is:\n\nR_X=\\begin{bmatrix}\n1&.97\\\\\n.97&1\n\\end{bmatrix}\n\nThe vector of means is:\n\\mu_X=\\{100,100\\}\nThe vector of standard deviations is:\n\\sigma_X=\\{15,15\\}\nThe observed covariance matrix is:\n\\Sigma_X=\\sigma_X^\\prime R_X\\sigma_X\nThe true score covariance matrix is the same as the observed score covariance matrix except that the diagonal of \\Sigma_X is multiplied by the vector of reliability coefficients r_{XX}:\n\\Sigma_T=\\Sigma_X \\circ \\begin{bmatrix}.97&1\\\\1&.95\\end{bmatrix}\nThe cross-covariances between X and T also equal \\Sigma_T.\nWe can use equations from Eaton (2007, p. 116) to specify the conditional means and covariance matrix of the true scores, controlling for the observed scores:\n\nEaton, M. L. (2007). Multivariate statistics: A vector space approach. Inst. of Mathematical Statistics.\n\n\\begin{align}\n\\mu_{T\\mkern 2mu\\mid X}&=\\mu_X+\\Sigma_T\\Sigma_X^{-1}\\left(X-\\mu_X\\right)\\\\\n\\Sigma_{T\\mkern 2mu\\mid X}&=\\Sigma_T-\\Sigma_T\\Sigma_X^{-1}\\Sigma_T^\\prime\n\\end{align}\n\nWe can imagine that the true scores conditioned on the observed scores are multivariate normal variates:\n\\left(T\\mid X\\right)\\sim\\mathcal{N}\\left(\\mu_{T\\mkern 2mu\\mid X}, \\Sigma_{T\\mkern 2mu\\mid X}\\right)\nWe can estimate the probability that both true scores are 70 or lower using the cumulative distribution function of the multivariate normal distribution with upper bounds of 70 for both IQ true scores and adaptive behavior true scores. Under the conditions specified previously, Figure 11.5 shows that the probability that both scores are 70 or lower is about .53.\n\n\n\n\n\n\n\nFigure 11.5: If IQ = 68 and Adaptive Behavior = 67, what is the probability that both true scores are less than or equal to 70?",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#structure",
    "href": "unfinished.html#structure",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "\n12.1 Structure",
    "text": "12.1 Structure\nExploratory factor analysis\nConfirmatory factor analysis\nStructural equation modeling",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#reliability",
    "href": "unfinished.html#reliability",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "\n12.2 Reliability",
    "text": "12.2 Reliability\nRetest reliability\nAlternate-form reliability\nSplit-half reliability\nInternal consistency\n\nCronbach’s Alpha\nMcDonald’s Omega\n\nConditional reliability (IRT)",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#validity",
    "href": "unfinished.html#validity",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "\n12.3 Validity",
    "text": "12.3 Validity\nFace validity\nContent validity\nCriterion-oriented validity\n\nConcurrent validity\nPredictive validity\n\nDiscriminant validity\nConvergent validity\nIncremental validity\nConstruct validity",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "unfinished.html#profiles",
    "href": "unfinished.html#profiles",
    "title": "\n11  Unfinished Odds and Ends\n",
    "section": "\n12.4 Profiles",
    "text": "12.4 Profiles\nDifference scores\nOutliers\nHighest-lowest scores\nMultivariate profile shape\nConditional profiles",
    "crumbs": [
      "Variables",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Unfinished Odds and Ends</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Azzalini, A. (2023). Sn: The skew-normal and related distributions\nsuch as the skew-t and the SUN. http://azzalini.stat.unipd.it/SN/\n\n\nBenson, N. F., Beaujean, A. A., Donohue, A., & Ward, E. (2018). W\nScores: Background and derivation. Journal\nof Psychoeducational Assessment, 36(3), 273–277. https://doi.org/10.1177/0734282916677433\n\n\nChang, W. (2023). Extrafont: Tools for using fonts. https://github.com/wch/extrafont\n\n\nCicchetti, D., Bronen, R., Spencer, S., Haut, S., Berg, A., Oliver, P.,\n& Tyrer, P. (2006). Rating scales, scales of measurement, issues of\nreliability: Resolving some critical issues for clinicians\nand researchers. The Journal of Nervous and Mental Disease,\n194(8), 557–564.\n\n\nClark, A. (2004). Natural-born cyborgs: Minds,\ntechnologies, and the future of human intelligence. Oxford\nUniversity Press, USA.\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003).\nApplied multiple regression/correlation analysis for the behavioral\nsciences. L. Erlbaum Associates.\n\n\nCrocker, L., & Algina, J. (2006). Introduction to classical and\nmodern test theory. Cengage Learning.\n\n\nEaton, M. L. (2007). Multivariate statistics: A vector\nspace approach. Inst. of Mathematical Statistics.\n\n\nEmbretson, S. E., & Reise, S. P. (2000). Item response theory\nfor psychologists. Lawrence Erlbaum.\n\n\nFurr, R. (2017). Psychometrics: An introduction\n(3rd ed.). Sage.\n\n\nGarcia, D. M., Schmitt, M. T., Branscombe, N. R., & Ellemers, N.\n(2010). Women’s reactions to ingroup members who protest discriminatory\ntreatment: The importance of beliefs about inequality and\nresponse appropriateness. European Journal of Social\nPsychology, 40(5), 733–745.\n\n\nGawande, A. (2007). Better: A surgeon’s notes on\nperformance. Metropolitan Books.\n\n\nGombin, J. (2018). Qualvar: Implements indices of qualitative\nvariation proposed by wilcox (1973). http://joelgombin.github.io/qualvar/\n\n\nIhaka, R. (1998). R: Past and future history.\nInterface 98. https://www.stat.auckland.ac.nz/~ihaka/downloads/Interface98.pdf\n\n\nJensen, A. R. (2006). Clocking the mind: Mental\nchronometry and individual differences. Elsevier Science Limited.\n\n\nLey, P. (1972). Quantitative aspects of psychological assessment:\nAn introduction. Duckworth.\n\n\nLüdecke, D. (2021). Sjmisc: Data and variable transformation\nfunctions. https://strengejacke.github.io/sjmisc/\n\n\nMichell, J. (1997). Quantitative science and the definition of\nmeasurement in psychology. British Journal of Psychology,\n88(3), 355–383. https://doi.org/10.1111/j.2044-8295.1997.tb02641.x\n\n\nMichell, J. (2004). Item response models, Pathological\nscience and the shape of error: Reply to\nBorsboom and Mellenbergh. Theory &\nPsychology, 14(1), 121–129.\n\n\nMichell, J. (2008). Is psychometrics pathological science?\nMeasurement, 6(1-2), 7–24.\n\n\nMüller, K., & Wickham, H. (2023). Tibble: Simple data\nframes. https://tibble.tidyverse.org/\n\n\nNunnally, J. C. (1967). Psychometric theory. McGraw-Hill.\n\n\nPearson, K., & Heron, D. (1913). On Theories of\nAssociation. Biometrika, 9(1/2), 159–315.\nhttps://doi.org/10.2307/2331805\n\n\nPedersen, T. L. (2024). Ggforce: Accelerating ggplot2. https://ggforce.data-imaginist.com\n\n\nPedersen, T. L., & Robinson, D. (2024). Gganimate: A grammar of\nanimated graphics. https://gganimate.com\n\n\nPoncet, P. (2019). Modeest: Mode estimation. https://github.com/paulponcet/modeest\n\n\nR Core Team. (2024). R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRaykov, T., & Marcoulides, G. A. (2011). Introduction to\npsychometric theory. Routledge.\n\n\nSchneider, W. J. (2023). Psycheval: A psychological evaluation\ntoolkit. https://github.com/wjschne/psycheval\n\n\nSharpsteen, C., & Bracken, C. (2023). tikzDevice: R graphics\noutput in LaTeX format. https://github.com/daqana/tikzDevice\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103(2684), 677–680. https://doi.org/10.1126/science.103.2684.677\n\n\nStrang, G. (2016). Introduction to linear algebra (5th\nedition). Cambridge press.\n\n\nWickham, H. (2023a). Forcats: Tools for working with categorical\nvariables (factors). https://forcats.tidyverse.org/\n\n\nWickham, H. (2023b). Stringr: Simple, consistent wrappers for common\nstring operations. https://stringr.tidyverse.org\n\n\nWickham, H. (2023c). Tidyverse: Easily install and load the\ntidyverse. https://tidyverse.tidyverse.org\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K.,\nWilke, C., Woo, K., Yutani, H., Dunnington, D., & van den Brand, T.\n(2024). ggplot2: Create elegant data visualisations using the\ngrammar of graphics. https://ggplot2.tidyverse.org\n\n\nWickham, H., François, R., Henry, L., Müller, K., & Vaughan, D.\n(2023). Dplyr: A grammar of data manipulation. https://dplyr.tidyverse.org\n\n\nWickham, H., & Henry, L. (2023). Purrr: Functional programming\ntools. https://purrr.tidyverse.org/\n\n\nWickham, H., Vaughan, D., & Girlich, M. (2024). Tidyr: Tidy\nmessy data. https://tidyr.tidyverse.org\n\n\nWilcox, A. R. (1973). Indices of qualitative variation and political\nmeasurement. The Western Political Quarterly, 26(2),\n325. https://doi.org/10.2307/446831\n\n\nWoodcock, R. W. (1973). Woodcock Reading Mastery\nTests. American Guidance Service.\n\n\nWoodcock, R. W., & Dalh, M. N. (1971). A common scale for the\nmeasurement of person ability and item difficulty (AGS Paper No.\n10). American Guidance Service.\n\n\nWuertz, D., Setz, T., & Theussl, S. (2023). fMultivar: Rmetrics\n- modeling of multivariate financial return distributions. https://www.rmetrics.org\n\n\nXie, Y. (2023). Knitr: A general-purpose package for dynamic report\ngeneration in r. https://yihui.org/knitr/\n\n\nZacharias, J. R. (1975). The trouble with IQ tests.\nNational Elementary Principal, 54(4), 23–29.",
    "crumbs": [
      "Variables",
      "References"
    ]
  },
  {
    "objectID": "matrix_algebra.html",
    "href": "matrix_algebra.html",
    "title": "Matrix Algebra in R",
    "section": "",
    "text": "Matrix algebra sounds intimidating.  If I could start somewhere else, I would, but psychometrics is too painful without matrix notation. Spending time to become familiar with matrix algebra makes a lot of otherwise difficult things easy. I am continually amazed and surprised by the diverse ways in which learning this material has paid dividends to me. I use matrix algebra to solve problems not just in “mathy” applications like psychometrics and statistics but in domains I never imagined it would have utility (e.g., art, design, and music). Trust me. This is potent stuff, and its power can be yours.Matrix algebra refers to performing arithmetic operations and algebraic manipulations on numbers arranged in rectangular arrays.For now, we will focus solely on numeric data, setting aside the possibility of creating vectors and matrices with non-numeric data such as logical (Boolean) values, nominal data, ordinal values, dates, or complex structures such as lists.\nDespite appearances, matrix algebra is not so hard. In my opinion, learning regular algebra is the hard part. If you made it through high school with a strong understanding of algebra, you will see that matrix algebra is mostly an extension of what you already know. Give it a try and see what happens. If learning matrix algebra seems hard, firming up your foundational knowledge of regular algebra may be a better place to start.\nAlgebra uses symbols in formulas to represent numbers. You are probability familiar with using algebraic methods to solve for unknown values in equations. For example, in the equation below, there is only one possible value of x that makes the equation true:\n\n\\begin{aligned}\n&x-1&=&~0\\\\\n&x-1+1&=&~0+1\\\\\n&x+0&=&~1\\\\\n&x&=&~1\n\\end{aligned}\n\nMatrix algebra allows us to use one symbol to represent more than one number at a time. So, instead of using one symbol to represent one number (e.g., x=4), a single symbol can represent a sequence of numbers: \\mathbf{x}=\\{3,1,5,9\\}\nOr just one symbol can represent a rectangular array of numbers:\n\n\\mathbf{A}=\\begin{bmatrix}\n5&7&9&2\\\\\n7&0&1&4\\\\\n3&2&8&0\n\\end{bmatrix}\n\nIn this way, we can perform the same operations (e.g., addition, multiplication, exponentiation) on many numbers at once (e.g., creating statistical models from many rows of data). The notation of matrix algebra often makes what would otherwise be large, messy, complex statistical formulas look simple, tidy, and compact.\nIn matrix algebra, we can arrange numbers as scalars (single values), vectors (sequences of values), and matrices (values in rectangular arrays).",
    "crumbs": [
      "Matrix Algebra in R"
    ]
  }
]