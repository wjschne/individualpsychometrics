{
  "hash": "99bc6d87ee7e37c519196a8b75d47638",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Composite Scores\"\n---\n\n\n\n\nClinicians use [composite scores]{.defword title=\"A **composite variable** is the (possibly weighted) sum of two or more variables.\"} whenever they can because they tend to be more reliable and valid than single scores. How much more reliable is a composite score than a single score? It, of course, depends. First, the reliability coefficients of all of the scores that make up the composite score matter. Second, the correlations among the components of the composite matter. The more reliable and correlated the components, the more reliable the composite.\n\nAs discussed previously, it is quite possible for different scores from the same test to have different reliability coefficients. Scores from the same test tend to have similar reliability coefficients, though it is quite possible for differences in reliability to be large. In a well designed test, such cases are rare. If we can assume that the test is well designed and that test score reliability coefficients are within a fairly narrow range, we can assume that the classical reliability coefficient is a good estimate of the reliability of all scores from a particular test. To the degree that such assumptions are incorrect, the following equations will be less accurate. \n\nBefore we can discuss the reliability of composites, we must cover more basic statistics as they apply to composite scores. I will present this material in three ways. First, I will show the equations in standard notation. Adjacent to this, I will show the same equations in matrix notation.^[Why bother with matrices? It does seem to be an unnecessary complication at first. However, many things become clearer and simpler with matrices, which I hope to illustrate. Furthermore, R was designed to work elegantly with vectors and matrices.] Then I will walk through the calculations using R.\n\n## The Mean of the Sum Is the Sum of the Means\n\nThis section is going to make a simple idea look complicated. If you get lost, this is what I am trying to say: \n\n> If we create a new random variable by adding a bunch of random variables together, the mean of that new variable is found by adding together the means of all the variables we started with.\n\n\nI'm sorry for what comes next, but the work we put into it will pay off later. Okay, now let's make that simple idea formal and hard to understand: \n\nWe can start with the example of two variables: $X_1$ and $X_2.$ The sum of these variables, which we will call $X_S$, has a mean, which is the sum of the means of $X_1$ and $X_2.$ That is, $\\mu_{S}=\\mu_{1}+\\mu_{2}.$\n\n\nWhat if we have three variables? or four? or five? It would be tedious to illustrate each case one by one. We need a way of talking about the sum of a bunch of random variables but without worrying about how many variables there are. Here we go:\n\n### Calculating a Sum \n\nSuppose $k$ is a positive integer greater than 1. So if there are $k$ random variables, the notation for the set of all them is $\\{X_1,...,X_k\\}.$ However, it is even more compact to use matrix notation such that $\\mathbf{X}=\\{X_1,...,X_k\\}.$\n\nNow, $\\mathbf{X}$ is a set of random variables in their entirety, without referencing any particular values those variables might generate. A set of particular values of these variables would be shown as $\\mathbf{x}$ or $\\{x_1,...,x_k\\}.$ In regular notation, the sum of these particular values would be:\n\n$$\nx_S=\\sum_{i=1}^{k}{x_i}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-colwidths='[20,80]'}\n::: {.cell-output-display}\n\n\n|Symbol |Meaning                                                              |\n|:------|:--------------------------------------------------------------------|\n|$x_S$  |The sum of all $k$ scores in $\\{x_1,...,x_k\\}$                       |\n|$x_i$  |A particular score generated by variable $X_i$                       |\n|$k$    |The number of variables in $\\{X_1,...,X_k\\}$, $(k \\in \\mathbb{N}_1)$ |\n\n\n:::\n:::\n\n\n\nThe same formula is more compact in matrix notation:\n\n$$\nx_S=\\mathbf{x'1}\n$$\n\n\nWhere\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\" tbl-colwidths='[20,80]'}\n::: {.cell-output-display}\n\n\n|Symbol       |Meaning                                               |\n|:------------|:-----------------------------------------------------|\n|$\\mathbf{x}$ |A $k \\times 1$ vector of scores $\\{x_1,x_2,...,x_k\\}$ |\n|$\\mathbf{1}$ |A $k \\times 1$ vector of ones $\\{1_1,1_2,...,1_k\\}$   |\n\n\n:::\n:::\n\n\nThe $\\mathbf{1}$ symbol may be a bit confusing. It is a column of ones that has the same length (number of elements) as $\\mathbf{x}.$ Suppose that $\\mathbf{x}$ has a length of three. In this case:\n\n$$\\mathbf{1}_3=\\begin{bmatrix}1\\\\ 1\\\\ 1 \\end{bmatrix}$$\n\n\nAlso, $\\mathbf{x'}$, is $\\mathbf{x}$ [transposed]{.defword title=\"To **transpose** means to make all columns of a matrix into rows (or all rows into columns).\"}. \n\n\n:::{.column-margin}\n\nTransposition is noted with a prime symbol $(\\mathbf{^\\prime})$. If \n\n$$\n\\mathbf{A}= \\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\n$$ \nthen\n\n$$\n\\mathbf{A'}= \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\\\\\n\\end{bmatrix}\n$$\n\nA column of 3 ones:\n\n$$\n\\mathbf{1}_3=\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\n$$\n\nTransposed, a column of 2 ones is a row of 2 ones.\n\n$$\n\\mathbf{{1'}}_2=\\begin{bmatrix}1&1\\end{bmatrix}\n$$\n\n\nTypically, the number of ones is implied such that the length of the column or row will be compatible with the adjacent matrix. For example, post-multiplying by a vector ones will create a vector of row totals:\n\n$$\n\\begin{align*}\n\\mathbf{A1}&=\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1+2+3\\\\4+5+6\\end{bmatrix}\\\\\n&=\\begin{bmatrix}6\\\\15\\end{bmatrix}\n\\end{align*}\n$$\n\nPre-multiplying by a vector of ones will create column totals:\n\n$$\n\\begin{align*}\n\\mathbf{1'A}&=\\begin{bmatrix}1\\\\1\\end{bmatrix}'\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1&1\\end{bmatrix}\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\\\\\n\\end{bmatrix}\\\\\n&=\\begin{bmatrix}1+4 & 2+5 & 3+6\\end{bmatrix}\\\\&=\\begin{bmatrix}5 & 7 & 9\\end{bmatrix}\n\\end{align*}\n$$\n\nTo create a sum of the entire matrix, multiply by $\\mathbf{1}$ on both sides:\n\n$$\n\\mathbf{1}'\\mathbf{A1}=21\n$$\n\n:::\n\nTherefore, $\\mathbf{x}'$ is a row vector. A row vector multiplied by a column vector is the sum of the product of each analogous element in the pair of vectors. Thus,\n\n$$\n\\begin{align*}\n\\mathbf{x'1}&=\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3 \\end{bmatrix}'\n\\begin{bmatrix}1\\\\ 1\\\\ 1 \\end{bmatrix} \\\\\n&=\\begin{bmatrix} x_1 & x_2 & x_3\\end{bmatrix}\n\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}\\\\\n&=x_1 \\times 1 + x_2 \\times 1 + x_3 \\times 1\\\\\n&=  x_1 + x_2 + x_3\n\\end{align*}\n$$\n\nLet's do some calculations in R with a particular example. Suppose that there are three variables: $\\mathbf{X}=\\{X_1, X_2, X_3\\}.$ In R, we will create a vector of the names of variables in $\\mathbf{X}$:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nXnames <- c(\"X1\",\"X2\",\"X3\")\n```\n:::\n\n\nNow suppose that there are three particular scores: $\\mathbf{x}=\\{100,120,118\\}$\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# x = vector of particular scores from variables X1, X2, X3\nx <- c(110,120,118)\n\n# The `names` function returns or sets the names of vector elements.\n# Applying Xnames to x (to make output easier to read)\nnames(x) <- Xnames \n```\n:::\n\n\n:::{callout-note}\n\nAlternatively, names and values can be set in a single line:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- c(X1 = 110, X2 = 120, X3 = 118)\n```\n:::\n\n\nFor reasons of clarity and readability, I prefer this approach for name assignments that I only need to perform once. However, when I need to apply the same names to different objects repeated, I use the `names` function to apply a vector of names.\n\n:::\n\nThe sum of these three scores can be calculated in a variety of ways. Here is the easiest:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# x_S = The sum of scores x1, x2, and x3\nx_S <- sum(x)\n```\n:::\n\n\nHowever, if we want to be matrix algebra masochists (and, apparently, at least one of us does!), we could do this:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# A vector of ones the same length as x\nones <- rep(1, length(x))\n\n# Notes: \n# The rep function creates vectors of repeated elements. \n# For example, rep(5,3) is the same as c(5,5,5).\n# \n# The length function returns the number of elements in a vector. \n# For example, length(c(3,3)) returns `2`.\n\n\n# Calculating x_S with matrix algebra\nx_S <- t(x) %*% ones\n\n# Notes: \n# The t function transposes a vector or matrix.\n# The operator %*% multiplies compatible matrices.\n```\n:::\n\n\nEither way that we calculate it, $x_S = 348.$\n\n### Calculating the Mean of a Sum \n\nThe mean of $X_S$ is:\n\n$$\n\\mu_{S}=\\sum_{i=1}^{k}{\\mu_i}=\\mathbf{\\mu'1}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol         |Meaning                                                         |\n|:--------------|:---------------------------------------------------------------|\n|$\\mu_S$        |The mean of $X_S$                                               |\n|$\\mu_i$        |The mean of $X_i$                                               |\n|$\\mathbf{\\mu}$ |A $k \\times 1$ vector of means of the variables in $\\mathbf{X}$ |\n\n\n:::\n:::\n\n\nSuppose that the means of $X_1$, $X_2$, and $X_3$ are all 100. \n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# m = vector of means of X\nm <- c(100,100,100)\n```\n:::\n\nAgain, the mean of $X_S$ $(\\mu_S)$ can be calculated in two ways:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# m_S = The mean of S\n# The easy way\nm_S <- sum(m)\n\n# With matrix algebra\nm_S <- t(m) %*% ones\n```\n:::\n\nRunning this code, we can see that $\\mu_S = 300.$\n\n### Calculating the Mean of a Weighted Sum \n\nThe mean of a weighted sum is the weighted sum of the means. That is, if\n\n\\begin{equation}\nx_S=\\sum_{i=1}^{k}{x_i w_i}=\\mathbf{x'w}\n\\end{equation}\n\nWhere\n\n\\begin{conditions*}\nw_i & The weight for $X_i$\\\\\n\\mathbf{w} & A $k \\times 1$ vector of weights for each of the variables in $\\mathbf{X}$\\\\\n\\end{conditions*}\n\nthen\n\n\\begin{equation}\n\\mu_S=\\sum_{i=1}^{k}{\\mu_i w_i}=\\mathbf{\\mu'w}\n\\end{equation}\n\n:::{.column-margin}\nNote that the calculation of $X_S$ and $\\mu_S$ with matrix algebra is the same as it was with an equally weighted sum except that instead of post-multiplying by a vector of ones $(\\mathbf{x^\\prime 1})$, we post-multiply by a vector of weights $(\\mathbf{x^\\prime w}).$ In truth, an equally weighted sum is a special case of a weighted sum in which $\\mathbf{w}$ consists entirely of ones.\n:::\n\nSuppose that $\\mathbf{w} = \\{0.5,1,2\\}.$ That is, the weight for $X_1$ is 0.5, the weight for $X_2$ is 1, and the weight for $X_3$ is 2. We will continue to use the same values for $\\mathbf{x}$ and $\\mathbf{\\mu}$ as before:\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# w = The vector of weight for variables X1, X2, and X3\nw = c(0.5, 1, 2)\n\n# The easy way\nx_S_weighted <- sum(x * w)\nm_S_weighted <- sum(m * w)\n\n# Notes:\n# The multiplication operator * multiplies analogous elements \n# of vectors  and matrices. In the example, `x * w` is \n# c(110 * 0.5, 120 * 1, 118 * 2)\n\n# With matrix algebra\nx_S_weighted <- t(x) %*% w\nm_S_weighted <- t(m) %*% w\n```\n:::\n\n\nRunning the code shows that $x_S = 411$ and that $\\mu_S = 350.$\n\n## The Variance of the Sum Is the Sum of the Covariance Matrix\n\n:::{.column-margin}\nUnfortunately, the notation for a covariance matrix is a bold capital sigma $\\mathbf{\\Sigma}$, which is easily confused with the summation symbol, which is generally larger and not bold: $\\sum.$\n:::\n\nIf variables are uncorrelated, the variance of their sum is the sum of their variances. However, this is not true when variables are substantially correlated. The formula for the variance of a sum looks more complex than it is. It is just the sum of the covariance matrix.\n\n$$\n\\sigma_{X_S}^2=\\sum_{i=1}^{k}{\\sum_{j=1}^{k}{\\sigma_{ij}}}=\\mathbf{1'\\Sigma 1}\n$$ {#eq-variancesum}\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol            |Meaning                                                                    |\n|:-----------------|:--------------------------------------------------------------------------|\n|$\\sigma_{X_S}^2$  |The variance of $X_S$                                                      |\n|$\\sigma_{ij}$     |The covariance between $X_i$ and $X_j$ $(\\sigma_{ij}=\\sigma_i^2$ if $i=j)$ |\n|$\\mathbf{\\Sigma}$ |The $k \\times k$ covariance matrix of all the variables in $\\mathbf{X}$    |\n\n\n:::\n:::\n\n\n:::{.column-margin}\nThe symbol for a sample correlation is the Roman lowercase $r$, and a matrix of such correlations is an uppercase $\\mathbf{R}.$ Therefore, the population correlation coefficient is a Greek lowercase rho: $\\rho.$ This, unfortunately means that a matrix of correlations should be an uppercase rho: $\\mathbf{P}.$ Somehow, statisticians are okay with $\\rho$ looking a lot like an italicized Roman letter *p*. However, using an uppercase rho $(\\mathbf{P})$ for a correlation matrix is too weird even for statisticians! You know, hobgoblins of little minds and all...\n:::\n\n:::{#fig-bienayme .column-margin}\n\n![](images/Bienayme.jpeg)\n\nIrénée-Jules Bienaymé (1796--1878)<br>[Image Credit](https://commons.wikimedia.org/wiki/File:Ir%C3%A9n%C3%A9e-Jules_Bienaym%C3%A9.jpeg)\n:::\n\nIrénée-Jules Bienaymé is credited with the derivation of @eq-variancesum. The 2-test version of the equation, @eq-bienayme, is called [Bienayme's Identity](https://en.wikipedia.org/wiki/Bienaym%C3%A9's_identity).\n\n$$\n\\sigma_{X+Y}^2 = \\sigma_X^2 + 2\\sigma_{XY} +\\sigma_Y^2\n$$ {#eq-bienayme}\n\n:::{.callout-note title=\"Proof of @eq-bienayme\" collapse=\"true\"}\n\nTo prove @eq-bienayme, recall that variance in general is defined as \n\n$$\n\\begin{aligned}\n\\mathrm{Var}(X)&=\\mathcal{E}\\left(\\left(X-\n\\mathcal{E}\\left(X\\right)\\right)^2\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X-\n\\mu_X\\right)^2\\right)\\\\\n&=\\sigma_X^2\n\\end{aligned}\n$$ {#eq-variancedef}\n\nand that covariance is defined as\n\n$$\n\\begin{aligned}\n\\mathrm{Cov}(XY)&=\\mathcal{E}\\left(\\left(X-\n\\mathcal{E}\\left(X\\right)\\right)\\left(Y-\n\\mathcal{E}\\left(Y\\right)\\right)\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X-\n\\mu_X\\right)\\left(Y-\n\\mu_Y\\right)\\right)\\\\\n&=\\sigma_{XY}\n\\end{aligned}\n$$ {#eq-covariancedef}\n\nApplying @eq-variancedef to $X+Y$ and making substitutions from @eq-covariancedef:\n\n$$\n\\begin{aligned}\n\\rm{Var}(X+Y)&=\\mathcal{E}\\left(\\left(\\left(X+Y\\right)-\n\\mathcal{E}\\left(X+Y\\right)\\right)^2\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X+Y-\n\\mathcal{E}\\left(X\\right)-\\mathcal{E}\\left(Y\\right)\\right)^2\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X+Y-\n\\mu_X-\\mu_Y\\right)^2\\right)\\\\\n&=\\mathcal{E}\\left(\\left(\\left(X-\n\\mu_X\\right)+\\left(Y-\\mu_Y\\right)\\right)^2\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X-\n\\mu_X\\right)^2+2\\left(X-\\mu_X\\right)\\left(Y-\\mu_Y\\right)+\\left(Y-\\mu_Y\\right)^2\\right)\\\\\n&=\\mathcal{E}\\left(\\left(X-\n\\mu_X\\right)^2\\right)+2\\mathcal{E}\\left(\\left(X-\\mu_X\\right)\\left(Y-\\mu_Y\\right)\\right)+\\mathcal{E}\\left(\\left(Y-\\mu_Y\\right)^2\\right)\\\\\n&=\\sigma_X^2+2\\sigma_{XY}+\\sigma_Y^2\n\\end{aligned}\n$$\nProof adapted from @sochProofVarianceSum2020.\n\n:::\n\n\n\nSuppose that the standard deviations of $X_1$, $X_2$, and $X_3$ are all 15. Thus, $\\sigma=\\{15,15,15\\}.$ The correlations among the three variables are shown in matrix $\\mathbf{R}$:\n\n\n\n$$\n\\mathbf{R} = \\begin{bmatrix}\n1 & 0.5 & 0.6\\\\\n0.5 & 1 & 0.7\\\\\n0.6 & 0.7 & 1\n\\end{bmatrix}\n$$\n\nTo create this matrix in R:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# R = correlation matrix of variables in X\nR <- matrix(c(1, 0.5, 0.6, \n              0.5, 1, 0.7,  \n              0.6, 0.7, 1),\n    nrow = 3,\n    ncol = 3,\n    byrow = TRUE)\n\nR\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]  1.0  0.5  0.6\n[2,]  0.5  1.0  0.7\n[3,]  0.6  0.7  1.0\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe covariance matrix $\\mathbf{\\Sigma}$ can be computed from the correlation matrix and the standard deviations like so:\n\n$$\n\\sigma_{ij} = \\sigma_{i} \\sigma_{j} \\rho_{ij}\n$$\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol        |Meaning                                  |\n|:-------------|:----------------------------------------|\n|$\\sigma_{ij}$ |The covariance between $X_i$ and $X_j$   |\n|$\\rho_{ij}$   |The correlation between $X_i$ and $X_j$  |\n|$\\sigma_{i}$  |The standard deviation of variable $X_i$ |\n|$\\sigma_{j}$  |The standard deviation of variable $X_j$ |\n\n\n:::\n:::\n\n\nUnfortunately, computing a covariance matrix like this in a computer program is inelegant because we have to make use of looping:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# R = Correlation matrix of variables in X\nR <- matrix(c(1,0.5,0.6,\n              0.5,1,0.7,\n              0.6,0.7,1), nrow = 3)\nrownames(R) <- colnames(R) <- Xnames #Apply names\n\n# s = The standard deviations of variables in X\ns <- c(15,15,15)\n\n# k = The number of variables\nk <- length(s)\n\n# CM = Covariance Matrix\n# Initialize k by k matrix of zeroes\nCM <- matrix(0, k, k)\nrownames(CM) <- colnames(CM) <- Xnames\nfor (i in seq(1, k)) {\n  for (j in seq(1, k)) {\n    CM[i, j] = s[i] * s[j] * R[i, j]\n  }\n}\n\nCM\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      X1    X2    X3\nX1 225.0 112.5 135.0\nX2 112.5 225.0 157.5\nX3 135.0 157.5 225.0\n```\n\n\n:::\n:::\n\n\n:::{.column-margin}\nThe $\\mathtt{diag}$ function has three purposes. First, it can extract the diagonal vector from a matrix:\n$$\nA_{kk} =\n \\begin{bmatrix}\n  a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n  a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  a_{k1} & a_{k2} & \\cdots & a_{kk}\n \\end{bmatrix}\n$$\nThen \n$$\\mathtt{diag}(\\mathbf{A}) = \\{a_{11},a_{22},...,a_{kk}\\}$$\nSecond, the `diag` function inserts a vector into the the diagonal of a $k \\times k$ matrix of zeros:\n$$\n\\mathtt{diag}(\\mathbf{a}) = \n\\begin{bmatrix}\n  a_{1} & 0 & \\cdots & 0 \\\\\n  0 & a_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & a_{k}\n \\end{bmatrix}\n$$\nThird, the `diag` function converts integer $k$ into a $k \\times k$ identity matrix:\n$$\n\\mathtt{diag}(k) = \n\\begin{bmatrix}\n  1 & 0 & \\cdots & 0 \\\\\n  0 & 1 & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & 1\n \\end{bmatrix}\n$$\n:::\n\nTo calculate the covariance matrix in matrix notation, we can make use of the $\\mathtt{diag}$ function. \n\n\n$$\n\\begin{align*}\n\\mathbf{\\Sigma}&=\\mathtt{diag}(\\mathbf{\\sigma}) \\mathbf{R}\\, \\mathtt{diag}(\\mathbf{\\sigma})\\\\ \n&=\\begin{bmatrix}\n  \\sigma_{1} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & \\sigma_{k}\n \\end{bmatrix}\n  \\begin{bmatrix}\n  1 & \\rho_{12} & \\cdots & \\rho_{1k} \\\\\n  \\rho_{21} & 1 & \\cdots & \\rho_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\rho_{k1} & \\rho_{k2} & \\cdots & 1\n \\end{bmatrix}\n\\begin{bmatrix}\n  \\sigma_{1} & 0 & \\cdots & 0 \\\\\n  0 & \\sigma_{2} & \\cdots & 0 \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  0 & 0 & \\cdots & \\sigma_{k}\n \\end{bmatrix}\\\\\n  &=  \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{1}\\sigma_{2}\\rho_{12} & \\cdots & \\sigma_{1}\\sigma_{k}\\rho_{1k} \\\\\n  \\sigma_{2}\\sigma_{1}\\rho_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2}\\sigma_{k}\\rho_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k}\\sigma_{1}\\rho_{k1} & \\sigma_{k}\\sigma_{2}\\rho_{k2} & \\cdots & \\sigma_{k}^2\n \\end{bmatrix}\\\\\n   &=  \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n \\end{bmatrix}\n\\end{align*}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol            |Meaning                                               |\n|:-----------------|:-----------------------------------------------------|\n|$\\mathbf{\\Sigma}$ |The covariance matrix of variables in $\\mathbf{X}$    |\n|$\\mathbf{\\sigma}$ |The standard deviations of variables in  $\\mathbf{X}$ |\n|$\\mathbf{R}$      |The correlation matrix of variables in $\\mathbf{X}$   |\n\n\n:::\n:::\n\n\nSo this is where matrix algebra starts to shine. We do not need to initialize the variable that contains the covariance matrix, or calculate $k$, or do any looping. \n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# CM = Covariance matrix of variables in X\nCM <- diag(s) %*% R %*% diag(s)\n```\n:::\n\n\nBeautiful!\n\nRunning the code, we get:\n$$\n\\mathbf{\\Sigma}=\\begin{bmatrix}\n225 & 112.5 & 135\\\\\n112.5 & 225 & 157.5\\\\\n135 & 157.5 & 225\n\\end{bmatrix}\n$$\n\nTo calculate the variance of $X_S$ if it is an unweighted sum we apply @eq-variancesum:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n:::\n\nHere we see that $\\sigma_X^2=1485$\n\n## Calculating the Variance of a Weighted Sum\n\nIf $X_S$ is a weighted sum, its variance is the weighted sum of the covariance matrix.\n\n\n$$\n\\sigma_S^2=\\sum_{i=1}^{k}{\\sum_{j=1}^{k}{w_i w_j \\sigma_{ij}}}=\\mathbf{w'\\Sigma w}\n$$ {#eq-varianceweightedsum}\n\nContinuing with the same variables in our example, we see that things are starting to become clumsy and ugly without matrix algebra:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# First we initialize var_S as 0\nvar_S_weighted <- 0\n# Now we loop through k rows and k columns of CM\nfor (i in seq(1, k)) {\n  for (j in seq(1, k)) {\n    var_S_weighted <- var_S_weighted + w[i] * w[j] * CM[i, j]\n  }\n}\n```\n:::\n\n\nWith matrix algebra, this all happens with a single line of code:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nvar_S_weighted <- t(w) %*% CM %*% w\n```\n:::\n\n\nIf we don't need to know the covariance matrix, we can skip its calculation:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\nvar_S_weighted <- t(w * s) %*% R %*% (w * s)\n```\n:::\n\n\n\nAll three methods give the same answer: $\\sigma_X^2 = 2193.75$\n\n## Calculating a Composite Score\n\nWe now have all of the information needed to make a composite score. First we will make an unweighted composite.\n\n\\begin{equation}\nC = \\frac{x_S-\\mu_S}{\\sigma_{X_S}}\\sigma_C + \\mu_C\n(\\#eq:Composite)\n\\end{equation}\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n::: {.cell-output-display}\n\n\n|Symbol         |Meaning                                        |\n|:--------------|:----------------------------------------------|\n|$C$            |The composite variable $C$                     |\n|$x_S$          |The sum of all $k$ scores in $\\{x_1,...,x_k\\}$ |\n|$\\sigma_{X_S}$ |The standard deviation of $X_S$                |\n|$\\sigma_C$     |The standard deviation of $C$                  |\n|$\\mu_C$        |The mean of $C$                                |\n\n\n:::\n:::\n\n\nContinuing with our example, suppose that $\\mu_C=100$ and $\\sigma_C=15.$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# m_C = Composite mean\nm_C <- 100\n# s_C = Composite standard deviation\ns_C <- 15\n# C = The composite score\nC <- ((x_S - m_S) / sqrt(var_S)) * s_C + m_C\n```\n:::\n\n\n$$\nC=119\n$$\n\n### Calculating a Weighted Composite Score\n\nIn matrix notation, an unweighted composite score is calculated like so:\n\n$$\nC=\\mathbf{\\frac{1'(x-\\mu_x)}{\\sqrt{1'\\Sigma 1}}}\\sigma_C+\\mu_C\n$$\nReplacing each vector of ones $\\mathbf{1}$ with a weight vector $\\mathbf{w}$ gives us the formula for computing a weighted composite score:\n\n$$\nC_w=\\mathbf{\\frac{w'(x-\\mu_x)}{\\sqrt{w'\\Sigma w}}}\\sigma_C+\\mu_C\n$$\n\n:::{.column-margin}\nIn clinical practice, the most common kind of composite score is an equally weighted composite consisting of scores with the same mean and standard deviation. Sometimes scores are weighted by the degree to which they correlate with the construct the composite is intended to measure. Other weighting schemes are also possible. In most cases, it makes sense to first convert all scores to z-scores and then weight the z-scores. Failing to convert the scores to a common metric such as z-scores will result in an implicit weighting by standard deviations. That is, the score with the largest standard deviation will have the most weight in the composite score. Converting to z-scores first will equalize the each score&rsquo;s influence on the composite score. We can convert all the scores in $\\mathbf{x}$ to z-scores like so:\n  \n$$\n\\mathbf{z} = \\frac{\\mathbf{x}-\\mathbf{\\mu}}{\\mathbf{\\sigma}}\n$$\n\nThe nice thing about z-scores is that their means are zeros, their standard deviations are ones, and their covariances are correlations. Thus, the formula for a weighted composite score consisting of z-scores is fairly simple, especially if the composite score is a z-score itself:\n  \n$$\nC = \\frac{\\mathbf{w^\\prime z}}{\\sqrt{\\mathbf{w^\\prime Rw}}}\\sigma_C+\\mu_C\n$$\n:::\n\n\nContinuing with our example, computing a composite score with $X_1=110$, $X_2=120$, and $X_3=118$:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# C = The composite score\nC <- (t(w) %*% (x - m) / sqrt(t(w) %*% CM %*% w)) * 15  + 100\n\n# With index scores, we round to the nearest integer\nC <- round(C)\n\n# Notes:\n# The round function rounds to the nearest integer by default,\n# but you can round to any number of digits you wish. For example, \n# rounding to 2 significant digits (i.e., the hundredths place), \n# would be round(x,2).\n```\n:::\n\n\nHere we see, that the weighted composite score $C = 120.$ If we had wanted an equally weighted composite score, the `w` vector would be set to equal ones. If we had done so, $C$ would have been $119$\ninstead of $120.$\n\n## The Reliability Coefficient of a Composite\n\nCalculating the reliability coefficient of a composite score is much easier than it might appear at first. Remember that reliability is the ratio of a score's true score variance to its total variance:\n\n$$\nr_{XX} = \\frac{\\sigma_T^2}{\\sigma_X^2}\n$$\n\nThe variance of a composite score is the sum of the covariance matrix of the composite's component scores. \n\n$$\n\\sigma_X^2=\\mathbf{1}'\\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n \\end{bmatrix}\\mathbf{1}\n$$\n\nThe covariance matrix of the component true scores is exactly the same as the covariance matrix of the component observed scores, except that the variances on the diagonal are multiplied by the reliability coefficients of each of the component scores like so:\n\n\n$$\n\\sigma_T^2=\\mathbf{1}'\\begin{bmatrix}\n  {\\color{firebrick}r_{11}}\\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & {\\color{firebrick}r_{22}}\\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & {\\color{firebrick}r_{kk}}\\sigma_{k}^2\n \\end{bmatrix}\\mathbf{1}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# r_XX = Reliability coefficients for X1, X2, and X3\nr_XX <- c(0.88,0.80,0.76)\n\n# CM_T = Covariance matrix of true scores\nCM_T <- CM\n\n# Replace diagonal with true score variances\ndiag(CM_T) <- diag(CM) * r_XX\n\n# r_C = Reliability coefficient of composite score\nr_C <- sum(CM_T) / sum(CM)\n```\n:::\n\n\nThe reliability coefficient of the unweighted composite is 0.92.\n\nFor a weighted composite, the reliability is calculated by relplacing each vector of ones $\\mathbf{1}$ with a weight vector $\\mathbf{w}$: \n\n$$\n\\rho_{XX}=\\frac{\\sigma_T^2}\n               {\\sigma_X^2}\n         =\\frac{\\mathbf{w}^{\\prime}\n               \\begin{bmatrix}\n  {\\color{firebrick}r_{11}}\\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & {\\color{firebrick}r_{22}}\\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & {\\color{firebrick}r_{kk}}\\sigma_{k}^2\n                \\end{bmatrix}\n                \\mathbf{w}}\n          {\\mathbf{w}^{\\prime}\n               \\begin{bmatrix}\n  \\sigma_{1}^2 & \\sigma_{12} & \\cdots & \\sigma_{1k} \\\\\n  \\sigma_{21} & \\sigma_{2}^2 & \\cdots & \\sigma_{2 k} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  \\sigma_{k1} & \\sigma_{k2} & \\cdots & \\sigma_{k}^2\n \\end{bmatrix}\\mathbf{w}}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Reliability coefficient of the weighted composite\nr_C_w <- t(w) %*% CM_T %*% w / t(w) %*% CM %*% w\n```\n:::\n\n\nThe reliability coefficient of the weighted composite is 0.88.\n\n## Composite Scores and Their Correlations\n\nIf the population correlations among all of the components are known, it is possible to calculate the correlations among composite scores made from these components. Such correlations can be used in many practical applications, including in prediction models and in the evaluation of difference scores.\n\nSuppose that Composite $A$ is calculated from the sum of two component tests, $A_1$ and $A_2.$ Composite $B$ is calculated from the sum of $B_1$ and $B_2.$ Suppose that the correlation matrix for the four components is:\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n:::\n\n\n\n$$\n\\mathbf{R} = \\begin{array}{r|cccc}\n & \\color{royalblue}{A_1} & \\color{royalblue}{A_2} & \\color{firebrick}{B_1} & \\color{firebrick}{B_2}\\\\\n\\hline \\color{royalblue}{A_1} & \\color{royalblue}{1} & \\color{royalblue}{.30} & \\color{purple}{.35} & \\color{purple}{.40}\\\\\n\\color{royalblue}{A_2} & \\color{royalblue}{.30} & \\color{royalblue}{1} & \\color{purple}{.42} & \\color{purple}{.48}\\\\\n\\color{firebrick}{B_1} & \\color{purple}{.35} & \\color{purple}{.42} & \\color{firebrick}{1} & \\color{firebrick}{.56}\\\\\n\\color{firebrick}{B_2} & \\color{purple}{.40} & \\color{purple}{.48} & \\color{firebrick}{.56} & \\color{firebrick}{1}\n\\end{array}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make correlation matrix R\nR <- matrix(c(1, .30, .35, .40,\n              .30, 1, .42, .48,\n              .35, .42, 1, .56,\n              .40, .48, .56, 1),\n            nrow = 4,\n            ncol = 4)\n\n# Display R\nR\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3] [,4]\n[1,] 1.00 0.30 0.35 0.40\n[2,] 0.30 1.00 0.42 0.48\n[3,] 0.35 0.42 1.00 0.56\n[4,] 0.40 0.48 0.56 1.00\n```\n\n\n:::\n:::\n\n\n\n\nThe correlation between Composite $A$ and Composite $B$ is calculating by adding up the numbers is all three shaded regions of the correlation matrix and then dividing the sum of \"between\" correlations in purple by the geometric mean of the sums from the \"within\" correlations in the blue and red regions in the correlation matrix $\\mathbf{R}$ like so:\n\n$$r_{AB}=\\dfrac{\\color{purple}{\\text{Sum of Correlations between A and B}}}{\\sqrt{\\color{royalblue}{\\text{Sum of Correlations Within A}}\\times\\color{firebrick}{{\\text{Sum of Correlations Within B}}}}}$$\n\nTo calculate these sums using matrix algebra, we first construct a \"weight matrix\" $\\mathbf{W}$ that tells us which tests are in which composite. The 1s in the first two rows of column 1 tell us that tests $A_1$ and $A_2$ belong to composite $A$. Likewise, the 1s in the last two rows of column 2 tell us that $B_1$ and $B_2$ belong to composite $B$.\n\n$$\n\\mathbf{W}=\n\\begin{array}{r|cc}\n & \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A_1} & \\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{A_2} & \\color{royalblue}{1} & 0\\\\\n\\color{firebrick}{B_1} & 0 & \\color{firebrick}{1}\\\\\n\\color{firebrick}{B_2} & 0 & \\color{firebrick}{1}\n\\end{array} \n$$\n\nHere is one way to make the weight matrix in R:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make a 4 by 2 matrix of 0s\nW <- matrix(0, nrow = 4, ncol = 2)\n\n# Assign 1s to rows 1 and 2 to column 1\nW[1:2,1] <- 1\n# Assign 1s to rows 3 and 4 to column 2\nW[3:4,2] <- 1\n\n# Display W\nW\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    0\n[3,]    0    1\n[4,]    0    1\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n:::\n\n\nThe covariance matrix $\\mathbf{\\Sigma_{AB}}$ is calculated by pre- and post-mulitplying $\\mathbf{R}$ by the wieght matrix \\mathbf{W}.\n\n$$\n\\begin{aligned}\\mathbf{\\Sigma_{AB}} &= \\mathbf{W'RW}\\\\[2ex] \n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{1} & 0\\\\\n0 & \\color{firebrick}{1}\\\\\n0 & \\color{firebrick}{1}\n\\end{array}\\right]'\\left[\\begin{array}{cccc}\n\\color{royalblue}{1} & \\color{royalblue}{.30} & \\color{purple}{.35} & \\color{purple}{.40}\\\\\n\\color{royalblue}{.30} & \\color{royalblue}{1} & \\color{purple}{.42} & \\color{purple}{.48}\\\\\n\\color{purple}{.35} & \\color{purple}{.42} & \\color{firebrick}{1} & \\color{firebrick}{.56}\\\\\n\\color{purple}{.40} & \\color{purple}{.48} & \\color{firebrick}{.56} & \\color{firebrick}{1}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & 0\\\\\n\\color{royalblue}{1} & 0\\\\\n0 & \\color{firebrick}{1}\\\\\n0 & \\color{firebrick}{1}\n\\end{array}\\right]\\\\[2ex]\n&=\\begin{array}{r|cc}\n & \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A} & \\color{royalblue}{1 + .30 + .30 + 1} & \\color{purple}{.35 + .42 + .40 + .48}\\\\\n\\color{firebrick}{B} & \\color{purple}{.35 + .40 + .42 + .48} & \\color{firebrick}{1 + .56 + .56 + 1}\n\\end{array}\\\\[2ex]\n&=\\begin{array}{r|cc}\n & \\color{royalblue}{A} & \\color{firebrick}{B}\\\\\n\\hline \\color{royalblue}{A} & \\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{firebrick}{B} & \\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\n\\end{aligned}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Covariance matrix of composites A and B\nSigma_AB <- t(W) %*% R %*% W\n\n# Display Sigma_AB\nSigma_AB\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 2.60 1.65\n[2,] 1.65 3.12\n```\n\n\n:::\n:::\n\n\n\n\nNow we need to extract the variance diagonal from the covariance matrix so that we can use them to convert the covariance matrix to a correlation matrix. The variances are put on a diagonal matrix, and then taking the square root converts the variances to standard deviations in matrix $\\mathbf{\\sigma}$. \n\n$$\n\\begin{aligned}\n\\mathbf{\\sigma}\n&=\\mathtt{diag}(\\mathtt{diag}(\\mathbf{\\Sigma_{AB}}))^{\\frac{1}{2}}\\\\\n&=\\mathtt{diag}\\left(\n\\mathtt{diag}\\left({\n\\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]\n   }\\right)\n   \\right)^{\\frac{1}{2}}\\\\\n   &=\\mathtt{diag}\\left(\n   \\left[\\begin{array}{c}\n\\color{royalblue}{2.60}\\\\\n\\color{firebrick}{3.12}\n\\end{array}\\right]\n   \\right)^{\\frac{1}{2}}\\\\\n   &=\\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & 0\\\\\n0 & \\color{firebrick}{3.12}\n\\end{array}\\right]^{\\frac{1}{2}}\\\\\n   &=\\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Standard deviations\nsigma <- diag(diag(Sigma_AB)^(0.5))\n\n# Display sigma\nsigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]     [,2]\n[1,] 1.612452 0.000000\n[2,] 0.000000 1.766352\n```\n\n\n:::\n:::\n\n\n\nPre- and post-multiplying the covariance matrix $\\mathbf{\\Sigma_{AB}}$ by the inverted standard devation matrix $\\mathbf{\\sigma}$ to yield the correlations between composites $A$ and $B$.\n\n$$\n\\begin{aligned}\n\\mathbf{R_{AB}}&=\\mathbf{\\sigma}^{-1}\\mathbf{\\Sigma_{AB}}\\mathbf{\\sigma}^{-1}\\\\[2ex]\n&={\\mathbf{\\sigma}^{-1} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]^{-1}}\n   {\\mathbf{\\Sigma_{AB}} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]}\n   {\\mathbf{\\sigma}^{-1} \\atop \\left[\\begin{array}{cc}\n\\color{royalblue}{1.6125} & 0\\\\\n0 & \\color{firebrick}{1.7664}\n\\end{array}\\right]^{-1}}\\\\[1ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{.6202} & 0\\\\\n0 & \\color{firebrick}{.5661}\n\\end{array}\\right]\n   \\left[\\begin{array}{cc}\n\\color{royalblue}{2.60} & \\color{purple}{1.65}\\\\\n\\color{purple}{1.65} & \\color{firebrick}{3.12}\n\\end{array}\\right]\n   \\left[\\begin{array}{cc}\n\\color{royalblue}{.6202} & 0\\\\\n0 & \\color{firebrick}{.5661}\n\\end{array}\\right]\\\\[1ex]\n&=\\left[\\begin{array}{cc}\n\\color{royalblue}{1} & \\color{purple}{.58}\\\\\n\\color{purple}{.58} & \\color{firebrick}{1}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n:::{.column-margin}\nThe `solve` function inverts a square matrix. Inverting a single number $x$ gives its reciprocal. If you multiply the inverse of a number by the nunbe itself, you get 1, the *identity* for the multiplication operator. $x^{-1}x=1$, multiplying a matrix by its inverse creates an identity matrix (a matrix with ones on the diagonal and zeroes elsewhere):\n  \n$$\\mathbf{A^{-1}A = I}$$\n:::\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Correlations between composites A and B\nR_AB <- solve(sigma) %*% Sigma_AB %*% solve(sigma)\nR_AB\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]      [,2]\n[1,] 1.0000000 0.5793219\n[2,] 0.5793219 1.0000000\n```\n\n\n:::\n:::\n\n\n\nThese calculations in R can be greatly simplified using the `cov2cor` function, which converts covariances to correlations:\n\n\n::: {.cell .fig-cap-location-margin .tbl-cap-location-margin layout-align=\"center\"}\n\n```{.r .cell-code}\n# Correlation matrix of composites A and B\nR_AB <- cov2cor(t(W) %*% R %*% W)\n\nR_AB\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]      [,2]\n[1,] 1.0000000 0.5793219\n[2,] 0.5793219 1.0000000\n```\n\n\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}